{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Two_layer_NN(object):\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['w1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['w2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def loss_grad(self, X, y = None, reg=0.0):\n",
    "        \n",
    "        w1,b1 = self.params['w1'],self.params['b1']\n",
    "        w2,b2 = self.params['w2'],self.params['b2']\n",
    "        N, D = X.shape\n",
    "        #compute the forward pass\n",
    "        \n",
    "        scores=None\n",
    "        z1=np.dot(X,w1) + b1\n",
    "        a1=np.maximum(0, z1); #Relu layer\n",
    "        scores=np.dot(a1, w2) + b2\n",
    "        \n",
    "        if y is None:\n",
    "            return scores\n",
    "        \n",
    "        else:\n",
    "            #compute loss and gradient\n",
    "            \n",
    "            exp_scores=np.exp(scores)\n",
    "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "            \n",
    "            corect_logprobs = -np.log(probs[range(N),y])\n",
    "            data_loss = np.sum(corect_logprobs) / N\n",
    "            reg_loss = 0.5 * reg * np.sum(w1 * w1) + 0.5 * reg * np.sum(w2 * w2)\n",
    "            loss = data_loss + reg_loss\n",
    "            \n",
    "            #grads\n",
    "            \n",
    "            grads={}\n",
    "            dscores = probs\n",
    "            dscores[range(N),y] -= 1\n",
    "            dscores /= N\n",
    "\n",
    "            # W2 and b2\n",
    "            grads['w2'] = np.dot(a1.T, dscores)\n",
    "            grads['b2'] = np.sum(dscores, axis=0)\n",
    "            # next backprop into hidden layer\n",
    "            dhidden = np.dot(dscores, w2.T)\n",
    "            # backprop the ReLU non-linearity\n",
    "            dhidden[a1 <= 0] = 0\n",
    "            # finally into W,b\n",
    "            grads['w1'] = np.dot(X.T, dhidden)\n",
    "            grads['b1'] = np.sum(dhidden, axis=0)\n",
    "\n",
    "            # add regularization gradient contribution\n",
    "            grads['w2'] += reg * w2\n",
    "            grads['w1'] += reg * w1\n",
    "            \n",
    "            return loss,grads\n",
    "        \n",
    "    def train(self, X, y, X_val, y_val, learning_rate=1e-3, learning_rate_decay=1, reg=1e-5, num_epochs=200, verbose=True):\n",
    "        \n",
    "        num_train = X.shape[0]\n",
    "        \n",
    "        it=num_epochs*10\n",
    "        \n",
    "        \n",
    "        for it in xrange(it):\n",
    "           \n",
    "            loss, grads = self.loss_grad(X, y=y, reg=reg)\n",
    "            #loss_history.append(loss)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.params['w1'] =self.params['w1'] - (learning_rate * grads['w1'])\n",
    "            self.params['b1'] =self.params['b1'] - (learning_rate * grads['b1'])\n",
    "            self.params['w2'] =self.params['w2'] - (learning_rate * grads['w2'])\n",
    "            self.params['b2'] =self.params['b2'] - (learning_rate * grads['b2'])\n",
    "            \n",
    "           \n",
    "            train_acc = (self.predict(X) == y).mean()\n",
    "            val_acc = (self.predict(X_val) == y_val).mean()\n",
    "            \n",
    "            if verbose and it % 10 == 0:\n",
    "                print 'epochs %d / %d: loss %f : training accuracy %f, and val accuracy %f' % (it/10, num_epochs, loss,train_acc, val_acc)\n",
    "            \n",
    "            #train_acc_history.append(train_acc)\n",
    "            #val_acc_history.append(val_acc)\n",
    "            \n",
    "            \n",
    "            learning_rate *= learning_rate_decay\n",
    "             \n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        y_pred = None\n",
    "        \n",
    "        z1 = np.dot(X,self.params['w1']) + self.params['b1']\n",
    "        a1 = np.maximum(0, z1) # pass through ReLU activation function\n",
    "        scores = np.dot(a1,self.params['w2']) + self.params['b2']\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "    \n",
    "        return y_pred\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) \n",
      "(10000,) \n",
      "2 \n",
      "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6, 2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 0, 3, 7, 3, 3, 5, 2, 2, 7, 1, 1, 1, 2, 2, 0, 9, 5, 7, 9, 2, 2, 5, 2, 4, 3, 1, 1, 8, 2, 1, 1, 4, 9, 7, 8, 5, 9, 6, 7, 3, 1, 9, 0, 3, 1, 3, 5, 4, 5, 7, 7, 4, 7, 9, 4, 2, 3, 8, 0, 1, 6, 1, 1, 4, 1, 8, 3, 9, 6, 6, 1, 8, 5, 2, 9, 9, 8, 1, 7, 7, 0, 0, 6, 9, 1, 2, 2, 9, 2, 6, 6, 1, 9, 5, 0, 4, 7, 6, 7, 1, 8, 1, 1, 2, 8, 1, 3, 3, 6, 2, 4, 9, 9, 5, 4, 3, 6, 7, 4, 6, 8, 5, 5, 4, 3, 1, 8, 4, 7, 6, 0, 9, 5, 1, 3, 8, 2, 7, 5, 3, 4, 1, 5, 7, 0, 4, 7, 5, 5, 1, 0, 9, 6, 9, 0, 8, 7, 8, 8, 2, 5, 2, 3, 5, 0, 6, 1, 9, 3, 6, 9, 1, 3, 9, 6, 6, 7, 1, 0, 9, 5, 8, 5, 2, 9, 0, 8, 8, 0, 6, 9, 1, 1, 6, 3, 7, 6, 6, 0, 6, 6, 1, 7, 1, 5, 8, 3, 6, 6, 8, 6, 8, 4, 6, 6, 1, 3, 8, 3, 4, 1, 7, 1, 3, 8, 5, 1, 1, 4, 0, 9, 3, 7, 4, 9, 9, 2, 4, 9, 9, 1, 0, 5, 9, 0, 8, 2, 1, 2, 0, 5, 6, 3, 2, 7, 8, 8, 6, 0, 7, 9, 4, 5, 6, 4, 2, 1, 1, 2, 1, 5, 9, 9, 0, 8, 4, 1, 1, 6, 3, 3, 9, 0, 7, 9, 7, 7, 9, 1, 5, 1, 6, 6, 8, 7, 1, 3, 0, 3, 3, 2, 4, 5, 7, 5, 9, 0, 3, 4, 0, 4, 4, 6, 0, 0, 6, 6, 0, 8, 1, 6, 2, 9, 2, 5, 9, 6, 7, 4, 1, 8, 7, 3, 6, 9, 3, 0, 4, 0, 5, 1, 0, 3, 4, 8, 5, 4, 7, 2, 3, 9, 7, 6, 7, 1, 4, 7, 0, 1, 7, 3, 1, 8, 4, 4, 2, 0, 2, 2, 0, 0, 9, 0, 9, 6, 8, 2, 7, 7, 4, 0, 3, 0, 8, 9, 4, 2, 7, 2, 5, 2, 5, 1, 9, 4, 8, 5, 1, 7, 4, 4, 0, 6, 9, 0, 7, 8, 8, 9, 9, 3, 3, 4, 0, 4, 5, 6, 6, 0, 1, 0, 8, 0, 4, 8, 8, 1, 5, 2, 6, 8, 1, 0, 0, 7, 7, 5, 9, 6, 2, 8, 3, 4, 7, 3, 9, 0, 1, 2, 4, 8, 1, 8, 6, 4, 4, 5, 7, 1, 3, 9, 8, 0, 1, 7, 5, 8, 2, 8, 0, 4, 1, 8, 9, 8, 2, 9, 9, 2, 7, 5, 7, 3, 8, 8, 4, 4, 2, 7, 1, 6, 4, 0, 4, 6, 9, 7, 6, 2, 5, 5, 1, 7, 2, 2, 2, 9, 5, 4, 2, 7, 8, 1, 3, 4, 3, 7, 6, 9, 8, 0, 6, 0, 2, 2, 2, 1, 8, 4, 0, 1, 8, 8, 1, 5, 7, 6, 4, 5, 8, 7, 1, 9, 1, 9, 8, 4, 7, 3, 8, 8, 2, 6, 6, 7, 1, 6, 8, 1, 9, 7, 8, 3, 0, 1, 0, 8, 8, 3, 0, 0, 1, 5, 0, 8, 8, 7, 9, 9, 0, 9, 4, 1, 3, 6, 6, 4, 4, 7, 5, 6, 0, 8, 0, 3, 2, 8, 4, 6, 9, 9, 7, 0, 3, 3, 6, 7, 4, 9, 1, 6, 2, 7, 2, 2, 0, 6, 7, 5, 7, 6, 8, 9, 0, 9, 4, 4, 7, 0, 9, 4, 9, 6, 9, 4, 5, 7, 9, 2, 4, 5, 1, 4, 3, 9, 6, 5, 6, 9, 3, 3, 5, 0, 7, 2, 1, 3, 6, 4, 0, 0, 2, 5, 0, 1, 0, 2, 3, 9, 8, 4, 9, 8, 0, 2, 6, 4, 4, 0, 1, 8, 8, 3, 6, 9, 6, 6, 7, 8, 2, 4, 5, 7, 6, 5, 3, 0, 5, 0, 5, 0, 8, 2, 6, 7, 3, 8, 2, 1, 7, 6, 7, 1, 0, 9, 5, 5, 0, 1, 7, 6, 9, 0, 4, 7, 7, 1, 5, 9, 4, 0, 8, 5, 9, 9, 6, 7, 1, 8, 3, 2, 3, 8, 2, 2, 4, 6, 0, 0, 5, 3, 8, 2, 3, 7, 2, 9, 3, 8, 7, 8, 2, 7, 9, 0, 2, 3, 2, 2, 2, 3, 3, 6, 2, 3, 2, 8, 0, 5, 5, 1, 4, 5, 6, 6, 2, 7, 0, 1, 7, 7, 8, 2, 9, 2, 2, 4, 2, 1, 1, 1, 6, 6, 6, 5, 1, 1, 7, 0, 4, 3, 3, 7, 1, 2, 3, 5, 5, 5, 6, 1, 4, 3, 7, 8, 8, 3, 6, 6, 2, 3, 0, 9, 4, 3, 8, 0, 0, 1, 1, 5, 4, 9, 3, 1, 8, 9, 3, 9, 9, 2, 9, 4, 8, 2, 9, 8, 8, 1, 5, 3, 6, 8, 7, 6, 9, 8, 0, 6, 4, 0, 0, 2, 5, 8, 2, 0, 2, 7, 6, 9, 7, 1, 5, 5, 6, 6, 3, 6, 2, 4, 7, 0, 5, 6, 4, 6, 5, 2, 4, 6, 1, 6, 0, 4, 0, 3, 1, 8, 5, 4, 4, 1, 7, 3, 9, 4, 7, 9, 7, 3, 7, 2, 8, 4, 6, 6, 1, 2, 9, 0, 4, 8, 7, 3, 9, 8, 7, 7, 0, 2, 4, 1, 1, 4, 1, 5, 4, 0, 5, 6, 2, 8, 5, 0, 2, 1, 3, 5, 7, 3, 5, 1, 3, 5, 9, 4, 3, 2, 4, 4, 1, 4, 2, 2, 3, 8, 0, 6, 8, 5, 6, 6, 4, 7, 1, 1, 4, 6, 2, 3, 9, 6, 9, 1, 3, 6, 8, 5, 9, 6, 8, 1, 6, 0, 2, 3, 7, 9, 0, 9, 7, 6, 3, 9, 2, 6, 1, 6, 7, 3, 8, 3, 8, 3, 8, 5, 9, 6, 1, 2, 5, 2, 1, 4, 3, 7, 5, 9, 3, 9, 3, 2, 9, 1, 8, 5, 9, 7, 2, 6, 0, 8, 5, 7, 1, 5, 8, 5, 7, 1, 5, 0, 3, 9, 3, 6, 9, 1, 3, 9, 8, 2, 2, 3, 2, 5, 9, 7, 9, 9, 8, 9, 7, 0, 3, 3, 2, 0, 3, 7, 6, 3, 3, 2, 0, 6, 6, 5, 5, 7, 5, 9, 8, 2, 9, 8, 0, 4, 0, 1, 2, 0, 4, 7, 3, 8, 5, 1, 6, 6, 5, 5, 4, 6, 3, 6, 8, 2, 3, 7, 0, 7, 0, 4, 1, 9, 5, 7, 8, 6, 6, 8, 0, 7, 2, 8, 4, 8, 2, 0, 9, 0, 0, 2, 9, 6, 6, 5, 6, 0, 3, 7, 5, 5, 7, 9, 3, 4, 5, 0, 5, 2, 3, 2, 6, 0, 4, 9, 0, 7, 0, 9, 7, 2, 6, 4, 6, 9, 5, 4, 7, 0, 6, 8, 8, 9, 9, 9, 0, 9, 8, 6, 4, 8, 1, 9, 1, 0, 5, 8, 6, 9, 6, 0, 8, 1, 3, 9, 4, 8, 4, 3, 2, 6, 0, 8, 9, 9, 4, 3, 0, 2, 4, 4, 0, 3, 5, 7, 5, 7, 7, 9, 0, 9, 5, 3, 8, 2, 4, 2, 3, 1, 2, 8, 9, 2, 8, 1, 4, 2, 0, 4, 5, 4, 8, 1, 7, 4, 1, 1, 0, 2, 7, 7, 4, 4, 4, 4, 8, 4, 3, 6, 6, 0, 1, 3, 9, 8, 4, 8, 9, 6, 2, 0, 5, 5, 9, 4, 2, 0, 8, 8, 0, 4, 0, 7, 6, 9, 5, 3, 5, 4, 4, 4, 4, 3, 7, 9, 2, 5, 1, 8, 3, 2, 6, 9, 6, 3, 1, 7, 4, 6, 3, 7, 8, 6, 2, 4, 6, 8, 0, 1, 9, 9, 1, 0, 0, 8, 9, 4, 7, 4, 4, 1, 9, 8, 8, 6, 1, 7, 4, 8, 8, 8, 0, 5, 6, 6, 8, 3, 4, 4, 1, 2, 1, 5, 7, 1, 7, 2, 8, 5, 9, 5, 6, 1, 9, 5, 0, 4, 3, 3, 0, 8, 2, 8, 0, 9, 0, 4, 6, 9, 2, 8, 2, 7, 7, 2, 2, 7, 1, 6, 1, 3, 4, 4, 8, 6, 0, 1, 9, 4, 2, 7, 5, 3, 9, 0, 1, 9, 0, 9, 8, 7, 0, 4, 9, 0, 5, 2, 1, 0, 2, 8, 8, 0, 5, 7, 6, 6, 5, 3, 7, 7, 7, 4, 2, 0, 1, 7, 3, 2, 7, 3, 2, 2, 1, 5, 9, 9, 8, 0, 8, 4, 3, 3, 8, 5, 0, 8, 4, 5, 7, 5, 1, 5, 0, 2, 0, 5, 4, 2, 3, 6, 2, 6, 2, 2, 3, 4, 6, 6, 5, 3, 0, 1, 2, 7, 7, 5, 3, 1, 1, 7, 6, 1, 6, 3, 3, 3, 3, 4, 8, 1, 0, 7, 7, 6, 8, 1, 2, 5, 3, 4, 1, 1, 9, 3, 1, 4, 2, 7, 1, 6, 5, 7, 7, 7, 6, 6, 1, 9, 5, 0, 9, 5, 7, 0, 0, 4, 6, 8, 0, 1, 5, 0, 9, 3, 1, 1, 2, 0, 3, 2, 9, 1, 9, 6, 2, 4, 8, 8, 5, 9, 2, 1, 9, 4, 4, 3, 0, 6, 6, 0, 4, 1, 0, 9, 5, 6, 5, 6, 9, 4, 4, 2, 6, 8, 4, 7, 6, 5, 9, 8, 7, 1, 9, 5, 4, 3, 5, 4, 3, 4, 1, 5, 5, 4, 0, 8, 4, 4, 0, 9, 2, 8, 9, 8, 0, 2, 2, 2, 6, 7, 8, 1, 9, 8, 0, 3, 8, 6, 8, 1, 6, 5, 4, 2, 1, 4, 3, 9, 7, 8, 3, 0, 8, 3, 4, 2, 9, 1, 0, 0, 3, 0, 4, 5, 9, 0, 7, 5, 9, 5, 8, 8, 6, 3, 1, 9, 5, 2, 4, 7, 6, 1, 8, 6, 9, 3, 1, 3, 7, 4, 0, 6, 7, 6, 9, 2, 4, 1, 9, 8, 5, 8, 2, 2, 5, 0, 2, 0, 7, 0, 6, 6, 4, 8, 7, 9, 6, 9, 2, 3, 8, 8, 3, 9, 9, 8, 7, 2, 3, 5, 5, 1, 8, 7, 4, 3, 5, 2, 2, 2, 1, 4, 2, 2, 8, 4, 9, 8, 2, 1, 2, 5, 6, 3, 4, 5, 7, 6, 7, 6, 5, 0, 2, 4, 4, 3, 0, 4, 2, 6, 1, 8, 8, 3, 0, 7, 4, 9, 7, 9, 2, 0, 4, 7, 3, 7, 6, 6, 2, 3, 7, 3, 6, 8, 2, 3, 3, 5, 5, 5, 2, 4, 2, 8, 7, 4, 3, 7, 7, 8, 5, 2, 8, 4, 3, 5, 4, 2, 9, 1, 4, 0, 0, 5, 6, 5, 6, 8, 0, 3, 4, 4, 4, 2, 2, 0, 5, 0, 3, 4, 7, 7, 3, 9, 7, 3, 7, 3, 7, 7, 7, 1, 7, 4, 4, 7, 9, 1, 7, 7, 4, 5, 9, 0, 8, 7, 3, 6, 2, 3, 8, 2, 5, 4, 8, 4, 0, 7, 5, 2, 7, 7, 2, 6, 4, 0, 2, 4, 3, 8, 9, 4, 5, 5, 6, 7, 1, 9, 6, 5, 0, 3, 4, 4, 0, 6, 6, 8, 3, 6, 0, 3, 3, 3, 8, 3, 3, 8, 4, 3, 8, 2, 9, 1, 4, 9, 5, 0, 6, 5, 0, 2, 6, 5, 4, 1, 5, 9, 0, 6, 2, 5, 4, 5, 8, 2, 8, 7, 5, 0, 7, 9, 9, 8, 2, 1, 0, 2, 7, 5, 0, 3, 8, 5, 0, 7, 9, 5, 8, 6, 9, 0, 7, 1, 9, 3, 5, 0, 5, 1, 7, 8, 9, 2, 0, 0, 8, 1, 1, 1, 6, 2, 3, 6, 2, 7, 1, 9, 3, 7, 6, 3, 1, 0, 5, 9, 4, 3, 9, 5, 0, 9, 2, 7, 5, 8, 0, 1, 3, 4, 8, 5, 1, 5, 7, 7, 2, 8, 4, 2, 2, 5, 3, 3, 1, 4, 9, 2, 4, 5, 2, 2, 3, 2, 1, 5, 8, 9, 2, 6, 6, 1, 7, 7, 5, 4, 4, 0, 5, 8, 8, 6, 6, 7, 2, 6, 4, 5, 1, 7, 5, 2, 4, 4, 6, 5, 2, 3, 8, 9, 3, 4, 3, 2, 6, 7, 2, 3, 2, 7, 7, 3, 0, 1, 4, 0, 6, 5, 5, 1, 6, 7, 6, 1, 3, 4, 0, 9, 9, 6, 8, 8, 3, 2, 3, 3, 8, 5, 3, 0, 9, 0, 8, 1, 3, 4, 8, 2, 4, 6, 1, 3, 5, 5, 1, 1, 1, 9, 5, 0, 4, 2, 9, 2, 5, 7, 4, 3, 3, 9, 1, 7, 2, 6, 1, 2, 9, 7, 9, 0, 4, 7, 8, 4, 9, 9, 4, 2, 9, 9, 8, 8, 6, 3, 2, 4, 6, 1, 7, 8, 2, 5, 8, 0, 4, 4, 5, 2, 4, 6, 5, 6, 5, 7, 7, 4, 5, 2, 0, 1, 1, 9, 6, 4, 3, 8, 7, 4, 0, 1, 5, 5, 0, 7, 0, 8, 5, 6, 1, 2, 3, 5, 8, 9, 6, 7, 6, 0, 7, 3, 1, 9, 4, 1, 8, 8, 1, 0, 6, 1, 7, 2, 5, 4, 5, 6, 6, 4, 8, 6, 4, 7, 9, 9, 4, 5, 1, 3, 9, 8, 6, 7, 3, 9, 5, 2, 5, 2, 1, 7, 1, 7, 0, 8, 3, 8, 3, 4, 1, 4, 0, 7, 9, 8, 5, 5, 6, 3, 9, 1, 2, 0, 0, 2, 8, 0, 2, 9, 3, 2, 0, 9, 9, 3, 2, 0, 6, 9, 7, 3, 7, 5, 1, 4, 6, 0, 7, 5, 8, 6, 7, 5, 2, 5, 5, 3, 3, 2, 2, 9, 9, 8, 8, 3, 5, 4, 3, 2, 8, 1, 1, 0, 7, 2, 1, 8, 7, 7, 5, 7, 6, 0, 4, 0, 3, 7, 3, 6, 2, 6, 5, 6, 3, 0, 3, 4, 5, 8, 4, 4, 0, 0, 1, 0, 6, 3, 8, 0, 1, 0, 7, 1, 3, 1, 8, 0, 2, 9, 8, 6, 5, 8, 4, 3, 4, 1, 0, 3, 9, 7, 4, 0, 3, 0, 8, 1, 1, 5, 7, 0, 0, 4, 8, 8, 8, 4, 1, 2, 9, 1, 2, 8, 7, 0, 7, 6, 2, 9, 0, 6, 7, 7, 5, 8, 2, 4, 4, 8, 4, 8, 0, 2, 7, 7, 6, 9, 1, 9, 9, 7, 5, 3, 7, 6, 0, 3, 9, 2, 9, 5, 6, 1, 0, 0, 2, 4, 5, 6, 3, 6, 4, 3, 5, 8, 9, 3, 9, 3, 4, 9, 3, 9, 5, 2, 8, 2, 6, 2, 2, 7, 2, 7, 6, 2, 4, 1, 2, 9, 9, 5, 2, 9, 8, 7, 7, 5, 6, 2, 8, 3, 7, 2, 3, 7, 3, 3, 6, 2, 1, 1, 1, 6, 9, 4, 0, 2, 4, 8, 0, 5, 4, 3, 1, 6, 1, 9, 8, 1, 1, 2, 8, 4, 8, 8, 3, 2, 6, 1, 0, 8, 1, 7, 6, 8, 5, 6, 3, 6, 7, 9, 2, 7, 7, 0, 7, 1, 7, 0, 6, 2, 7, 5, 0, 6, 9, 6, 2, 3, 1, 4, 1, 6, 9, 0, 5, 8, 9, 9, 6, 0, 8, 2, 0, 8, 9, 9, 8, 4, 3, 3, 6, 7, 0, 6, 4, 9, 1, 5, 8, 0, 0, 1, 0, 8, 9, 3, 8, 9, 3, 7, 8, 9, 4, 4, 6, 0, 9, 7, 2, 8, 0, 6, 2, 0, 6, 5, 5, 8, 5, 8, 6, 4, 0, 2, 6, 4, 3, 6, 2, 3, 7, 7, 2, 4, 2, 4, 9, 0, 2, 5, 4, 5, 4, 6, 0, 7, 7, 5, 0, 2, 0, 8, 5, 3, 7, 4, 5, 1, 9, 6, 5, 1, 8, 5, 4, 8, 5, 2, 3, 3, 8, 2, 2, 8, 2, 5, 7, 5, 5, 8, 7, 6, 1, 1, 8, 5, 1, 9, 9, 2, 2, 9, 1, 5, 2, 1, 3, 9, 9, 1, 3, 1, 2, 0, 6, 1, 7, 6, 8, 4, 0, 3, 3, 7, 2, 3, 6, 4, 5, 2, 7, 2, 4, 9, 5, 1, 6, 8, 8, 2, 1, 7, 1, 8, 0, 8, 5, 7, 0, 3, 6, 4, 2, 7, 7, 8, 2, 8, 9, 8, 8, 4, 6, 7, 0, 1, 2, 1, 1, 9, 4, 6, 4, 2, 7, 4, 6, 1, 9, 4, 9, 1, 4, 5, 1, 9, 2, 8, 0, 1, 5, 1, 7, 9, 2, 0, 4, 8, 4, 0, 2, 0, 1, 0, 5, 3, 6, 2, 2, 2, 2, 6, 6, 8, 6, 2, 3, 5, 2, 8, 9, 8, 0, 1, 5, 0, 9, 1, 6, 7, 9, 7, 3, 6, 2, 0, 4, 7, 6, 4, 4, 5, 2, 3, 4, 4, 5, 2, 9, 6, 1, 4, 7, 4, 8, 6, 5, 9, 8, 9, 9, 4, 4, 8, 6, 6, 4, 2, 1, 8, 3, 3, 0, 3, 8, 9, 2, 6, 2, 6, 5, 6, 8, 4, 3, 5, 0, 3, 2, 0, 0, 4, 8, 9, 8, 6, 9, 2, 1, 0, 7, 9, 0, 5, 0, 6, 2, 0, 6, 7, 6, 2, 6, 7, 8, 2, 4, 7, 6, 8, 7, 7, 3, 2, 5, 9, 3, 8, 2, 6, 4, 9, 6, 2, 1, 4, 0, 6, 6, 4, 3, 4, 5, 3, 6, 5, 9, 9, 5, 1, 6, 0, 2, 7, 9, 5, 3, 5, 6, 2, 1, 5, 5, 9, 0, 1, 3, 4, 5, 7, 4, 5, 9, 6, 1, 7, 4, 2, 8, 8, 5, 6, 0, 3, 4, 9, 4, 5, 6, 0, 7, 0, 3, 0, 7, 5, 8, 3, 8, 1, 5, 9, 1, 0, 1, 8, 0, 8, 2, 5, 6, 7, 1, 0, 2, 9, 0, 4, 1, 9, 5, 3, 5, 7, 3, 1, 9, 3, 9, 4, 9, 6, 0, 8, 8, 6, 6, 9, 6, 9, 3, 2, 0, 1, 7, 3, 6, 5, 3, 3, 3, 6, 4, 0, 0, 8, 4, 2, 2, 2, 1, 8, 6, 8, 5, 3, 9, 9, 0, 9, 8, 1, 5, 4, 3, 7, 0, 9, 7, 6, 0, 4, 1, 9, 8, 4, 7, 7, 5, 9, 7, 7, 5, 5, 6, 8, 5, 7, 5, 8, 8, 2, 5, 5, 2, 8, 4, 5, 6, 8, 8, 5, 3, 5, 5, 9, 4, 3, 3, 9, 0, 2, 4, 1, 0, 2, 7, 5, 3, 4, 3, 7, 6, 2, 3, 5, 2, 0, 5, 9, 6, 3, 9, 6, 0, 2, 5, 6, 7, 7, 2, 0, 1, 7, 0, 8, 9, 4, 3, 4, 3, 5, 3, 6, 0, 4, 2, 6, 9, 5, 1, 1, 9, 1, 9, 2, 4, 9, 1, 9, 8, 6, 3, 6, 8, 2, 0, 9, 3, 5, 8, 7, 4, 2, 2, 7, 9, 1, 8, 2, 4, 7, 6, 0, 5, 5, 6, 9, 8, 8, 5, 5, 7, 1, 2, 7, 9, 0, 5, 7, 5, 2, 5, 4, 7, 1, 3, 3, 4, 3, 6, 2, 0, 5, 8, 0, 4, 2, 9, 2, 4, 2, 3, 8, 0, 8, 4, 6, 4, 2, 8, 6, 9, 8, 4, 1, 9, 7, 0, 8, 3, 2, 4, 6, 8, 2, 4, 1, 3, 0, 8, 2, 0, 8, 0, 0, 0, 0, 2, 2, 3, 6, 9, 7, 0, 9, 1, 0, 7, 2, 8, 3, 0, 8, 4, 3, 5, 8, 7, 8, 4, 0, 0, 9, 7, 3, 1, 5, 2, 6, 5, 3, 2, 3, 7, 0, 3, 4, 1, 1, 8, 9, 0, 4, 8, 3, 1, 1, 5, 7, 2, 3, 9, 4, 5, 2, 5, 4, 7, 1, 7, 6, 2, 6, 1, 8, 8, 9, 4, 9, 5, 6, 6, 7, 1, 1, 2, 2, 4, 8, 5, 5, 9, 2, 9, 8, 5, 7, 2, 3, 8, 5, 7, 4, 7, 6, 2, 6, 0, 6, 2, 0, 4, 0, 1, 4, 2, 3, 5, 8, 7, 5, 1, 5, 2, 2, 5, 3, 6, 5, 4, 3, 2, 9, 4, 4, 0, 1, 8, 8, 6, 5, 3, 7, 2, 3, 2, 9, 0, 9, 2, 7, 8, 1, 2, 1, 3, 7, 2, 9, 3, 5, 5, 8, 4, 5, 5, 8, 5, 5, 6, 9, 5, 8, 6, 1, 2, 4, 2, 4, 9, 2, 1, 9, 3, 0, 8, 2, 8, 4, 2, 6, 4, 1, 3, 6, 9, 4, 6, 8, 6, 9, 6, 4, 7, 2, 0, 9, 4, 4, 4, 8, 9, 7, 7, 9, 3, 5, 0, 0, 3, 3, 3, 7, 6, 1, 4, 5, 2, 4, 4, 8, 5, 6, 0, 3, 4, 6, 3, 2, 0, 3, 5, 7, 0, 3, 5, 4, 7, 8, 5, 7, 0, 2, 8, 0, 2, 4, 6, 6, 6, 3, 7, 6, 3, 5, 9, 9, 7, 4, 9, 1, 9, 6, 5, 1, 7, 0, 2, 0, 2, 7, 4, 9, 8, 3, 5, 3, 0, 5, 1, 6, 7, 7, 8, 8, 2, 7, 9, 2, 4, 6, 5, 9, 8, 4, 6, 7, 8, 2, 8, 5, 8, 9, 0, 2, 3, 0, 9, 7, 3, 7, 9, 5, 3, 6, 6, 9, 9, 1, 5, 1, 2, 2, 4, 0, 9, 5, 4, 2, 8, 9, 9, 2, 0, 2, 6, 9, 1, 3, 8, 8, 6, 6, 4, 9, 6, 4, 4, 0, 4, 9, 2, 2, 0, 4, 0, 9, 6, 7, 9, 0, 9, 3, 9, 9, 9, 6, 2, 3, 6, 7, 6, 0, 5, 7, 6, 4, 4, 9, 8, 0, 9, 4, 6, 0, 7, 1, 3, 6, 6, 0, 4, 7, 1, 1, 4, 9, 8, 6, 8, 0, 6, 1, 1, 8, 5, 5, 5, 3, 4, 5, 5, 8, 6, 9, 3, 4, 3, 4, 2, 4, 3, 3, 8, 8, 9, 5, 7, 2, 8, 2, 3, 1, 9, 6, 4, 8, 5, 1, 6, 1, 7, 6, 4, 3, 2, 3, 9, 0, 5, 2, 3, 9, 5, 3, 2, 4, 4, 5, 8, 3, 4, 5, 6, 6, 7, 3, 1, 6, 5, 9, 8, 3, 2, 0, 4, 7, 3, 9, 9, 4, 1, 6, 7, 6, 6, 1, 5, 8, 1, 0, 8, 6, 3, 7, 1, 5, 9, 4, 1, 6, 0, 3, 7, 2, 9, 2, 2, 6, 9, 1, 0, 5, 7, 4, 2, 8, 9, 0, 8, 6, 4, 3, 3, 6, 4, 5, 4, 2, 3, 1, 4, 6, 8, 3, 9, 7, 9, 4, 0, 8, 9, 8, 0, 8, 1, 9, 3, 2, 7, 0, 6, 0, 5, 9, 5, 4, 8, 1, 0, 2, 5, 0, 8, 3, 0, 1, 4, 0, 3, 1, 6, 4, 7, 9, 5, 5, 0, 4, 7, 9, 1, 9, 1, 4, 6, 3, 9, 9, 9, 8, 9, 7, 1, 1, 4, 1, 0, 7, 0, 9, 6, 6, 2, 4, 7, 6, 4, 5, 8, 0, 4, 7, 1, 7, 3, 4, 7, 5, 5, 2, 0, 2, 0, 5, 5, 8, 5, 8, 4, 0, 8, 4, 9, 7, 6, 9, 8, 0, 8, 4, 2, 9, 1, 4, 6, 0, 7, 7, 1, 6, 1, 5, 6, 5, 7, 3, 8, 4, 5, 8, 5, 8, 5, 9, 1, 3, 2, 5, 3, 1, 2, 1, 2, 6, 7, 2, 0, 5, 3, 1, 6, 2, 7, 3, 6, 0, 9, 8, 0, 6, 4, 3, 2, 6, 8, 7, 2, 1, 1, 9, 5, 4, 5, 8, 0, 1, 0, 9, 1, 8, 2, 4, 7, 4, 2, 4, 4, 7, 0, 0, 6, 4, 6, 7, 8, 5, 9, 9, 2, 9, 3, 3, 5, 7, 7, 6, 7, 0, 0, 0, 6, 3, 1, 2, 6, 3, 7, 3, 2, 2, 8, 8, 6, 6, 0, 0, 3, 1, 1, 1, 3, 4, 4, 6, 4, 7, 3, 2, 3, 9, 7, 2, 7, 9, 2, 3, 6, 7, 5, 9, 2, 6, 3, 5, 6, 5, 3, 4, 6, 9, 1, 7, 7, 5, 9, 4, 2, 3, 8, 0, 9, 6, 6, 1, 3, 6, 6, 3, 9, 3, 6, 5, 2, 0, 4, 7, 4, 8, 5, 5, 0, 0, 9, 0, 3, 6, 2, 9, 8, 1, 1, 4, 3, 4, 0, 3, 7, 3, 8, 6, 4, 2, 4, 1, 2, 7, 5, 2, 6, 8, 0, 4, 6, 4, 2, 3, 2, 4, 1, 6, 0, 5, 8, 8, 1, 8, 6, 2, 8, 2, 4, 4, 0, 9, 7, 6, 7, 4, 2, 3, 8, 9, 4, 8, 8, 4, 2, 0, 1, 4, 4, 8, 6, 6, 7, 2, 7, 0, 0, 7, 2, 3, 1, 8, 9, 7, 7, 0, 1, 7, 9, 7, 7, 5, 9, 2, 7, 5, 5, 1, 4, 4, 7, 1, 3, 6, 3, 0, 0, 0, 8, 0, 8, 5, 6, 8, 6, 6, 4, 0, 0, 8, 6, 7, 4, 8, 3, 9, 4, 9, 8, 2, 3, 0, 4, 4, 8, 1, 8, 4, 3, 8, 8, 4, 2, 4, 3, 6, 3, 0, 5, 8, 0, 7, 5, 4, 7, 5, 8, 9, 0, 4, 6, 7, 1, 1, 4, 3, 5, 7, 1, 7, 1, 9, 3, 0, 0, 8, 9, 7, 2, 8, 9, 9, 3, 5, 0, 3, 3, 2, 1, 0, 1, 5, 2, 7, 1, 4, 1, 3, 5, 1, 9, 8, 5, 2, 0, 7, 2, 9, 2, 0, 9, 2, 2, 6, 2, 1, 8, 4, 4, 8, 4, 1, 7, 0, 3, 6, 1, 6, 3, 7, 4, 4, 7, 8, 2, 7, 4, 7, 3, 6, 6, 3, 5, 9, 8, 6, 0, 0, 8, 4, 3, 6, 9, 6, 2, 5, 0, 6, 3, 1, 5, 2, 2, 7, 3, 4, 6, 2, 8, 7, 6, 3, 9, 3, 4, 3, 6, 7, 7, 1, 1, 7, 8, 4, 5, 3, 6, 5, 6, 2, 9, 2, 5, 9, 1, 1, 7, 8, 3, 5, 1, 3, 8, 5, 0, 8, 0, 2, 3, 9, 8, 3, 0, 0, 7, 5, 7, 3, 2, 3, 3, 7, 5, 5, 6, 9, 0, 2, 1, 2, 5, 5, 1, 4, 0, 7, 1, 8, 8, 4, 0, 1, 7, 8, 9, 0, 7, 2, 5, 7, 8, 3, 0, 1, 2, 3, 9, 2, 7, 0, 0, 2, 2, 4, 1, 5, 3, 3, 5, 1, 5, 5, 9, 0, 9, 8, 0, 9, 3, 0, 7, 9, 6, 5, 5, 2, 6, 8, 0, 1, 7, 7, 7, 1, 3, 0, 1, 4, 1, 0, 3, 1, 9, 0, 6, 3, 6, 0, 5, 4, 0, 8, 8, 0, 1, 5, 8, 3, 7, 2, 9, 1, 9, 9, 2, 7, 6, 1, 0, 9, 8, 8, 6, 0, 2, 0, 5, 0, 8, 9, 2, 6, 6, 1, 2, 1, 5, 8, 9, 7, 4, 1, 2, 8, 9, 0, 2, 0, 9, 9, 1, 1, 5, 5, 3, 9, 3, 1, 5, 2, 2, 5, 1, 6, 9, 5, 4, 5, 4, 9, 6, 0, 8, 2, 1, 9, 7, 0, 8, 0, 1, 0, 2, 3, 8, 6, 4, 1, 5, 6, 1, 9, 4, 9, 6, 3, 8, 7, 3, 0, 3, 9, 7, 2, 6, 2, 9, 4, 6, 0, 9, 6, 7, 4, 7, 0, 9, 9, 5, 6, 5, 4, 1, 3, 5, 5, 2, 9, 5, 2, 9, 7, 7, 4, 9, 4, 4, 8, 7, 8, 7, 4, 5, 0, 8, 2, 3, 0, 0, 8, 8, 4, 0, 2, 5, 7, 2, 6, 2, 5, 6, 9, 7, 3, 4, 1, 5, 4, 8, 8, 4, 3, 0, 7, 2, 9, 4, 6, 6, 4, 9, 2, 4, 8, 8, 0, 3, 8, 1, 4, 1, 0, 6, 6, 9, 1, 6, 2, 3, 4, 0, 1, 2, 2, 0, 8, 8, 4, 7, 9, 9, 6, 6, 0, 5, 6, 7, 3, 6, 7, 8, 7, 3, 1, 0, 3, 8, 4, 5, 1, 8, 6, 2, 8, 5, 7, 6, 2, 8, 6, 5, 8, 0, 5, 9, 2, 7, 0, 5, 8, 6, 8, 9, 3, 8, 3, 2, 6, 8, 7, 8, 3, 3, 8, 5, 4, 1, 2, 9, 6, 5, 9, 4, 3, 8, 5, 5, 8, 8, 2, 0, 3, 1, 4, 1, 0, 8, 0, 5, 8, 4, 9, 1, 8, 0, 0, 6, 7, 7, 7, 5, 6, 3, 6, 5, 8, 2, 6, 5, 7, 5, 4, 7, 3, 2, 9, 5, 8, 8, 5, 7, 2, 2, 8, 7, 6, 7, 7, 2, 7, 6, 4, 0, 6, 3, 1, 4, 6, 3, 9, 8, 3, 8, 1, 0, 0, 5, 3, 5, 8, 4, 0, 6, 1, 7, 0, 2, 2, 1, 7, 3, 0, 8, 6, 7, 7, 0, 0, 1, 4, 1, 1, 6, 6, 6, 1, 3, 6, 0, 1, 3, 8, 5, 0, 1, 6, 5, 5, 4, 3, 9, 8, 6, 0, 5, 4, 9, 2, 8, 4, 8, 8, 2, 1, 4, 8, 6, 7, 3, 1, 3, 4, 9, 4, 8, 4, 5, 0, 9, 1, 3, 8, 7, 5, 4, 6, 6, 7, 9, 0, 5, 2, 3, 3, 3, 9, 0, 9, 2, 9, 1, 0, 2, 3, 9, 6, 6, 1, 6, 3, 7, 6, 2, 7, 0, 4, 8, 9, 6, 0, 7, 7, 5, 0, 4, 5, 9, 6, 4, 5, 7, 9, 3, 7, 8, 5, 0, 4, 1, 3, 8, 6, 9, 5, 7, 6, 7, 7, 3, 4, 2, 3, 6, 4, 5, 0, 1, 9, 3, 8, 4, 4, 1, 9, 8, 9, 3, 6, 0, 9, 1, 6, 3, 2, 3, 0, 5, 8, 8, 0, 0, 6, 2, 3, 5, 1, 5, 7, 1, 0, 9, 8, 3, 3, 5, 6, 3, 2, 1, 3, 7, 0, 7, 7, 2, 4, 7, 7, 6, 2, 6, 1, 7, 2, 3, 3, 0, 2, 2, 9, 6, 5, 5, 1, 9, 7, 9, 4, 6, 4, 1, 9, 1, 3, 6, 1, 2, 2, 5, 8, 0, 8, 5, 1, 7, 6, 7, 9, 3, 0, 9, 2, 2, 6, 7, 6, 9, 1, 9, 4, 0, 5, 6, 6, 4, 4, 4, 1, 6, 3, 2, 8, 9, 4, 7, 2, 3, 9, 3, 4, 0, 1, 1, 1, 9, 9, 7, 5, 1, 9, 2, 5, 6, 1, 5, 3, 0, 1, 4, 9, 4, 2, 0, 8, 3, 2, 7, 3, 8, 6, 1, 4, 2, 4, 9, 4, 6, 9, 1, 5, 9, 9, 1, 4, 2, 7, 2, 7, 3, 1, 9, 8, 8, 1, 2, 0, 8, 8, 2, 8, 7, 8, 4, 1, 7, 4, 3, 1, 8, 6, 4, 5, 0, 5, 6, 7, 5, 9, 8, 8, 8, 9, 7, 3, 8, 7, 8, 2, 1, 8, 8, 4, 9, 1, 5, 4, 2, 6, 6, 9, 2, 7, 2, 8, 4, 2, 9, 1, 6, 5, 8, 5, 9, 7, 6, 2, 9, 1, 8, 7, 0, 9, 9, 7, 6, 3, 9, 3, 0, 0, 0, 7, 7, 0, 7, 3, 9, 0, 9, 2, 1, 5, 7, 1, 6, 8, 2, 0, 4, 2, 1, 4, 0, 4, 8, 0, 4, 9, 9, 6, 3, 6, 1, 7, 4, 6, 8, 6, 0, 7, 4, 8, 7, 5, 7, 5, 2, 0, 5, 8, 6, 6, 6, 8, 8, 5, 6, 6, 9, 9, 2, 7, 2, 1, 9, 3, 9, 0, 1, 5, 9, 9, 7, 9, 6, 5, 3, 7, 6, 7, 0, 4, 2, 3, 9, 6, 2, 5, 7, 5, 0, 2, 8, 2, 9, 6, 1, 5, 1, 3, 6, 3, 0, 8, 0, 6, 3, 5, 0, 1, 4, 6, 7, 9, 1, 5, 4, 0, 4, 6, 2, 3, 8, 7, 5, 7, 3, 4, 8, 9, 8, 3, 6, 0, 5, 1, 1, 9, 4, 8, 9, 9, 9, 5, 0, 8, 0, 2, 0, 3, 6, 3, 9, 0, 3, 3, 8, 0, 8, 8, 0, 9, 4, 8, 4, 6, 4, 4, 3, 3, 0, 4, 9, 6, 7, 9, 5, 4, 3, 5, 5, 5, 2, 3, 6, 5, 5, 9, 1, 6, 4, 1, 0, 5, 7, 9, 3, 0, 6, 8, 9, 7, 6, 2, 8, 0, 0, 2, 9, 0, 3, 2, 7, 2, 7, 3, 2, 3, 7, 7, 5, 3, 8, 5, 4, 0, 2, 9, 1, 3, 2, 5, 8, 0, 6, 1, 7, 5, 9, 7, 7, 5, 0, 1, 1, 9, 4, 2, 1, 8, 2, 2, 2, 7, 3, 7, 0, 4, 6, 6, 2, 9, 3, 0, 6, 0, 1, 9, 3, 5, 4, 9, 6, 7, 3, 6, 8, 3, 9, 2, 0, 9, 5, 4, 7, 2, 3, 9, 9, 5, 0, 3, 1, 7, 2, 4, 7, 9, 9, 0, 4, 4, 5, 1, 7, 1, 8, 4, 1, 4, 2, 1, 6, 9, 2, 2, 0, 8, 0, 8, 8, 5, 5, 2, 8, 0, 9, 5, 7, 4, 9, 5, 2, 0, 8, 0, 2, 6, 9, 1, 3, 0, 1, 8, 9, 4, 5, 9, 8, 0, 7, 8, 4, 4, 3, 6, 6, 5, 6, 1, 0, 1, 2, 8, 9, 6, 5, 9, 9, 9, 6, 5, 0, 9, 5, 0, 5, 8, 0, 0, 4, 3, 4, 2, 6, 1, 7, 6, 2, 3, 1, 2, 8, 2, 6, 0, 3, 4, 1, 6, 2, 3, 4, 2, 6, 5, 8, 9, 8, 3, 5, 7, 5, 7, 0, 4, 6, 3, 4, 0, 3, 6, 2, 1, 7, 5, 8, 5, 8, 7, 4, 5, 4, 0, 2, 1, 5, 7, 2, 6, 8, 0, 7, 7, 4, 1, 0, 7, 9, 2, 7, 0, 9, 8, 7, 7, 5, 2, 7, 9, 7, 6, 4, 8, 4, 6, 4, 1, 7, 9, 8, 8, 6, 3, 7, 3, 3, 9, 7, 0, 7, 1, 8, 8, 5, 7, 1, 1, 7, 4, 0, 4, 1, 2, 2, 9, 8, 7, 1, 3, 2, 2, 4, 1, 0, 4, 5, 1, 5, 6, 1, 8, 2, 5, 3, 9, 5, 1, 7, 0, 4, 6, 7, 2, 3, 5, 5, 9, 0, 5, 7, 8, 3, 6, 0, 3, 1, 9, 2, 7, 0, 9, 8, 2, 9, 8, 3, 2, 5, 0, 0, 8, 3, 3, 6, 6, 4, 9, 6, 0, 9, 1, 5, 7, 0, 2, 7, 8, 2, 9, 4, 1, 3, 1, 6, 5, 6, 2, 4, 2, 8, 5, 8, 6, 4, 4, 8, 8, 6, 2, 3, 3, 8, 1, 3, 4, 5, 6, 2, 9, 0, 7, 2, 7, 0, 8, 7, 4, 8, 2, 7, 7, 7, 9, 9, 1, 4, 8, 3, 7, 4, 3, 4, 1, 8, 8, 4, 1, 0, 8, 0, 0, 0, 4, 7, 1, 5, 5, 5, 3, 6, 5, 3, 2, 9, 0, 5, 1, 0, 0, 9, 1, 7, 8, 9, 2, 3, 5, 7, 1, 0, 8, 8, 8, 0, 7, 2, 7, 2, 5, 0, 7, 7, 9, 4, 8, 6, 5, 9, 6, 8, 6, 9, 4, 0, 4, 7, 2, 9, 0, 3, 0, 9, 5, 2, 2, 1, 4, 4, 2, 5, 6, 2, 4, 2, 5, 3, 1, 7, 5, 9, 6, 9, 5, 9, 4, 1, 2, 5, 9, 8, 3, 6, 5, 5, 4, 8, 7, 9, 5, 0, 8, 4, 9, 5, 7, 3, 3, 0, 2, 1, 5, 9, 0, 1, 2, 3, 3, 7, 2, 1, 0, 7, 7, 1, 3, 4, 2, 5, 9, 4, 0, 8, 9, 5, 3, 0, 5, 5, 6, 5, 6, 6, 2, 0, 8, 5, 6, 5, 4, 5, 5, 2, 6, 3, 1, 4, 9, 0, 0, 9, 4, 9, 7, 6, 7, 1, 1, 1, 8, 0, 6, 2, 6, 4, 9, 1, 5, 0, 2, 8, 3, 1, 1, 8, 8, 5, 1, 0, 8, 4, 6, 2, 7, 6, 9, 9, 3, 9, 8, 5, 5, 2, 7, 9, 8, 8, 1, 3, 9, 7, 0, 8, 3, 4, 3, 0, 9, 6, 4, 9, 4, 8, 2, 9, 6, 4, 4, 8, 9, 5, 3, 3, 0, 0, 7, 0, 1, 8, 6, 7, 9, 1, 7, 3, 3, 3, 9, 5, 4, 4, 3, 4, 3, 3, 2, 2, 4, 4, 6, 4, 5, 7, 3, 7, 8, 5, 0, 5, 3, 6, 8, 1, 2, 2, 3, 2, 1, 5, 1, 4, 7, 2, 9, 8, 7, 0, 1, 0, 4, 1, 9, 6, 8, 5, 4, 8, 4, 6, 5, 6, 7, 3, 7, 9, 5, 0, 7, 6, 3, 9, 2, 4, 6, 9, 1, 4, 7, 1, 6, 4, 9, 0, 6, 7, 5, 4, 4, 9, 7, 3, 1, 9, 7, 9, 4, 6, 7, 4, 9, 6, 1, 2, 6, 4, 7, 6, 7, 1, 4, 3, 2, 3, 7, 0, 3, 9, 4, 0, 0, 4, 2, 8, 2, 1, 3, 7, 7, 8, 2, 3, 1, 7, 2, 0, 7, 3, 4, 3, 5, 2, 7, 6, 8, 5, 4, 5, 2, 8, 3, 4, 5, 8, 1, 3, 1, 6, 3, 5, 1, 0, 5, 1, 9, 5, 2, 9, 4, 2, 7, 9, 9, 1, 6, 3, 2, 0, 1, 7, 0, 0, 1, 0, 5, 4, 8, 0, 9, 0, 1, 9, 3, 7, 4, 2, 4, 2, 6, 2, 0, 3, 6, 0, 8, 7, 0, 0, 7, 3, 8, 3, 3, 6, 8, 2, 7, 4, 1, 2, 9, 2, 4, 6, 6, 8, 5, 1, 4, 3, 7, 5, 0, 9, 2, 0, 1, 3, 9, 4, 3, 1, 8, 5, 4, 9, 9, 0, 7, 6, 5, 5, 2, 1, 6, 2, 7, 4, 6, 4, 1, 6, 5, 1, 9, 3, 8, 7, 2, 3, 6, 1, 4, 3, 2, 8, 2, 8, 6, 4, 6, 8, 4, 5, 2, 8, 0, 9, 7, 3, 5, 2, 4, 6, 7, 2, 1, 3, 3, 3, 3, 1, 1, 3, 7, 4, 2, 6, 8, 2, 0, 0, 4, 6, 3, 8, 4, 6, 3, 0, 7, 8, 1, 9, 6, 0, 5, 2, 6, 7, 6, 4, 9, 6, 4, 7, 8, 5, 9, 4, 5, 1, 4, 6, 6, 7, 9, 3, 8, 2, 0, 1, 6, 9, 2, 6, 2, 1, 6, 7, 0, 6, 3, 3, 3, 2, 1, 4, 5, 4, 0, 7, 8, 6, 3, 7, 4, 0, 9, 6, 7, 3, 5, 8, 5, 6, 9, 5, 6, 6, 2, 3, 3, 4, 9, 7, 1, 3, 4, 6, 2, 4, 3, 3, 0, 8, 6, 7, 7, 3, 4, 5, 4, 2, 3, 1, 0, 9, 4, 4, 4, 2, 5, 2, 0, 3, 3, 6, 2, 5, 1, 7, 9, 2, 8, 2, 0, 1, 8, 8, 7, 2, 3, 4, 3, 2, 1, 0, 7, 5, 9, 9, 6, 8, 6, 2, 3, 0, 8, 4, 7, 4, 7, 1, 9, 3, 1, 2, 1, 9, 0, 6, 0, 9, 8, 1, 3, 1, 9, 6, 1, 3, 5, 7, 7, 1, 8, 7, 4, 0, 1, 3, 2, 6, 0, 0, 5, 3, 2, 9, 9, 5, 7, 7, 5, 5, 7, 3, 1, 0, 1, 2, 9, 9, 5, 9, 1, 4, 0, 6, 7, 3, 7, 3, 8, 8, 0, 4, 6, 6, 5, 3, 1, 7, 9, 8, 3, 5, 1, 3, 1, 7, 0, 1, 1, 2, 7, 8, 6, 3, 0, 2, 6, 9, 6, 7, 6, 6, 1, 7, 3, 6, 6, 8, 2, 3, 7, 5, 0, 8, 7, 7, 1, 9, 3, 6, 1, 4, 5, 2, 8, 2, 8, 7, 6, 5, 1, 4, 5, 3, 3, 9, 4, 1, 3, 8, 8, 3, 8, 7, 7, 5, 6, 8, 5, 1, 2, 7, 2, 2, 8, 6, 2, 8, 5, 7, 4, 6, 9, 3, 1, 1, 5, 2, 8, 7, 1, 1, 7, 7, 8, 6, 8, 6, 7, 1, 6, 8, 1, 6, 0, 9, 9, 2, 3, 4, 7, 4, 7, 7, 2, 4, 9, 8, 3, 8, 3, 7, 1, 4, 5, 6, 3, 2, 2, 0, 6, 0, 9, 8, 4, 7, 7, 5, 0, 3, 1, 9, 1, 3, 5, 5, 4, 7, 4, 5, 4, 8, 8, 8, 3, 2, 1, 2, 9, 6, 1, 9, 4, 7, 6, 4, 1, 1, 4, 8, 1, 9, 2, 3, 9, 0, 3, 9, 4, 1, 8, 3, 4, 8, 3, 3, 1, 8, 2, 6, 0, 0, 2, 1, 0, 4, 6, 6, 2, 1, 0, 3, 1, 6, 1, 9, 6, 3, 9, 6, 7, 4, 6, 8, 4, 2, 3, 2, 0, 8, 6, 4, 9, 0, 7, 0, 1, 2, 5, 5, 1, 6, 3, 6, 2, 8, 5, 7, 1, 6, 1, 2, 1, 0, 0, 8, 8, 2, 8, 5, 7, 3, 3, 5, 4, 9, 4, 7, 6, 1, 8, 4, 9, 7, 4, 3, 8, 2, 9, 7, 7, 5, 6, 0, 6, 3, 1, 3, 8, 2, 2, 9, 7, 8, 8, 0, 0, 7, 5, 1, 1, 6, 5, 8, 7, 4, 6, 2, 7, 7, 8, 7, 5, 5, 7, 6, 4, 6, 2, 2, 8, 9, 6, 9, 4, 6, 8, 8, 2, 1, 3, 3, 8, 4, 0, 4, 7, 2, 5, 4, 4, 0, 8, 4, 3, 6, 3, 8, 8, 2, 9, 6, 9, 5, 6, 2, 3, 7, 1, 4, 7, 9, 0, 5, 8, 6, 7, 3, 1, 7, 2, 5, 6, 6, 0, 5, 0, 8, 4, 2, 4, 4, 7, 1, 5, 8, 5, 2, 3, 8, 8, 4, 9, 8, 5, 9, 5, 2, 5, 8, 4, 7, 3, 3, 3, 1, 3, 8, 9, 3, 8, 1, 8, 3, 6, 9, 2, 2, 6, 3, 5, 5, 7, 3, 6, 0, 1, 2, 1, 1, 8, 5, 2, 7, 3, 4, 4, 8, 5, 5, 6, 5, 3, 5, 4, 5, 2, 2, 9, 0, 8, 9, 3, 6, 1, 0, 6, 1, 1, 2, 0, 3, 5, 7, 0, 2, 7, 7, 0, 4, 5, 6, 7, 5, 7, 2, 5, 0, 7, 1, 9, 3, 7, 6, 0, 7, 3, 4, 4, 5, 2, 9, 2, 0, 3, 1, 6, 2, 2, 4, 7, 3, 1, 8, 7, 3, 0, 6, 5, 3, 5, 3, 8, 6, 8, 3, 3, 5, 8, 1, 6, 9, 5, 6, 0, 9, 9, 5, 6, 8, 9, 6, 0, 6, 3, 6, 1, 0, 6, 2, 3, 5, 1, 9, 0, 6, 6, 5, 6, 3, 4, 5, 1, 6, 7, 4, 2, 9, 6, 6, 2, 5, 8, 8, 4, 1, 7, 1, 8, 1, 1, 3, 7, 9, 5, 8, 4, 4, 1, 2, 0, 1, 4, 9, 2, 7, 5, 8, 3, 2, 3, 8, 0, 1, 3, 7, 6, 6, 4, 3, 5, 0, 6, 4, 7, 1, 7, 1, 0, 0, 8, 2, 7, 2, 7, 8, 6, 2, 9, 1, 6, 2, 6, 1, 3, 0, 5, 3, 0, 6, 1, 1, 5, 1, 7, 5, 3, 4, 1, 1, 4, 0, 8, 4, 2, 5, 4, 8, 3, 4, 3, 1, 1, 7, 9, 1, 1, 8, 0, 1, 1, 2, 1, 4, 4, 8, 7, 4, 8, 6, 3, 1, 9, 6, 6, 4, 4, 0, 2, 5, 9, 7, 1, 7, 6, 4, 7, 5, 4, 1, 0, 2, 3, 3, 8, 9, 9, 7, 5, 9, 9, 7, 9, 6, 0, 4, 5, 3, 3, 1, 5, 1, 8, 3, 8, 0, 5, 4, 8, 1, 3, 8, 9, 1, 8, 0, 0, 5, 8, 1, 2, 7, 8, 2, 9, 5, 8, 9, 9, 6, 4, 3, 9, 0, 7, 0, 2, 2, 3, 8, 3, 0, 9, 5, 6, 3, 1, 5, 2, 3, 9, 2, 6, 7, 3, 8, 3, 6, 3, 7, 3, 0, 6, 3, 3, 9, 1, 8, 8, 3, 8, 3, 3, 0, 1, 0, 0, 7, 8, 8, 0, 5, 0, 9, 1, 8, 5, 6, 3, 5, 3, 0, 2, 2, 8, 6, 2, 9, 0, 0, 1, 0, 4, 3, 3, 8, 8, 4, 0, 6, 3, 1, 6, 2, 0, 7, 5, 7, 0, 1, 6, 0, 3, 2, 6, 9, 2, 9, 4, 7, 4, 5, 8, 5, 4, 7, 7, 3, 3, 7, 8, 9, 9, 1, 0, 4, 6, 2, 7, 5, 1, 8, 0, 7, 9, 3, 4, 2, 1, 0, 2, 5, 6, 0, 3, 9, 4, 2, 1, 7, 0, 1, 5, 4, 3, 7, 1, 7, 1, 9, 7, 8, 7, 0, 7, 8, 3, 2, 8, 7, 4, 5, 7, 1, 4, 7, 4, 4, 8, 9, 9, 8, 0, 6, 2, 4, 2, 4, 7, 6, 8, 1, 7, 4, 0, 2, 9, 8, 0, 6, 1, 6, 3, 9, 2, 0, 0, 4, 9, 2, 1, 0, 2, 2, 0, 2, 3, 7, 3, 8, 6, 4, 3, 7, 1, 0, 6, 4, 5, 8, 5, 3, 3, 7, 1, 7, 5, 7, 9, 2, 3, 4, 9, 9, 8, 9, 8, 2, 5, 9, 5, 0, 6, 8, 9, 9, 6, 1, 1, 9, 1, 6, 9, 4, 2, 2, 6, 8, 7, 1, 3, 6, 1, 1, 1, 5, 7, 5, 0, 1, 5, 8, 0, 6, 4, 3, 1, 7, 8, 1, 2, 1, 2, 5, 4, 8, 2, 9, 0, 8, 3, 5, 4, 9, 3, 6, 5, 7, 0, 7, 9, 9, 2, 6, 9, 9, 3, 2, 4, 3, 5, 0, 3, 0, 5, 1, 4, 5, 1, 1, 5, 5, 7, 0, 2, 0, 5, 9, 4, 0, 3, 8, 9, 0, 9, 8, 6, 1, 2, 7, 6, 7, 0, 3, 3, 3, 0, 6, 2, 0, 7, 0, 4, 3, 7, 3, 5, 7, 7, 4, 7, 6, 1, 4, 6, 6, 6, 0, 1, 4, 7, 7, 7, 1, 7, 8, 3, 7, 3, 9, 3, 2, 1, 0, 6, 2, 7, 5, 0, 6, 7, 2, 4, 3, 2, 9, 4, 9, 8, 2, 0, 6, 1, 1, 7, 5, 0, 6, 6, 2, 3, 5, 8, 3, 5, 7, 1, 8, 2, 2, 1, 4, 6, 2, 0, 1, 7, 8, 1, 2, 0, 2, 9, 9, 3, 7, 7, 4, 2, 0, 6, 8, 5, 9, 4, 9, 2, 3, 0, 5, 3, 4, 6, 1, 1, 0, 1, 6, 9, 6, 2, 9, 5, 7, 7, 3, 0, 5, 6, 3, 2, 0, 3, 3, 4, 8, 5, 1, 6, 5, 7, 7, 8, 2, 9, 9, 7, 3, 8, 2, 8, 6, 2, 2, 1, 4, 7, 4, 4, 6, 6, 9, 7, 2, 4, 6, 0, 8, 6, 6, 7, 0, 0, 0, 8, 2, 6, 6, 1, 5, 0, 9, 5, 6, 1, 4, 0, 7, 3, 1, 7, 9, 0, 1, 6, 6, 8, 8, 3, 0, 3, 3, 1, 6, 9, 4, 8, 8, 8, 6, 6, 2, 3, 9, 3, 8, 6, 2, 2, 0, 2, 5, 6, 8, 0, 2, 2, 8, 8, 3, 3, 3, 8, 2, 6, 6, 1, 7, 2, 7, 5, 5, 3, 8, 2, 0, 8, 4, 4, 8, 4, 5, 5, 7, 6, 3, 2, 3, 8, 1, 1, 6, 9, 0, 6, 7, 2, 4, 3, 4, 6, 8, 6, 0, 8, 8, 9, 7, 2, 7, 0, 4, 2, 3, 0, 6, 1, 0, 9, 3, 9, 1, 6, 2, 1, 1, 1, 0, 0, 0, 0, 2, 3, 0, 9, 3, 3, 7, 1, 6, 7, 9, 8, 2, 8, 4, 7, 0, 4, 0, 1, 8, 2, 3, 8, 1, 9, 9, 1, 6, 7, 1, 4, 0, 3, 5, 1, 0, 4, 8, 3, 5, 9, 7, 4, 2, 9, 9, 0, 2, 1, 8, 3, 9, 3, 5, 0, 5, 2, 3, 9, 0, 3, 3, 8, 3, 2, 2, 5, 8, 2, 6, 6, 2, 2, 1, 1, 7, 4, 2, 3, 8, 1, 2, 3, 4, 0, 2, 7, 5, 1, 0, 1, 2, 3, 6, 2, 5, 1, 5, 0, 0, 1, 7, 7, 5, 5, 6, 0, 4, 0, 3, 8, 6, 9, 8, 6, 1, 0, 2, 2, 6, 1, 8, 5, 3, 1, 9, 1, 1, 8, 5, 6, 1, 4, 2, 8, 4, 2, 7, 5, 5, 8, 0, 5, 1, 9, 3, 4, 8, 0, 8, 3, 5, 6, 4, 3, 6, 4, 0, 3, 4, 5, 3, 4, 2, 5, 3, 8, 0, 5, 8, 4, 0, 8, 5, 2, 5, 7, 8, 4, 2, 9, 5, 8, 8, 3, 3, 5, 3, 4, 9, 5, 0, 6, 1, 9, 0, 0, 9, 7, 1, 8, 3, 9, 2, 1, 7, 1, 9, 6, 9, 7, 7, 0, 2, 7, 7, 9, 9, 3, 5, 2, 2, 9, 2, 5, 5, 9, 5, 8, 3, 4, 1, 2, 5, 6, 3, 8, 4, 1, 2, 2, 0, 7, 6, 7, 1, 2, 8, 3, 1, 7, 4, 5, 9, 0, 3, 7, 8, 0, 7, 4, 2, 6, 9, 0, 2, 7, 4, 7, 3, 1, 7, 5, 1, 4, 0, 2, 4, 4, 3, 3, 0, 0, 8, 3, 3, 0, 9, 1, 7, 7, 0, 6, 9, 2, 4, 8, 5, 8, 5, 8, 9, 4, 4, 4, 8, 5, 3, 7, 0, 2, 5, 9, 8, 4, 9, 4, 2, 1, 3, 0, 3, 4, 8, 0, 1, 1, 9, 8, 7, 6, 1, 6, 3, 9, 6, 0, 2, 9, 2, 9, 6, 0, 0, 8, 6, 4, 9, 1, 1, 0, 5, 0, 2, 8, 4, 5, 9, 2, 6, 7, 4, 5, 5, 7, 6, 3, 8, 7, 9, 8, 4, 4, 6, 1, 1, 2, 0, 9, 1, 7, 2, 8, 8, 2, 1, 4, 4, 8, 4, 4, 6, 9, 5, 8, 5, 5, 7, 7, 5, 9, 5, 3, 4, 9, 9, 3, 7, 6, 9, 5, 8, 6, 6, 0, 7, 9, 2, 8, 3, 7, 0, 4, 0, 8, 8, 6, 2, 1, 2, 8, 9, 7, 2, 0, 2, 7, 6, 9, 1, 4, 8, 2, 8, 6, 7, 1, 1, 0, 1, 4, 6, 0, 4, 7, 1, 6, 0, 1, 9, 9, 8, 1, 6, 9, 1, 9, 9, 0, 1, 1, 7, 1, 8, 9, 1, 0, 0, 5, 7, 4, 4, 6, 5, 2, 5, 9, 8, 6, 5, 5, 3, 7, 8, 7, 7, 6, 3, 3, 1, 5, 9, 6, 7, 7, 3, 1, 7, 3, 2, 6, 6, 9, 8, 7, 2, 6, 8, 7, 7, 5, 1, 0, 7, 1, 7, 2, 1, 3, 4, 4, 9, 6, 2, 3, 8, 8, 0, 4, 7, 2, 7, 7, 9, 3, 7, 6, 8, 3, 9, 6, 4, 7, 4, 0, 3, 8, 9, 7, 0, 0, 5, 0, 7, 8, 8, 1, 8, 2, 9, 5, 8, 9, 5, 5, 2, 4, 8, 6, 3, 7, 2, 5, 5, 0, 3, 7, 4, 8, 1, 5, 1, 1, 7, 0, 1, 6, 7, 6, 8, 4, 0, 6, 6, 4, 2, 8, 7, 6, 3, 3, 7, 2, 3, 8, 9, 8, 7, 7, 4, 8, 3, 8, 7, 9, 2, 9, 9, 1, 5, 5, 4, 0, 6, 1, 2, 6, 3, 3, 2, 0, 5, 9, 7, 7, 0, 5, 0, 9, 2, 7, 7, 0, 9, 5, 5, 1, 9, 0, 3, 3, 9, 6, 7, 4, 8, 2, 5, 8, 2, 1, 4, 9, 0, 8, 2, 4, 0, 1, 2, 3, 0, 9, 4, 8, 3, 8, 9, 2, 3, 1, 4, 5, 1, 5, 1, 0, 2, 5, 2, 1, 8, 6, 4, 5, 0, 9, 3, 1, 0, 9, 1, 0, 7, 6, 6, 7, 7, 2, 6, 9, 3, 4, 5, 1, 9, 6, 6, 4, 8, 8, 6, 0, 4, 9, 5, 9, 9, 3, 9, 9, 0, 2, 2, 7, 3, 5, 1, 0, 9, 5, 8, 0, 8, 1, 1, 0, 0, 7, 4, 8, 2, 3, 3, 2, 5, 6, 0, 1, 8, 2, 6, 6, 2, 1, 3, 6, 5, 8, 3, 2, 0, 8, 1, 7, 9, 7, 2, 0, 4, 6, 8, 1, 5, 7, 9, 7, 0, 4, 1, 7, 5, 8, 7, 4, 8, 7, 3, 7, 4, 4, 0, 4, 7, 4, 3, 5, 3, 8, 1, 7, 8, 5, 7, 0, 8, 7, 5, 5, 4, 7, 1, 9, 0, 5, 2, 6, 1, 4, 8, 0, 6, 3, 1, 9, 7, 3, 5, 6, 6, 9, 8, 0, 5, 9, 6, 2, 6, 9, 5, 8, 3, 0, 5, 1, 7, 2, 8, 0, 4, 0, 8, 1, 3, 9, 7, 1, 3, 6, 7, 4, 8, 6, 7, 5, 3, 4, 3, 5, 2, 6, 9, 5, 0, 1, 7, 4, 8, 9, 4, 2, 2, 9, 0, 3, 3, 1, 3, 0, 4, 2, 1, 4, 0, 8, 2, 1, 4, 4, 3, 6, 6, 8, 3, 9, 9, 9, 9, 6, 5, 0, 8, 2, 3, 5, 9, 8, 2, 8, 3, 4, 1, 1, 7, 6, 9, 2, 1, 4, 6, 3, 3, 2, 1, 8, 1, 0, 0, 0, 3, 4, 0, 8, 0, 9, 1, 2, 8, 0, 1, 7, 7, 3, 4, 7, 8, 0, 2, 7, 1, 3, 7, 4, 6, 1, 7, 6, 1, 2, 6, 9, 8, 2, 0, 1, 0, 9, 6, 5, 3, 2, 4, 6, 5, 4, 9, 3, 8, 8, 7, 3, 6, 1, 2, 9, 8, 8, 1, 0, 3, 0, 8, 0, 0, 6, 6, 1, 3, 3, 6, 6, 5, 7, 5, 3, 8, 2, 1, 9, 0, 4, 6, 9, 2, 3, 9, 4, 1, 4, 2, 2, 3, 4, 8, 4, 3, 0, 6, 8, 8, 6, 2, 3, 3, 7, 3, 5, 2, 9, 6, 0, 2, 3, 5, 2, 4, 9, 5, 1, 6, 0, 3, 3, 7, 3, 7, 9, 8, 1, 5, 2, 8, 0, 2, 9, 5, 6, 6, 2, 6, 7, 4, 2, 4, 5, 8, 7, 6, 1, 1, 9, 5, 9, 7, 2, 1, 2, 9, 6, 8, 0, 3, 7, 7, 7, 8, 2, 3, 9, 0, 4, 1, 6, 4, 5, 0, 6, 6, 7, 2, 6, 9, 7, 9, 6, 6, 1, 3, 2, 4, 1, 1, 0, 4, 2, 6, 0, 8, 4, 4, 9, 8, 1, 5, 8, 1, 1, 8, 8, 9, 7, 2, 4, 5, 6, 4, 9, 6, 7, 5, 0, 8, 3, 5, 4, 5, 0, 5, 0, 2, 3, 0, 3, 7, 5, 0, 2, 3, 2, 3, 5, 9, 6, 3, 3, 8, 3, 9, 1, 7, 3, 0, 2, 0, 7, 5, 1, 1, 2, 6, 6, 1, 6, 8, 0, 8, 1, 0, 7, 2, 7, 3, 2, 3, 1, 9, 5, 3, 6, 9, 2, 4, 3, 8, 3, 8, 7, 2, 4, 2, 8, 4, 4, 2, 5, 6, 3, 9, 4, 6, 7, 0, 4, 4, 1, 8, 6, 2, 7, 4, 3, 1, 0, 7, 0, 9, 5, 8, 0, 1, 6, 9, 5, 0, 0, 2, 8, 5, 4, 0, 5, 1, 3, 3, 4, 9, 5, 3, 3, 1, 0, 1, 2, 2, 3, 6, 1, 2, 7, 8, 1, 9, 0, 6, 8, 6, 0, 8, 9, 2, 3, 7, 6, 2, 9, 8, 8, 6, 3, 0, 6, 9, 5, 9, 0, 7, 2, 0, 1, 2, 4, 1, 7, 5, 5, 2, 9, 2, 0, 5, 7, 4, 7, 7, 2, 7, 6, 5, 1, 5, 9, 7, 2, 0, 8, 9, 8, 6, 8, 5, 0, 7, 7, 8, 2, 1, 6, 1, 3, 1, 6, 6, 8, 2, 3, 7, 9, 8, 3, 8, 5, 7, 0, 1, 0, 7, 4, 2, 0, 4, 9, 4, 2, 7, 7, 4, 4, 1, 1, 0, 9, 0, 9, 8, 6, 4, 7, 2, 2, 2, 6, 0, 2, 1, 0, 2, 8, 4, 4, 6, 5, 9, 0, 1, 1, 8, 5, 1, 1, 8, 9, 0, 6, 4, 8, 4, 5, 7, 1, 4, 6, 2, 2, 6, 8, 2, 9, 2, 2, 7, 1, 4, 5, 3, 1, 0, 8, 4, 4, 3, 7, 8, 9, 3, 6, 1, 2, 1, 2, 2, 7, 1, 1, 1, 2, 5, 7, 2, 2, 7, 3, 4, 4, 5, 1, 0, 2, 9, 8, 3, 9, 1, 2, 0, 5, 9, 5, 9, 3, 1, 0, 3, 3, 0, 4, 0, 8, 4, 0, 1, 7, 5, 8, 3, 6, 9, 8, 7, 2, 8, 2, 3, 4, 5, 2, 6, 7, 4, 2, 7, 5, 5, 4, 8, 8, 1, 3, 8, 5, 4, 8, 9, 6, 9, 4, 3, 0, 1, 2, 2, 4, 6, 8, 8, 5, 0, 3, 1, 8, 3, 0, 7, 9, 8, 8, 9, 5, 2, 6, 4, 6, 8, 9, 8, 4, 8, 0, 5, 7, 1, 4, 8, 8, 3, 4, 0, 0, 5, 8, 3, 9, 3, 5, 9, 9, 6, 5, 8, 6, 3, 7, 2, 4, 0, 9, 2, 1, 0, 7, 3, 9, 8, 8, 4, 7, 7, 6, 9, 5, 8, 4, 6, 2, 2, 7, 6, 3, 3, 9, 2, 7, 6, 8, 1, 1, 5, 8, 5, 4, 4, 0, 3, 6, 1, 0, 6, 2, 6, 0, 3, 4, 6, 2, 6, 1, 1, 0, 7, 6, 5, 4, 5, 7, 1, 4, 4, 4, 7, 4, 4, 1, 4, 3, 6, 6, 8, 9, 6, 7, 2, 4, 3, 5, 3, 7, 8, 6, 2, 0, 0, 0, 2, 2, 5, 7, 1, 9, 6, 1, 9, 5, 9, 0, 6, 5, 4, 3, 8, 0, 6, 7, 1, 7, 4, 9, 8, 2, 6, 7, 3, 0, 5, 6, 7, 5, 7, 1, 6, 6, 4, 3, 3, 9, 8, 4, 3, 6, 4, 1, 6, 7, 4, 0, 1, 6, 3, 1, 2, 0, 0, 8, 6, 1, 6, 3, 8, 5, 0, 9, 1, 5, 4, 4, 0, 5, 2, 6, 1, 5, 9, 0, 8, 8, 1, 4, 9, 4, 4, 1, 0, 7, 3, 9, 1, 0, 2, 3, 1, 5, 2, 6, 9, 2, 3, 0, 7, 4, 3, 3, 0, 9, 3, 8, 3, 4, 2, 2, 1, 9, 0, 2, 8, 6, 6, 0, 3, 3, 6, 5, 3, 4, 1, 2, 0, 8, 9, 4, 1, 7, 2, 6, 1, 3, 3, 0, 1, 9, 5, 4, 4, 8, 2, 6, 2, 9, 7, 7, 7, 9, 8, 9, 4, 4, 7, 1, 0, 4, 3, 6, 3, 9, 8, 3, 6, 8, 3, 6, 6, 2, 6, 7, 3, 0, 0, 0, 2, 5, 1, 2, 9, 2, 2, 1, 6, 3, 9, 1, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.transpose(unpickle('cifar-10/data_batch_1')['data'])\n",
    "#x_train=np.transpose(x_train)[0:20,:]\n",
    "x_train=np.transpose(x_train)\n",
    "y_train = unpickle('cifar-10/data_batch_1')['labels']\n",
    "#y_train =y_train[0:20]\n",
    "\n",
    "print np.shape(x_train),'\\n',np.shape(y_train),'\\n',y_train[6],'\\n',y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.302580936246871\n"
     ]
    }
   ],
   "source": [
    "net=Two_layer_NN(32*32*3, 50, 10)\n",
    "loss,g=net.loss_grad(x_train,y=y_train)\n",
    "print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0 / 1500: loss 2.302581 : training accuracy 0.104400, and val accuracy 0.104400\n",
      "epochs 1 / 1500: loss 2.302526 : training accuracy 0.119900, and val accuracy 0.119900\n",
      "epochs 2 / 1500: loss 2.302460 : training accuracy 0.103600, and val accuracy 0.103600\n",
      "epochs 3 / 1500: loss 2.302374 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 4 / 1500: loss 2.302246 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 5 / 1500: loss 2.302047 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 6 / 1500: loss 2.301737 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 7 / 1500: loss 2.301253 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 8 / 1500: loss 2.300506 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 9 / 1500: loss 2.299388 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 10 / 1500: loss 2.297788 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 11 / 1500: loss 2.295628 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 12 / 1500: loss 2.292878 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 13 / 1500: loss 2.289498 : training accuracy 0.102300, and val accuracy 0.102300\n",
      "epochs 14 / 1500: loss 2.285340 : training accuracy 0.117600, and val accuracy 0.117600\n",
      "epochs 15 / 1500: loss 2.280145 : training accuracy 0.130900, and val accuracy 0.130900\n",
      "epochs 16 / 1500: loss 2.273720 : training accuracy 0.129600, and val accuracy 0.129600\n",
      "epochs 17 / 1500: loss 2.266058 : training accuracy 0.132500, and val accuracy 0.132500\n",
      "epochs 18 / 1500: loss 2.257187 : training accuracy 0.136700, and val accuracy 0.136700\n",
      "epochs 19 / 1500: loss 2.247008 : training accuracy 0.147400, and val accuracy 0.147400\n",
      "epochs 20 / 1500: loss 2.235321 : training accuracy 0.159600, and val accuracy 0.159600\n",
      "epochs 21 / 1500: loss 2.222047 : training accuracy 0.169300, and val accuracy 0.169300\n",
      "epochs 22 / 1500: loss 2.207418 : training accuracy 0.165900, and val accuracy 0.165900\n",
      "epochs 23 / 1500: loss 2.192078 : training accuracy 0.171700, and val accuracy 0.171700\n",
      "epochs 24 / 1500: loss 2.176879 : training accuracy 0.177400, and val accuracy 0.177400\n",
      "epochs 25 / 1500: loss 2.162569 : training accuracy 0.192900, and val accuracy 0.192900\n",
      "epochs 26 / 1500: loss 2.149629 : training accuracy 0.190900, and val accuracy 0.190900\n",
      "epochs 27 / 1500: loss 2.138232 : training accuracy 0.192400, and val accuracy 0.192400\n",
      "epochs 28 / 1500: loss 2.128310 : training accuracy 0.193500, and val accuracy 0.193500\n",
      "epochs 29 / 1500: loss 2.119669 : training accuracy 0.193800, and val accuracy 0.193800\n",
      "epochs 30 / 1500: loss 2.112064 : training accuracy 0.194100, and val accuracy 0.194100\n",
      "epochs 31 / 1500: loss 2.105266 : training accuracy 0.195300, and val accuracy 0.195300\n",
      "epochs 32 / 1500: loss 2.099070 : training accuracy 0.197200, and val accuracy 0.197200\n",
      "epochs 33 / 1500: loss 2.093304 : training accuracy 0.198400, and val accuracy 0.198400\n",
      "epochs 34 / 1500: loss 2.087843 : training accuracy 0.203300, and val accuracy 0.203300\n",
      "epochs 35 / 1500: loss 2.082588 : training accuracy 0.209500, and val accuracy 0.209500\n",
      "epochs 36 / 1500: loss 2.077450 : training accuracy 0.216000, and val accuracy 0.216000\n",
      "epochs 37 / 1500: loss 2.072381 : training accuracy 0.221200, and val accuracy 0.221200\n",
      "epochs 38 / 1500: loss 2.067332 : training accuracy 0.229100, and val accuracy 0.229100\n",
      "epochs 39 / 1500: loss 2.062268 : training accuracy 0.233200, and val accuracy 0.233200\n",
      "epochs 40 / 1500: loss 2.057174 : training accuracy 0.238200, and val accuracy 0.238200\n",
      "epochs 41 / 1500: loss 2.052044 : training accuracy 0.241900, and val accuracy 0.241900\n",
      "epochs 42 / 1500: loss 2.046883 : training accuracy 0.246600, and val accuracy 0.246600\n",
      "epochs 43 / 1500: loss 2.041708 : training accuracy 0.249700, and val accuracy 0.249700\n",
      "epochs 44 / 1500: loss 2.036547 : training accuracy 0.254000, and val accuracy 0.254000\n",
      "epochs 45 / 1500: loss 2.031431 : training accuracy 0.255100, and val accuracy 0.255100\n",
      "epochs 46 / 1500: loss 2.026389 : training accuracy 0.255600, and val accuracy 0.255600\n",
      "epochs 47 / 1500: loss 2.021446 : training accuracy 0.256800, and val accuracy 0.256800\n",
      "epochs 48 / 1500: loss 2.016618 : training accuracy 0.258700, and val accuracy 0.258700\n",
      "epochs 49 / 1500: loss 2.011918 : training accuracy 0.261100, and val accuracy 0.261100\n",
      "epochs 50 / 1500: loss 2.007346 : training accuracy 0.263400, and val accuracy 0.263400\n",
      "epochs 51 / 1500: loss 2.002902 : training accuracy 0.264000, and val accuracy 0.264000\n",
      "epochs 52 / 1500: loss 1.998575 : training accuracy 0.266300, and val accuracy 0.266300\n",
      "epochs 53 / 1500: loss 1.994349 : training accuracy 0.267700, and val accuracy 0.267700\n",
      "epochs 54 / 1500: loss 1.990218 : training accuracy 0.270300, and val accuracy 0.270300\n",
      "epochs 55 / 1500: loss 1.986167 : training accuracy 0.273100, and val accuracy 0.273100\n",
      "epochs 56 / 1500: loss 1.982194 : training accuracy 0.274800, and val accuracy 0.274800\n",
      "epochs 57 / 1500: loss 1.978283 : training accuracy 0.276700, and val accuracy 0.276700\n",
      "epochs 58 / 1500: loss 1.974428 : training accuracy 0.278900, and val accuracy 0.278900\n",
      "epochs 59 / 1500: loss 1.970629 : training accuracy 0.281900, and val accuracy 0.281900\n",
      "epochs 60 / 1500: loss 1.966886 : training accuracy 0.283200, and val accuracy 0.283200\n",
      "epochs 61 / 1500: loss 1.963210 : training accuracy 0.285000, and val accuracy 0.285000\n",
      "epochs 62 / 1500: loss 1.959596 : training accuracy 0.284800, and val accuracy 0.284800\n",
      "epochs 63 / 1500: loss 1.956050 : training accuracy 0.285800, and val accuracy 0.285800\n",
      "epochs 64 / 1500: loss 1.952575 : training accuracy 0.287600, and val accuracy 0.287600\n",
      "epochs 65 / 1500: loss 1.949175 : training accuracy 0.290300, and val accuracy 0.290300\n",
      "epochs 66 / 1500: loss 1.945861 : training accuracy 0.292500, and val accuracy 0.292500\n",
      "epochs 67 / 1500: loss 1.942649 : training accuracy 0.293800, and val accuracy 0.293800\n",
      "epochs 68 / 1500: loss 1.939539 : training accuracy 0.294300, and val accuracy 0.294300\n",
      "epochs 69 / 1500: loss 1.936530 : training accuracy 0.294600, and val accuracy 0.294600\n",
      "epochs 70 / 1500: loss 1.933617 : training accuracy 0.295300, and val accuracy 0.295300\n",
      "epochs 71 / 1500: loss 1.930803 : training accuracy 0.297900, and val accuracy 0.297900\n",
      "epochs 72 / 1500: loss 1.928076 : training accuracy 0.300200, and val accuracy 0.300200\n",
      "epochs 73 / 1500: loss 1.925432 : training accuracy 0.300000, and val accuracy 0.300000\n",
      "epochs 74 / 1500: loss 1.922867 : training accuracy 0.301600, and val accuracy 0.301600\n",
      "epochs 75 / 1500: loss 1.920378 : training accuracy 0.302200, and val accuracy 0.302200\n",
      "epochs 76 / 1500: loss 1.917955 : training accuracy 0.303200, and val accuracy 0.303200\n",
      "epochs 77 / 1500: loss 1.915584 : training accuracy 0.303700, and val accuracy 0.303700\n",
      "epochs 78 / 1500: loss 1.913266 : training accuracy 0.305400, and val accuracy 0.305400\n",
      "epochs 79 / 1500: loss 1.910999 : training accuracy 0.305300, and val accuracy 0.305300\n",
      "epochs 80 / 1500: loss 1.908767 : training accuracy 0.306600, and val accuracy 0.306600\n",
      "epochs 81 / 1500: loss 1.906563 : training accuracy 0.308000, and val accuracy 0.308000\n",
      "epochs 82 / 1500: loss 1.904386 : training accuracy 0.310200, and val accuracy 0.310200\n",
      "epochs 83 / 1500: loss 1.902229 : training accuracy 0.312100, and val accuracy 0.312100\n",
      "epochs 84 / 1500: loss 1.900086 : training accuracy 0.313400, and val accuracy 0.313400\n",
      "epochs 85 / 1500: loss 1.897952 : training accuracy 0.313900, and val accuracy 0.313900\n",
      "epochs 86 / 1500: loss 1.895831 : training accuracy 0.315100, and val accuracy 0.315100\n",
      "epochs 87 / 1500: loss 1.893717 : training accuracy 0.317200, and val accuracy 0.317200\n",
      "epochs 88 / 1500: loss 1.891603 : training accuracy 0.318500, and val accuracy 0.318500\n",
      "epochs 89 / 1500: loss 1.889490 : training accuracy 0.321300, and val accuracy 0.321300\n",
      "epochs 90 / 1500: loss 1.887376 : training accuracy 0.321500, and val accuracy 0.321500\n",
      "epochs 91 / 1500: loss 1.885256 : training accuracy 0.322000, and val accuracy 0.322000\n",
      "epochs 92 / 1500: loss 1.883134 : training accuracy 0.323100, and val accuracy 0.323100\n",
      "epochs 93 / 1500: loss 1.881009 : training accuracy 0.323400, and val accuracy 0.323400\n",
      "epochs 94 / 1500: loss 1.878882 : training accuracy 0.324500, and val accuracy 0.324500\n",
      "epochs 95 / 1500: loss 1.876754 : training accuracy 0.325400, and val accuracy 0.325400\n",
      "epochs 96 / 1500: loss 1.874625 : training accuracy 0.326800, and val accuracy 0.326800\n",
      "epochs 97 / 1500: loss 1.872491 : training accuracy 0.327400, and val accuracy 0.327400\n",
      "epochs 98 / 1500: loss 1.870354 : training accuracy 0.327800, and val accuracy 0.327800\n",
      "epochs 99 / 1500: loss 1.868215 : training accuracy 0.329300, and val accuracy 0.329300\n",
      "epochs 100 / 1500: loss 1.866075 : training accuracy 0.330500, and val accuracy 0.330500\n",
      "epochs 101 / 1500: loss 1.863935 : training accuracy 0.331300, and val accuracy 0.331300\n",
      "epochs 102 / 1500: loss 1.861795 : training accuracy 0.332100, and val accuracy 0.332100\n",
      "epochs 103 / 1500: loss 1.859653 : training accuracy 0.333700, and val accuracy 0.333700\n",
      "epochs 104 / 1500: loss 1.857513 : training accuracy 0.334900, and val accuracy 0.334900\n",
      "epochs 105 / 1500: loss 1.855373 : training accuracy 0.336300, and val accuracy 0.336300\n",
      "epochs 106 / 1500: loss 1.853230 : training accuracy 0.337200, and val accuracy 0.337200\n",
      "epochs 107 / 1500: loss 1.851091 : training accuracy 0.337600, and val accuracy 0.337600\n",
      "epochs 108 / 1500: loss 1.848958 : training accuracy 0.338500, and val accuracy 0.338500\n",
      "epochs 109 / 1500: loss 1.846831 : training accuracy 0.339800, and val accuracy 0.339800\n",
      "epochs 110 / 1500: loss 1.844714 : training accuracy 0.339900, and val accuracy 0.339900\n",
      "epochs 111 / 1500: loss 1.842605 : training accuracy 0.341500, and val accuracy 0.341500\n",
      "epochs 112 / 1500: loss 1.840506 : training accuracy 0.343600, and val accuracy 0.343600\n",
      "epochs 113 / 1500: loss 1.838422 : training accuracy 0.344100, and val accuracy 0.344100\n",
      "epochs 114 / 1500: loss 1.836346 : training accuracy 0.344200, and val accuracy 0.344200\n",
      "epochs 115 / 1500: loss 1.834284 : training accuracy 0.345300, and val accuracy 0.345300\n",
      "epochs 116 / 1500: loss 1.832236 : training accuracy 0.346300, and val accuracy 0.346300\n",
      "epochs 117 / 1500: loss 1.830203 : training accuracy 0.347100, and val accuracy 0.347100\n",
      "epochs 118 / 1500: loss 1.828183 : training accuracy 0.347700, and val accuracy 0.347700\n",
      "epochs 119 / 1500: loss 1.826179 : training accuracy 0.349400, and val accuracy 0.349400\n",
      "epochs 120 / 1500: loss 1.824191 : training accuracy 0.350400, and val accuracy 0.350400\n",
      "epochs 121 / 1500: loss 1.822211 : training accuracy 0.351000, and val accuracy 0.351000\n",
      "epochs 122 / 1500: loss 1.820252 : training accuracy 0.352000, and val accuracy 0.352000\n",
      "epochs 123 / 1500: loss 1.818307 : training accuracy 0.352500, and val accuracy 0.352500\n",
      "epochs 124 / 1500: loss 1.816380 : training accuracy 0.352500, and val accuracy 0.352500\n",
      "epochs 125 / 1500: loss 1.814472 : training accuracy 0.353400, and val accuracy 0.353400\n",
      "epochs 126 / 1500: loss 1.812585 : training accuracy 0.354300, and val accuracy 0.354300\n",
      "epochs 127 / 1500: loss 1.810720 : training accuracy 0.355800, and val accuracy 0.355800\n",
      "epochs 128 / 1500: loss 1.808873 : training accuracy 0.355700, and val accuracy 0.355700\n",
      "epochs 129 / 1500: loss 1.807045 : training accuracy 0.355600, and val accuracy 0.355600\n",
      "epochs 130 / 1500: loss 1.805237 : training accuracy 0.355800, and val accuracy 0.355800\n",
      "epochs 131 / 1500: loss 1.803450 : training accuracy 0.356000, and val accuracy 0.356000\n",
      "epochs 132 / 1500: loss 1.801680 : training accuracy 0.356900, and val accuracy 0.356900\n",
      "epochs 133 / 1500: loss 1.799929 : training accuracy 0.358400, and val accuracy 0.358400\n",
      "epochs 134 / 1500: loss 1.798195 : training accuracy 0.358800, and val accuracy 0.358800\n",
      "epochs 135 / 1500: loss 1.796477 : training accuracy 0.360100, and val accuracy 0.360100\n",
      "epochs 136 / 1500: loss 1.794775 : training accuracy 0.360600, and val accuracy 0.360600\n",
      "epochs 137 / 1500: loss 1.793091 : training accuracy 0.361300, and val accuracy 0.361300\n",
      "epochs 138 / 1500: loss 1.791419 : training accuracy 0.362000, and val accuracy 0.362000\n",
      "epochs 139 / 1500: loss 1.789763 : training accuracy 0.362900, and val accuracy 0.362900\n",
      "epochs 140 / 1500: loss 1.788124 : training accuracy 0.363600, and val accuracy 0.363600\n",
      "epochs 141 / 1500: loss 1.786511 : training accuracy 0.363700, and val accuracy 0.363700\n",
      "epochs 142 / 1500: loss 1.784927 : training accuracy 0.364600, and val accuracy 0.364600\n",
      "epochs 143 / 1500: loss 1.783434 : training accuracy 0.365900, and val accuracy 0.365900\n",
      "epochs 144 / 1500: loss 1.782206 : training accuracy 0.366800, and val accuracy 0.366800\n",
      "epochs 145 / 1500: loss 1.781689 : training accuracy 0.366900, and val accuracy 0.366900\n",
      "epochs 146 / 1500: loss 1.783493 : training accuracy 0.367000, and val accuracy 0.367000\n",
      "epochs 147 / 1500: loss 1.789999 : training accuracy 0.365200, and val accuracy 0.365200\n",
      "epochs 148 / 1500: loss 1.795832 : training accuracy 0.362700, and val accuracy 0.362700\n",
      "epochs 149 / 1500: loss 1.792642 : training accuracy 0.365300, and val accuracy 0.365300\n",
      "epochs 150 / 1500: loss 1.786979 : training accuracy 0.367000, and val accuracy 0.367000\n",
      "epochs 151 / 1500: loss 1.782633 : training accuracy 0.369600, and val accuracy 0.369600\n",
      "epochs 152 / 1500: loss 1.779592 : training accuracy 0.370500, and val accuracy 0.370500\n",
      "epochs 153 / 1500: loss 1.777290 : training accuracy 0.371300, and val accuracy 0.371300\n",
      "epochs 154 / 1500: loss 1.775376 : training accuracy 0.371800, and val accuracy 0.371800\n",
      "epochs 155 / 1500: loss 1.773600 : training accuracy 0.372600, and val accuracy 0.372600\n",
      "epochs 156 / 1500: loss 1.771995 : training accuracy 0.373800, and val accuracy 0.373800\n",
      "epochs 157 / 1500: loss 1.770346 : training accuracy 0.373900, and val accuracy 0.373900\n",
      "epochs 158 / 1500: loss 1.769082 : training accuracy 0.374000, and val accuracy 0.374000\n",
      "epochs 159 / 1500: loss 1.767954 : training accuracy 0.374100, and val accuracy 0.374100\n",
      "epochs 160 / 1500: loss 1.766574 : training accuracy 0.375000, and val accuracy 0.375000\n",
      "epochs 161 / 1500: loss 1.764955 : training accuracy 0.376300, and val accuracy 0.376300\n",
      "epochs 162 / 1500: loss 1.763264 : training accuracy 0.376500, and val accuracy 0.376500\n",
      "epochs 163 / 1500: loss 1.761498 : training accuracy 0.377300, and val accuracy 0.377300\n",
      "epochs 164 / 1500: loss 1.759676 : training accuracy 0.378000, and val accuracy 0.378000\n",
      "epochs 165 / 1500: loss 1.758081 : training accuracy 0.378900, and val accuracy 0.378900\n",
      "epochs 166 / 1500: loss 1.756398 : training accuracy 0.379700, and val accuracy 0.379700\n",
      "epochs 167 / 1500: loss 1.754786 : training accuracy 0.380600, and val accuracy 0.380600\n",
      "epochs 168 / 1500: loss 1.753594 : training accuracy 0.380600, and val accuracy 0.380600\n",
      "epochs 169 / 1500: loss 1.752488 : training accuracy 0.380700, and val accuracy 0.380700\n",
      "epochs 170 / 1500: loss 1.750938 : training accuracy 0.381200, and val accuracy 0.381200\n",
      "epochs 171 / 1500: loss 1.749318 : training accuracy 0.382100, and val accuracy 0.382100\n",
      "epochs 172 / 1500: loss 1.747594 : training accuracy 0.383200, and val accuracy 0.383200\n",
      "epochs 173 / 1500: loss 1.746227 : training accuracy 0.384500, and val accuracy 0.384500\n",
      "epochs 174 / 1500: loss 1.745122 : training accuracy 0.384600, and val accuracy 0.384600\n",
      "epochs 175 / 1500: loss 1.744026 : training accuracy 0.384700, and val accuracy 0.384700\n",
      "epochs 176 / 1500: loss 1.742467 : training accuracy 0.386000, and val accuracy 0.386000\n",
      "epochs 177 / 1500: loss 1.740479 : training accuracy 0.387400, and val accuracy 0.387400\n",
      "epochs 178 / 1500: loss 1.738530 : training accuracy 0.387900, and val accuracy 0.387900\n",
      "epochs 179 / 1500: loss 1.736813 : training accuracy 0.388500, and val accuracy 0.388500\n",
      "epochs 180 / 1500: loss 1.735180 : training accuracy 0.389400, and val accuracy 0.389400\n",
      "epochs 181 / 1500: loss 1.733527 : training accuracy 0.390100, and val accuracy 0.390100\n",
      "epochs 182 / 1500: loss 1.731849 : training accuracy 0.391100, and val accuracy 0.391100\n",
      "epochs 183 / 1500: loss 1.730184 : training accuracy 0.392000, and val accuracy 0.392000\n",
      "epochs 184 / 1500: loss 1.728344 : training accuracy 0.392500, and val accuracy 0.392500\n",
      "epochs 185 / 1500: loss 1.726806 : training accuracy 0.393500, and val accuracy 0.393500\n",
      "epochs 186 / 1500: loss 1.725679 : training accuracy 0.393700, and val accuracy 0.393700\n",
      "epochs 187 / 1500: loss 1.724692 : training accuracy 0.394100, and val accuracy 0.394100\n",
      "epochs 188 / 1500: loss 1.723747 : training accuracy 0.394400, and val accuracy 0.394400\n",
      "epochs 189 / 1500: loss 1.722402 : training accuracy 0.394400, and val accuracy 0.394400\n",
      "epochs 190 / 1500: loss 1.721100 : training accuracy 0.394400, and val accuracy 0.394400\n",
      "epochs 191 / 1500: loss 1.719702 : training accuracy 0.395300, and val accuracy 0.395300\n",
      "epochs 192 / 1500: loss 1.718545 : training accuracy 0.395900, and val accuracy 0.395900\n",
      "epochs 193 / 1500: loss 1.717374 : training accuracy 0.396500, and val accuracy 0.396500\n",
      "epochs 194 / 1500: loss 1.715975 : training accuracy 0.397200, and val accuracy 0.397200\n",
      "epochs 195 / 1500: loss 1.714140 : training accuracy 0.397900, and val accuracy 0.397900\n",
      "epochs 196 / 1500: loss 1.712356 : training accuracy 0.398800, and val accuracy 0.398800\n",
      "epochs 197 / 1500: loss 1.710871 : training accuracy 0.399800, and val accuracy 0.399800\n",
      "epochs 198 / 1500: loss 1.709433 : training accuracy 0.399700, and val accuracy 0.399700\n",
      "epochs 199 / 1500: loss 1.708168 : training accuracy 0.399600, and val accuracy 0.399600\n",
      "epochs 200 / 1500: loss 1.707107 : training accuracy 0.399600, and val accuracy 0.399600\n",
      "epochs 201 / 1500: loss 1.706190 : training accuracy 0.399600, and val accuracy 0.399600\n",
      "epochs 202 / 1500: loss 1.704699 : training accuracy 0.399600, and val accuracy 0.399600\n",
      "epochs 203 / 1500: loss 1.702859 : training accuracy 0.401300, and val accuracy 0.401300\n",
      "epochs 204 / 1500: loss 1.701329 : training accuracy 0.401400, and val accuracy 0.401400\n",
      "epochs 205 / 1500: loss 1.699723 : training accuracy 0.402200, and val accuracy 0.402200\n",
      "epochs 206 / 1500: loss 1.697830 : training accuracy 0.404000, and val accuracy 0.404000\n",
      "epochs 207 / 1500: loss 1.696010 : training accuracy 0.405000, and val accuracy 0.405000\n",
      "epochs 208 / 1500: loss 1.694490 : training accuracy 0.405500, and val accuracy 0.405500\n",
      "epochs 209 / 1500: loss 1.693458 : training accuracy 0.405700, and val accuracy 0.405700\n",
      "epochs 210 / 1500: loss 1.692365 : training accuracy 0.405900, and val accuracy 0.405900\n",
      "epochs 211 / 1500: loss 1.691315 : training accuracy 0.406200, and val accuracy 0.406200\n",
      "epochs 212 / 1500: loss 1.689894 : training accuracy 0.405600, and val accuracy 0.405600\n",
      "epochs 213 / 1500: loss 1.688310 : training accuracy 0.405900, and val accuracy 0.405900\n",
      "epochs 214 / 1500: loss 1.686776 : training accuracy 0.406500, and val accuracy 0.406500\n",
      "epochs 215 / 1500: loss 1.685645 : training accuracy 0.406000, and val accuracy 0.406000\n",
      "epochs 216 / 1500: loss 1.684642 : training accuracy 0.406200, and val accuracy 0.406200\n",
      "epochs 217 / 1500: loss 1.684129 : training accuracy 0.406400, and val accuracy 0.406400\n",
      "epochs 218 / 1500: loss 1.683763 : training accuracy 0.405200, and val accuracy 0.405200\n",
      "epochs 219 / 1500: loss 1.683544 : training accuracy 0.404600, and val accuracy 0.404600\n",
      "epochs 220 / 1500: loss 1.684338 : training accuracy 0.404100, and val accuracy 0.404100\n",
      "epochs 221 / 1500: loss 1.687070 : training accuracy 0.399900, and val accuracy 0.399900\n",
      "epochs 222 / 1500: loss 1.686830 : training accuracy 0.399700, and val accuracy 0.399700\n",
      "epochs 223 / 1500: loss 1.682525 : training accuracy 0.400400, and val accuracy 0.400400\n",
      "epochs 224 / 1500: loss 1.677895 : training accuracy 0.402000, and val accuracy 0.402000\n",
      "epochs 225 / 1500: loss 1.674902 : training accuracy 0.404300, and val accuracy 0.404300\n",
      "epochs 226 / 1500: loss 1.672864 : training accuracy 0.406500, and val accuracy 0.406500\n",
      "epochs 227 / 1500: loss 1.672017 : training accuracy 0.406100, and val accuracy 0.406100\n",
      "epochs 228 / 1500: loss 1.671997 : training accuracy 0.405300, and val accuracy 0.405300\n",
      "epochs 229 / 1500: loss 1.671825 : training accuracy 0.407500, and val accuracy 0.407500\n",
      "epochs 230 / 1500: loss 1.671855 : training accuracy 0.406100, and val accuracy 0.406100\n",
      "epochs 231 / 1500: loss 1.671392 : training accuracy 0.407500, and val accuracy 0.407500\n",
      "epochs 232 / 1500: loss 1.670294 : training accuracy 0.408400, and val accuracy 0.408400\n",
      "epochs 233 / 1500: loss 1.670279 : training accuracy 0.412500, and val accuracy 0.412500\n",
      "epochs 234 / 1500: loss 1.672350 : training accuracy 0.409400, and val accuracy 0.409400\n",
      "epochs 235 / 1500: loss 1.675696 : training accuracy 0.407200, and val accuracy 0.407200\n",
      "epochs 236 / 1500: loss 1.673218 : training accuracy 0.410600, and val accuracy 0.410600\n",
      "epochs 237 / 1500: loss 1.665880 : training accuracy 0.415300, and val accuracy 0.415300\n",
      "epochs 238 / 1500: loss 1.660228 : training accuracy 0.418600, and val accuracy 0.418600\n",
      "epochs 239 / 1500: loss 1.656922 : training accuracy 0.419300, and val accuracy 0.419300\n",
      "epochs 240 / 1500: loss 1.655298 : training accuracy 0.419900, and val accuracy 0.419900\n",
      "epochs 241 / 1500: loss 1.654385 : training accuracy 0.419500, and val accuracy 0.419500\n",
      "epochs 242 / 1500: loss 1.653867 : training accuracy 0.418700, and val accuracy 0.418700\n",
      "epochs 243 / 1500: loss 1.653478 : training accuracy 0.418500, and val accuracy 0.418500\n",
      "epochs 244 / 1500: loss 1.653431 : training accuracy 0.418900, and val accuracy 0.418900\n",
      "epochs 245 / 1500: loss 1.652525 : training accuracy 0.419300, and val accuracy 0.419300\n",
      "epochs 246 / 1500: loss 1.650982 : training accuracy 0.419300, and val accuracy 0.419300\n",
      "epochs 247 / 1500: loss 1.649345 : training accuracy 0.420700, and val accuracy 0.420700\n",
      "epochs 248 / 1500: loss 1.647535 : training accuracy 0.423200, and val accuracy 0.423200\n",
      "epochs 249 / 1500: loss 1.646784 : training accuracy 0.421700, and val accuracy 0.421700\n",
      "epochs 250 / 1500: loss 1.647453 : training accuracy 0.417400, and val accuracy 0.417400\n",
      "epochs 251 / 1500: loss 1.648290 : training accuracy 0.413700, and val accuracy 0.413700\n",
      "epochs 252 / 1500: loss 1.645237 : training accuracy 0.414200, and val accuracy 0.414200\n",
      "epochs 253 / 1500: loss 1.642080 : training accuracy 0.414700, and val accuracy 0.414700\n",
      "epochs 254 / 1500: loss 1.644200 : training accuracy 0.413500, and val accuracy 0.413500\n",
      "epochs 255 / 1500: loss 1.648373 : training accuracy 0.411300, and val accuracy 0.411300\n",
      "epochs 256 / 1500: loss 1.647805 : training accuracy 0.413500, and val accuracy 0.413500\n",
      "epochs 257 / 1500: loss 1.642868 : training accuracy 0.414800, and val accuracy 0.414800\n",
      "epochs 258 / 1500: loss 1.638453 : training accuracy 0.419600, and val accuracy 0.419600\n",
      "epochs 259 / 1500: loss 1.635776 : training accuracy 0.420500, and val accuracy 0.420500\n",
      "epochs 260 / 1500: loss 1.633991 : training accuracy 0.421300, and val accuracy 0.421300\n",
      "epochs 261 / 1500: loss 1.632365 : training accuracy 0.421600, and val accuracy 0.421600\n",
      "epochs 262 / 1500: loss 1.629987 : training accuracy 0.423000, and val accuracy 0.423000\n",
      "epochs 263 / 1500: loss 1.628515 : training accuracy 0.423200, and val accuracy 0.423200\n",
      "epochs 264 / 1500: loss 1.627221 : training accuracy 0.422200, and val accuracy 0.422200\n",
      "epochs 265 / 1500: loss 1.625533 : training accuracy 0.423800, and val accuracy 0.423800\n",
      "epochs 266 / 1500: loss 1.623908 : training accuracy 0.423300, and val accuracy 0.423300\n",
      "epochs 267 / 1500: loss 1.624839 : training accuracy 0.421500, and val accuracy 0.421500\n",
      "epochs 268 / 1500: loss 1.628095 : training accuracy 0.422400, and val accuracy 0.422400\n",
      "epochs 269 / 1500: loss 1.629552 : training accuracy 0.424500, and val accuracy 0.424500\n",
      "epochs 270 / 1500: loss 1.627988 : training accuracy 0.425600, and val accuracy 0.425600\n",
      "epochs 271 / 1500: loss 1.626009 : training accuracy 0.429400, and val accuracy 0.429400\n",
      "epochs 272 / 1500: loss 1.625521 : training accuracy 0.429300, and val accuracy 0.429300\n",
      "epochs 273 / 1500: loss 1.630367 : training accuracy 0.424700, and val accuracy 0.424700\n",
      "epochs 274 / 1500: loss 1.631870 : training accuracy 0.423000, and val accuracy 0.423000\n",
      "epochs 275 / 1500: loss 1.622150 : training accuracy 0.427600, and val accuracy 0.427600\n",
      "epochs 276 / 1500: loss 1.617973 : training accuracy 0.429800, and val accuracy 0.429800\n",
      "epochs 277 / 1500: loss 1.615177 : training accuracy 0.431100, and val accuracy 0.431100\n",
      "epochs 278 / 1500: loss 1.612885 : training accuracy 0.432000, and val accuracy 0.432000\n",
      "epochs 279 / 1500: loss 1.611631 : training accuracy 0.432700, and val accuracy 0.432700\n",
      "epochs 280 / 1500: loss 1.609874 : training accuracy 0.434400, and val accuracy 0.434400\n",
      "epochs 281 / 1500: loss 1.608863 : training accuracy 0.435900, and val accuracy 0.435900\n",
      "epochs 282 / 1500: loss 1.608330 : training accuracy 0.434900, and val accuracy 0.434900\n",
      "epochs 283 / 1500: loss 1.608751 : training accuracy 0.435300, and val accuracy 0.435300\n",
      "epochs 284 / 1500: loss 1.608889 : training accuracy 0.433600, and val accuracy 0.433600\n",
      "epochs 285 / 1500: loss 1.607817 : training accuracy 0.433800, and val accuracy 0.433800\n",
      "epochs 286 / 1500: loss 1.608538 : training accuracy 0.431600, and val accuracy 0.431600\n",
      "epochs 287 / 1500: loss 1.616641 : training accuracy 0.423800, and val accuracy 0.423800\n",
      "epochs 288 / 1500: loss 1.616219 : training accuracy 0.426800, and val accuracy 0.426800\n",
      "epochs 289 / 1500: loss 1.605155 : training accuracy 0.431200, and val accuracy 0.431200\n",
      "epochs 290 / 1500: loss 1.599340 : training accuracy 0.434500, and val accuracy 0.434500\n",
      "epochs 291 / 1500: loss 1.597339 : training accuracy 0.434500, and val accuracy 0.434500\n",
      "epochs 292 / 1500: loss 1.595496 : training accuracy 0.434400, and val accuracy 0.434400\n",
      "epochs 293 / 1500: loss 1.593229 : training accuracy 0.435500, and val accuracy 0.435500\n",
      "epochs 294 / 1500: loss 1.590893 : training accuracy 0.437100, and val accuracy 0.437100\n",
      "epochs 295 / 1500: loss 1.588920 : training accuracy 0.438500, and val accuracy 0.438500\n",
      "epochs 296 / 1500: loss 1.587789 : training accuracy 0.439600, and val accuracy 0.439600\n",
      "epochs 297 / 1500: loss 1.587565 : training accuracy 0.435500, and val accuracy 0.435500\n",
      "epochs 298 / 1500: loss 1.590542 : training accuracy 0.434600, and val accuracy 0.434600\n",
      "epochs 299 / 1500: loss 1.594103 : training accuracy 0.435300, and val accuracy 0.435300\n",
      "epochs 300 / 1500: loss 1.593160 : training accuracy 0.437100, and val accuracy 0.437100\n",
      "epochs 301 / 1500: loss 1.591470 : training accuracy 0.440800, and val accuracy 0.440800\n",
      "epochs 302 / 1500: loss 1.591381 : training accuracy 0.443400, and val accuracy 0.443400\n",
      "epochs 303 / 1500: loss 1.594054 : training accuracy 0.439500, and val accuracy 0.439500\n",
      "epochs 304 / 1500: loss 1.593948 : training accuracy 0.437600, and val accuracy 0.437600\n",
      "epochs 305 / 1500: loss 1.586995 : training accuracy 0.442300, and val accuracy 0.442300\n",
      "epochs 306 / 1500: loss 1.582521 : training accuracy 0.443600, and val accuracy 0.443600\n",
      "epochs 307 / 1500: loss 1.580038 : training accuracy 0.444700, and val accuracy 0.444700\n",
      "epochs 308 / 1500: loss 1.578556 : training accuracy 0.445600, and val accuracy 0.445600\n",
      "epochs 309 / 1500: loss 1.576685 : training accuracy 0.448300, and val accuracy 0.448300\n",
      "epochs 310 / 1500: loss 1.574977 : training accuracy 0.449100, and val accuracy 0.449100\n",
      "epochs 311 / 1500: loss 1.576612 : training accuracy 0.449600, and val accuracy 0.449600\n",
      "epochs 312 / 1500: loss 1.578333 : training accuracy 0.448300, and val accuracy 0.448300\n",
      "epochs 313 / 1500: loss 1.575807 : training accuracy 0.448500, and val accuracy 0.448500\n",
      "epochs 314 / 1500: loss 1.573040 : training accuracy 0.449300, and val accuracy 0.449300\n",
      "epochs 315 / 1500: loss 1.570884 : training accuracy 0.448400, and val accuracy 0.448400\n",
      "epochs 316 / 1500: loss 1.570756 : training accuracy 0.444500, and val accuracy 0.444500\n",
      "epochs 317 / 1500: loss 1.574461 : training accuracy 0.442900, and val accuracy 0.442900\n",
      "epochs 318 / 1500: loss 1.580343 : training accuracy 0.441100, and val accuracy 0.441100\n",
      "epochs 319 / 1500: loss 1.571058 : training accuracy 0.444600, and val accuracy 0.444600\n",
      "epochs 320 / 1500: loss 1.564031 : training accuracy 0.447400, and val accuracy 0.447400\n",
      "epochs 321 / 1500: loss 1.561983 : training accuracy 0.448900, and val accuracy 0.448900\n",
      "epochs 322 / 1500: loss 1.559732 : training accuracy 0.450500, and val accuracy 0.450500\n",
      "epochs 323 / 1500: loss 1.558351 : training accuracy 0.452700, and val accuracy 0.452700\n",
      "epochs 324 / 1500: loss 1.556561 : training accuracy 0.453500, and val accuracy 0.453500\n",
      "epochs 325 / 1500: loss 1.555088 : training accuracy 0.454200, and val accuracy 0.454200\n",
      "epochs 326 / 1500: loss 1.553482 : training accuracy 0.455600, and val accuracy 0.455600\n",
      "epochs 327 / 1500: loss 1.553137 : training accuracy 0.455400, and val accuracy 0.455400\n",
      "epochs 328 / 1500: loss 1.556660 : training accuracy 0.454800, and val accuracy 0.454800\n",
      "epochs 329 / 1500: loss 1.567366 : training accuracy 0.450200, and val accuracy 0.450200\n",
      "epochs 330 / 1500: loss 1.556160 : training accuracy 0.455100, and val accuracy 0.455100\n",
      "epochs 331 / 1500: loss 1.555840 : training accuracy 0.455300, and val accuracy 0.455300\n",
      "epochs 332 / 1500: loss 1.557613 : training accuracy 0.456900, and val accuracy 0.456900\n",
      "epochs 333 / 1500: loss 1.557687 : training accuracy 0.455600, and val accuracy 0.455600\n",
      "epochs 334 / 1500: loss 1.557440 : training accuracy 0.451600, and val accuracy 0.451600\n",
      "epochs 335 / 1500: loss 1.557681 : training accuracy 0.452000, and val accuracy 0.452000\n",
      "epochs 336 / 1500: loss 1.561507 : training accuracy 0.447800, and val accuracy 0.447800\n",
      "epochs 337 / 1500: loss 1.562663 : training accuracy 0.445900, and val accuracy 0.445900\n",
      "epochs 338 / 1500: loss 1.552821 : training accuracy 0.452200, and val accuracy 0.452200\n",
      "epochs 339 / 1500: loss 1.546687 : training accuracy 0.454600, and val accuracy 0.454600\n",
      "epochs 340 / 1500: loss 1.542484 : training accuracy 0.457100, and val accuracy 0.457100\n",
      "epochs 341 / 1500: loss 1.539412 : training accuracy 0.456600, and val accuracy 0.456600\n",
      "epochs 342 / 1500: loss 1.537252 : training accuracy 0.456000, and val accuracy 0.456000\n",
      "epochs 343 / 1500: loss 1.535416 : training accuracy 0.455400, and val accuracy 0.455400\n",
      "epochs 344 / 1500: loss 1.533405 : training accuracy 0.458300, and val accuracy 0.458300\n",
      "epochs 345 / 1500: loss 1.536170 : training accuracy 0.456700, and val accuracy 0.456700\n",
      "epochs 346 / 1500: loss 1.544769 : training accuracy 0.455300, and val accuracy 0.455300\n",
      "epochs 347 / 1500: loss 1.546019 : training accuracy 0.457000, and val accuracy 0.457000\n",
      "epochs 348 / 1500: loss 1.543686 : training accuracy 0.457200, and val accuracy 0.457200\n",
      "epochs 349 / 1500: loss 1.541769 : training accuracy 0.458000, and val accuracy 0.458000\n",
      "epochs 350 / 1500: loss 1.536799 : training accuracy 0.458700, and val accuracy 0.458700\n",
      "epochs 351 / 1500: loss 1.538903 : training accuracy 0.458300, and val accuracy 0.458300\n",
      "epochs 352 / 1500: loss 1.547535 : training accuracy 0.453900, and val accuracy 0.453900\n",
      "epochs 353 / 1500: loss 1.536941 : training accuracy 0.460000, and val accuracy 0.460000\n",
      "epochs 354 / 1500: loss 1.528868 : training accuracy 0.461100, and val accuracy 0.461100\n",
      "epochs 355 / 1500: loss 1.527760 : training accuracy 0.461100, and val accuracy 0.461100\n",
      "epochs 356 / 1500: loss 1.551190 : training accuracy 0.448400, and val accuracy 0.448400\n",
      "epochs 357 / 1500: loss 1.540860 : training accuracy 0.452600, and val accuracy 0.452600\n",
      "epochs 358 / 1500: loss 1.526408 : training accuracy 0.461400, and val accuracy 0.461400\n",
      "epochs 359 / 1500: loss 1.524070 : training accuracy 0.460700, and val accuracy 0.460700\n",
      "epochs 360 / 1500: loss 1.521083 : training accuracy 0.460800, and val accuracy 0.460800\n",
      "epochs 361 / 1500: loss 1.517341 : training accuracy 0.462000, and val accuracy 0.462000\n",
      "epochs 362 / 1500: loss 1.514255 : training accuracy 0.465200, and val accuracy 0.465200\n",
      "epochs 363 / 1500: loss 1.511872 : training accuracy 0.465200, and val accuracy 0.465200\n",
      "epochs 364 / 1500: loss 1.515422 : training accuracy 0.460500, and val accuracy 0.460500\n",
      "epochs 365 / 1500: loss 1.526995 : training accuracy 0.459600, and val accuracy 0.459600\n",
      "epochs 366 / 1500: loss 1.525489 : training accuracy 0.461900, and val accuracy 0.461900\n",
      "epochs 367 / 1500: loss 1.521785 : training accuracy 0.462800, and val accuracy 0.462800\n",
      "epochs 368 / 1500: loss 1.522426 : training accuracy 0.461100, and val accuracy 0.461100\n",
      "epochs 369 / 1500: loss 1.519239 : training accuracy 0.463100, and val accuracy 0.463100\n",
      "epochs 370 / 1500: loss 1.520373 : training accuracy 0.463200, and val accuracy 0.463200\n",
      "epochs 371 / 1500: loss 1.520855 : training accuracy 0.464800, and val accuracy 0.464800\n",
      "epochs 372 / 1500: loss 1.515797 : training accuracy 0.465900, and val accuracy 0.465900\n",
      "epochs 373 / 1500: loss 1.512190 : training accuracy 0.469400, and val accuracy 0.469400\n",
      "epochs 374 / 1500: loss 1.509277 : training accuracy 0.469800, and val accuracy 0.469800\n",
      "epochs 375 / 1500: loss 1.506185 : training accuracy 0.472100, and val accuracy 0.472100\n",
      "epochs 376 / 1500: loss 1.503243 : training accuracy 0.471200, and val accuracy 0.471200\n",
      "epochs 377 / 1500: loss 1.510766 : training accuracy 0.461200, and val accuracy 0.461200\n",
      "epochs 378 / 1500: loss 1.538059 : training accuracy 0.453300, and val accuracy 0.453300\n",
      "epochs 379 / 1500: loss 1.514214 : training accuracy 0.462800, and val accuracy 0.462800\n",
      "epochs 380 / 1500: loss 1.511177 : training accuracy 0.463900, and val accuracy 0.463900\n",
      "epochs 381 / 1500: loss 1.507333 : training accuracy 0.466200, and val accuracy 0.466200\n",
      "epochs 382 / 1500: loss 1.503488 : training accuracy 0.467000, and val accuracy 0.467000\n",
      "epochs 383 / 1500: loss 1.499708 : training accuracy 0.470500, and val accuracy 0.470500\n",
      "epochs 384 / 1500: loss 1.496493 : training accuracy 0.470700, and val accuracy 0.470700\n",
      "epochs 385 / 1500: loss 1.497765 : training accuracy 0.470300, and val accuracy 0.470300\n",
      "epochs 386 / 1500: loss 1.507661 : training accuracy 0.464900, and val accuracy 0.464900\n",
      "epochs 387 / 1500: loss 1.512660 : training accuracy 0.467600, and val accuracy 0.467600\n",
      "epochs 388 / 1500: loss 1.500819 : training accuracy 0.470200, and val accuracy 0.470200\n",
      "epochs 389 / 1500: loss 1.493189 : training accuracy 0.472400, and val accuracy 0.472400\n",
      "epochs 390 / 1500: loss 1.496139 : training accuracy 0.470400, and val accuracy 0.470400\n",
      "epochs 391 / 1500: loss 1.500567 : training accuracy 0.472500, and val accuracy 0.472500\n",
      "epochs 392 / 1500: loss 1.505642 : training accuracy 0.471600, and val accuracy 0.471600\n",
      "epochs 393 / 1500: loss 1.504956 : training accuracy 0.473600, and val accuracy 0.473600\n",
      "epochs 394 / 1500: loss 1.498032 : training accuracy 0.475500, and val accuracy 0.475500\n",
      "epochs 395 / 1500: loss 1.496858 : training accuracy 0.474000, and val accuracy 0.474000\n",
      "epochs 396 / 1500: loss 1.507093 : training accuracy 0.470000, and val accuracy 0.470000\n",
      "epochs 397 / 1500: loss 1.506432 : training accuracy 0.468700, and val accuracy 0.468700\n",
      "epochs 398 / 1500: loss 1.496458 : training accuracy 0.473300, and val accuracy 0.473300\n",
      "epochs 399 / 1500: loss 1.493568 : training accuracy 0.474500, and val accuracy 0.474500\n",
      "epochs 400 / 1500: loss 1.488487 : training accuracy 0.476500, and val accuracy 0.476500\n",
      "epochs 401 / 1500: loss 1.484552 : training accuracy 0.477700, and val accuracy 0.477700\n",
      "epochs 402 / 1500: loss 1.482830 : training accuracy 0.476900, and val accuracy 0.476900\n",
      "epochs 403 / 1500: loss 1.484822 : training accuracy 0.475200, and val accuracy 0.475200\n",
      "epochs 404 / 1500: loss 1.487366 : training accuracy 0.474800, and val accuracy 0.474800\n",
      "epochs 405 / 1500: loss 1.487376 : training accuracy 0.477900, and val accuracy 0.477900\n",
      "epochs 406 / 1500: loss 1.490437 : training accuracy 0.476400, and val accuracy 0.476400\n",
      "epochs 407 / 1500: loss 1.487639 : training accuracy 0.478300, and val accuracy 0.478300\n",
      "epochs 408 / 1500: loss 1.480419 : training accuracy 0.477800, and val accuracy 0.477800\n",
      "epochs 409 / 1500: loss 1.484297 : training accuracy 0.478600, and val accuracy 0.478600\n",
      "epochs 410 / 1500: loss 1.487286 : training accuracy 0.477700, and val accuracy 0.477700\n",
      "epochs 411 / 1500: loss 1.483952 : training accuracy 0.476600, and val accuracy 0.476600\n",
      "epochs 412 / 1500: loss 1.479407 : training accuracy 0.476700, and val accuracy 0.476700\n",
      "epochs 413 / 1500: loss 1.476437 : training accuracy 0.476300, and val accuracy 0.476300\n",
      "epochs 414 / 1500: loss 1.472834 : training accuracy 0.479900, and val accuracy 0.479900\n",
      "epochs 415 / 1500: loss 1.468529 : training accuracy 0.483300, and val accuracy 0.483300\n",
      "epochs 416 / 1500: loss 1.467276 : training accuracy 0.481800, and val accuracy 0.481800\n",
      "epochs 417 / 1500: loss 1.476958 : training accuracy 0.474400, and val accuracy 0.474400\n",
      "epochs 418 / 1500: loss 1.482654 : training accuracy 0.475200, and val accuracy 0.475200\n",
      "epochs 419 / 1500: loss 1.481021 : training accuracy 0.474100, and val accuracy 0.474100\n",
      "epochs 420 / 1500: loss 1.476718 : training accuracy 0.478600, and val accuracy 0.478600\n",
      "epochs 421 / 1500: loss 1.468293 : training accuracy 0.483500, and val accuracy 0.483500\n",
      "epochs 422 / 1500: loss 1.463327 : training accuracy 0.484200, and val accuracy 0.484200\n",
      "epochs 423 / 1500: loss 1.460742 : training accuracy 0.485700, and val accuracy 0.485700\n",
      "epochs 424 / 1500: loss 1.461267 : training accuracy 0.486900, and val accuracy 0.486900\n",
      "epochs 425 / 1500: loss 1.483390 : training accuracy 0.478200, and val accuracy 0.478200\n",
      "epochs 426 / 1500: loss 1.475621 : training accuracy 0.480700, and val accuracy 0.480700\n",
      "epochs 427 / 1500: loss 1.468965 : training accuracy 0.487000, and val accuracy 0.487000\n",
      "epochs 428 / 1500: loss 1.465773 : training accuracy 0.486100, and val accuracy 0.486100\n",
      "epochs 429 / 1500: loss 1.466278 : training accuracy 0.484100, and val accuracy 0.484100\n",
      "epochs 430 / 1500: loss 1.464419 : training accuracy 0.481300, and val accuracy 0.481300\n",
      "epochs 431 / 1500: loss 1.489647 : training accuracy 0.469500, and val accuracy 0.469500\n",
      "epochs 432 / 1500: loss 1.468877 : training accuracy 0.479500, and val accuracy 0.479500\n",
      "epochs 433 / 1500: loss 1.457137 : training accuracy 0.483900, and val accuracy 0.483900\n",
      "epochs 434 / 1500: loss 1.457236 : training accuracy 0.483100, and val accuracy 0.483100\n",
      "epochs 435 / 1500: loss 1.459293 : training accuracy 0.483200, and val accuracy 0.483200\n",
      "epochs 436 / 1500: loss 1.454174 : training accuracy 0.487400, and val accuracy 0.487400\n",
      "epochs 437 / 1500: loss 1.464236 : training accuracy 0.480800, and val accuracy 0.480800\n",
      "epochs 438 / 1500: loss 1.479837 : training accuracy 0.478000, and val accuracy 0.478000\n",
      "epochs 439 / 1500: loss 1.457800 : training accuracy 0.488000, and val accuracy 0.488000\n",
      "epochs 440 / 1500: loss 1.454953 : training accuracy 0.487200, and val accuracy 0.487200\n",
      "epochs 441 / 1500: loss 1.458341 : training accuracy 0.489200, and val accuracy 0.489200\n",
      "epochs 442 / 1500: loss 1.463948 : training accuracy 0.485000, and val accuracy 0.485000\n",
      "epochs 443 / 1500: loss 1.469546 : training accuracy 0.483200, and val accuracy 0.483200\n",
      "epochs 444 / 1500: loss 1.456266 : training accuracy 0.489000, and val accuracy 0.489000\n",
      "epochs 445 / 1500: loss 1.445769 : training accuracy 0.492200, and val accuracy 0.492200\n",
      "epochs 446 / 1500: loss 1.439775 : training accuracy 0.492700, and val accuracy 0.492700\n",
      "epochs 447 / 1500: loss 1.440108 : training accuracy 0.489400, and val accuracy 0.489400\n",
      "epochs 448 / 1500: loss 1.467226 : training accuracy 0.483500, and val accuracy 0.483500\n",
      "epochs 449 / 1500: loss 1.469314 : training accuracy 0.488000, and val accuracy 0.488000\n",
      "epochs 450 / 1500: loss 1.450411 : training accuracy 0.493700, and val accuracy 0.493700\n",
      "epochs 451 / 1500: loss 1.444540 : training accuracy 0.494200, and val accuracy 0.494200\n",
      "epochs 452 / 1500: loss 1.442636 : training accuracy 0.493800, and val accuracy 0.493800\n",
      "epochs 453 / 1500: loss 1.447085 : training accuracy 0.487100, and val accuracy 0.487100\n",
      "epochs 454 / 1500: loss 1.443612 : training accuracy 0.489000, and val accuracy 0.489000\n",
      "epochs 455 / 1500: loss 1.436955 : training accuracy 0.494000, and val accuracy 0.494000\n",
      "epochs 456 / 1500: loss 1.444851 : training accuracy 0.495000, and val accuracy 0.495000\n",
      "epochs 457 / 1500: loss 1.445677 : training accuracy 0.489900, and val accuracy 0.489900\n",
      "epochs 458 / 1500: loss 1.444032 : training accuracy 0.490100, and val accuracy 0.490100\n",
      "epochs 459 / 1500: loss 1.445040 : training accuracy 0.490400, and val accuracy 0.490400\n",
      "epochs 460 / 1500: loss 1.442676 : training accuracy 0.492100, and val accuracy 0.492100\n",
      "epochs 461 / 1500: loss 1.443053 : training accuracy 0.493600, and val accuracy 0.493600\n",
      "epochs 462 / 1500: loss 1.451640 : training accuracy 0.484600, and val accuracy 0.484600\n",
      "epochs 463 / 1500: loss 1.440693 : training accuracy 0.492700, and val accuracy 0.492700\n",
      "epochs 464 / 1500: loss 1.429205 : training accuracy 0.496700, and val accuracy 0.496700\n",
      "epochs 465 / 1500: loss 1.425247 : training accuracy 0.496800, and val accuracy 0.496800\n",
      "epochs 466 / 1500: loss 1.420704 : training accuracy 0.498200, and val accuracy 0.498200\n",
      "epochs 467 / 1500: loss 1.426490 : training accuracy 0.499400, and val accuracy 0.499400\n",
      "epochs 468 / 1500: loss 1.448783 : training accuracy 0.493500, and val accuracy 0.493500\n",
      "epochs 469 / 1500: loss 1.441333 : training accuracy 0.494700, and val accuracy 0.494700\n",
      "epochs 470 / 1500: loss 1.463578 : training accuracy 0.485200, and val accuracy 0.485200\n",
      "epochs 471 / 1500: loss 1.455054 : training accuracy 0.490500, and val accuracy 0.490500\n",
      "epochs 472 / 1500: loss 1.435587 : training accuracy 0.495600, and val accuracy 0.495600\n",
      "epochs 473 / 1500: loss 1.427260 : training accuracy 0.496600, and val accuracy 0.496600\n",
      "epochs 474 / 1500: loss 1.421622 : training accuracy 0.499000, and val accuracy 0.499000\n",
      "epochs 475 / 1500: loss 1.416761 : training accuracy 0.501100, and val accuracy 0.501100\n",
      "epochs 476 / 1500: loss 1.413416 : training accuracy 0.504400, and val accuracy 0.504400\n",
      "epochs 477 / 1500: loss 1.412661 : training accuracy 0.504800, and val accuracy 0.504800\n",
      "epochs 478 / 1500: loss 1.426656 : training accuracy 0.498600, and val accuracy 0.498600\n",
      "epochs 479 / 1500: loss 1.441459 : training accuracy 0.494100, and val accuracy 0.494100\n",
      "epochs 480 / 1500: loss 1.422863 : training accuracy 0.499500, and val accuracy 0.499500\n",
      "epochs 481 / 1500: loss 1.417732 : training accuracy 0.502300, and val accuracy 0.502300\n",
      "epochs 482 / 1500: loss 1.413849 : training accuracy 0.503300, and val accuracy 0.503300\n",
      "epochs 483 / 1500: loss 1.420092 : training accuracy 0.499500, and val accuracy 0.499500\n",
      "epochs 484 / 1500: loss 1.437242 : training accuracy 0.490100, and val accuracy 0.490100\n",
      "epochs 485 / 1500: loss 1.424217 : training accuracy 0.494400, and val accuracy 0.494400\n",
      "epochs 486 / 1500: loss 1.414387 : training accuracy 0.500700, and val accuracy 0.500700\n",
      "epochs 487 / 1500: loss 1.428468 : training accuracy 0.502100, and val accuracy 0.502100\n",
      "epochs 488 / 1500: loss 1.439173 : training accuracy 0.499200, and val accuracy 0.499200\n",
      "epochs 489 / 1500: loss 1.440397 : training accuracy 0.497800, and val accuracy 0.497800\n",
      "epochs 490 / 1500: loss 1.449090 : training accuracy 0.491200, and val accuracy 0.491200\n",
      "epochs 491 / 1500: loss 1.424795 : training accuracy 0.502900, and val accuracy 0.502900\n",
      "epochs 492 / 1500: loss 1.413841 : training accuracy 0.505600, and val accuracy 0.505600\n",
      "epochs 493 / 1500: loss 1.407785 : training accuracy 0.505100, and val accuracy 0.505100\n",
      "epochs 494 / 1500: loss 1.403668 : training accuracy 0.506300, and val accuracy 0.506300\n",
      "epochs 495 / 1500: loss 1.402208 : training accuracy 0.508800, and val accuracy 0.508800\n",
      "epochs 496 / 1500: loss 1.407547 : training accuracy 0.504300, and val accuracy 0.504300\n",
      "epochs 497 / 1500: loss 1.410303 : training accuracy 0.505000, and val accuracy 0.505000\n",
      "epochs 498 / 1500: loss 1.405145 : training accuracy 0.504600, and val accuracy 0.504600\n",
      "epochs 499 / 1500: loss 1.401311 : training accuracy 0.506700, and val accuracy 0.506700\n",
      "epochs 500 / 1500: loss 1.397640 : training accuracy 0.506400, and val accuracy 0.506400\n",
      "epochs 501 / 1500: loss 1.407989 : training accuracy 0.504400, and val accuracy 0.504400\n",
      "epochs 502 / 1500: loss 1.419539 : training accuracy 0.502100, and val accuracy 0.502100\n",
      "epochs 503 / 1500: loss 1.421446 : training accuracy 0.505800, and val accuracy 0.505800\n",
      "epochs 504 / 1500: loss 1.411106 : training accuracy 0.508400, and val accuracy 0.508400\n",
      "epochs 505 / 1500: loss 1.406450 : training accuracy 0.510900, and val accuracy 0.510900\n",
      "epochs 506 / 1500: loss 1.415295 : training accuracy 0.506300, and val accuracy 0.506300\n",
      "epochs 507 / 1500: loss 1.429892 : training accuracy 0.496700, and val accuracy 0.496700\n",
      "epochs 508 / 1500: loss 1.419026 : training accuracy 0.500700, and val accuracy 0.500700\n",
      "epochs 509 / 1500: loss 1.399518 : training accuracy 0.508600, and val accuracy 0.508600\n",
      "epochs 510 / 1500: loss 1.395083 : training accuracy 0.509300, and val accuracy 0.509300\n",
      "epochs 511 / 1500: loss 1.392442 : training accuracy 0.511800, and val accuracy 0.511800\n",
      "epochs 512 / 1500: loss 1.384045 : training accuracy 0.514700, and val accuracy 0.514700\n",
      "epochs 513 / 1500: loss 1.382277 : training accuracy 0.518200, and val accuracy 0.518200\n",
      "epochs 514 / 1500: loss 1.387241 : training accuracy 0.514100, and val accuracy 0.514100\n",
      "epochs 515 / 1500: loss 1.393806 : training accuracy 0.511300, and val accuracy 0.511300\n",
      "epochs 516 / 1500: loss 1.393266 : training accuracy 0.507600, and val accuracy 0.507600\n",
      "epochs 517 / 1500: loss 1.419003 : training accuracy 0.497600, and val accuracy 0.497600\n",
      "epochs 518 / 1500: loss 1.402552 : training accuracy 0.504900, and val accuracy 0.504900\n",
      "epochs 519 / 1500: loss 1.393025 : training accuracy 0.509300, and val accuracy 0.509300\n",
      "epochs 520 / 1500: loss 1.401872 : training accuracy 0.509800, and val accuracy 0.509800\n",
      "epochs 521 / 1500: loss 1.402116 : training accuracy 0.511400, and val accuracy 0.511400\n",
      "epochs 522 / 1500: loss 1.396100 : training accuracy 0.513600, and val accuracy 0.513600\n",
      "epochs 523 / 1500: loss 1.398673 : training accuracy 0.510300, and val accuracy 0.510300\n",
      "epochs 524 / 1500: loss 1.392397 : training accuracy 0.514000, and val accuracy 0.514000\n",
      "epochs 525 / 1500: loss 1.384925 : training accuracy 0.515900, and val accuracy 0.515900\n",
      "epochs 526 / 1500: loss 1.392421 : training accuracy 0.506700, and val accuracy 0.506700\n",
      "epochs 527 / 1500: loss 1.411324 : training accuracy 0.509100, and val accuracy 0.509100\n",
      "epochs 528 / 1500: loss 1.395637 : training accuracy 0.509000, and val accuracy 0.509000\n",
      "epochs 529 / 1500: loss 1.386418 : training accuracy 0.513600, and val accuracy 0.513600\n",
      "epochs 530 / 1500: loss 1.387424 : training accuracy 0.508400, and val accuracy 0.508400\n",
      "epochs 531 / 1500: loss 1.387506 : training accuracy 0.509000, and val accuracy 0.509000\n",
      "epochs 532 / 1500: loss 1.392576 : training accuracy 0.507600, and val accuracy 0.507600\n",
      "epochs 533 / 1500: loss 1.396196 : training accuracy 0.504800, and val accuracy 0.504800\n",
      "epochs 534 / 1500: loss 1.411619 : training accuracy 0.506300, and val accuracy 0.506300\n",
      "epochs 535 / 1500: loss 1.383737 : training accuracy 0.520000, and val accuracy 0.520000\n",
      "epochs 536 / 1500: loss 1.376172 : training accuracy 0.517700, and val accuracy 0.517700\n",
      "epochs 537 / 1500: loss 1.373853 : training accuracy 0.518300, and val accuracy 0.518300\n",
      "epochs 538 / 1500: loss 1.386312 : training accuracy 0.514000, and val accuracy 0.514000\n",
      "epochs 539 / 1500: loss 1.379774 : training accuracy 0.519700, and val accuracy 0.519700\n",
      "epochs 540 / 1500: loss 1.372772 : training accuracy 0.519800, and val accuracy 0.519800\n",
      "epochs 541 / 1500: loss 1.383381 : training accuracy 0.514100, and val accuracy 0.514100\n",
      "epochs 542 / 1500: loss 1.399558 : training accuracy 0.510800, and val accuracy 0.510800\n",
      "epochs 543 / 1500: loss 1.381250 : training accuracy 0.515200, and val accuracy 0.515200\n",
      "epochs 544 / 1500: loss 1.382437 : training accuracy 0.511400, and val accuracy 0.511400\n",
      "epochs 545 / 1500: loss 1.371731 : training accuracy 0.514200, and val accuracy 0.514200\n",
      "epochs 546 / 1500: loss 1.366185 : training accuracy 0.520200, and val accuracy 0.520200\n",
      "epochs 547 / 1500: loss 1.379325 : training accuracy 0.521300, and val accuracy 0.521300\n",
      "epochs 548 / 1500: loss 1.403853 : training accuracy 0.513900, and val accuracy 0.513900\n",
      "epochs 549 / 1500: loss 1.388996 : training accuracy 0.514500, and val accuracy 0.514500\n",
      "epochs 550 / 1500: loss 1.374117 : training accuracy 0.519200, and val accuracy 0.519200\n",
      "epochs 551 / 1500: loss 1.382449 : training accuracy 0.513800, and val accuracy 0.513800\n",
      "epochs 552 / 1500: loss 1.387502 : training accuracy 0.518600, and val accuracy 0.518600\n",
      "epochs 553 / 1500: loss 1.379675 : training accuracy 0.524000, and val accuracy 0.524000\n",
      "epochs 554 / 1500: loss 1.380058 : training accuracy 0.521000, and val accuracy 0.521000\n",
      "epochs 555 / 1500: loss 1.371478 : training accuracy 0.523400, and val accuracy 0.523400\n",
      "epochs 556 / 1500: loss 1.382543 : training accuracy 0.513800, and val accuracy 0.513800\n",
      "epochs 557 / 1500: loss 1.370385 : training accuracy 0.519100, and val accuracy 0.519100\n",
      "epochs 558 / 1500: loss 1.359496 : training accuracy 0.521800, and val accuracy 0.521800\n",
      "epochs 559 / 1500: loss 1.361142 : training accuracy 0.520300, and val accuracy 0.520300\n",
      "epochs 560 / 1500: loss 1.380619 : training accuracy 0.518000, and val accuracy 0.518000\n",
      "epochs 561 / 1500: loss 1.379845 : training accuracy 0.521100, and val accuracy 0.521100\n",
      "epochs 562 / 1500: loss 1.381834 : training accuracy 0.516800, and val accuracy 0.516800\n",
      "epochs 563 / 1500: loss 1.367734 : training accuracy 0.522700, and val accuracy 0.522700\n",
      "epochs 564 / 1500: loss 1.384165 : training accuracy 0.517400, and val accuracy 0.517400\n",
      "epochs 565 / 1500: loss 1.376631 : training accuracy 0.521500, and val accuracy 0.521500\n",
      "epochs 566 / 1500: loss 1.358651 : training accuracy 0.527000, and val accuracy 0.527000\n",
      "epochs 567 / 1500: loss 1.356217 : training accuracy 0.527900, and val accuracy 0.527900\n",
      "epochs 568 / 1500: loss 1.375274 : training accuracy 0.517100, and val accuracy 0.517100\n",
      "epochs 569 / 1500: loss 1.381821 : training accuracy 0.519900, and val accuracy 0.519900\n",
      "epochs 570 / 1500: loss 1.356816 : training accuracy 0.530500, and val accuracy 0.530500\n",
      "epochs 571 / 1500: loss 1.358458 : training accuracy 0.528000, and val accuracy 0.528000\n",
      "epochs 572 / 1500: loss 1.361427 : training accuracy 0.519000, and val accuracy 0.519000\n",
      "epochs 573 / 1500: loss 1.387906 : training accuracy 0.507300, and val accuracy 0.507300\n",
      "epochs 574 / 1500: loss 1.359200 : training accuracy 0.523200, and val accuracy 0.523200\n",
      "epochs 575 / 1500: loss 1.361336 : training accuracy 0.527500, and val accuracy 0.527500\n",
      "epochs 576 / 1500: loss 1.361068 : training accuracy 0.523800, and val accuracy 0.523800\n",
      "epochs 577 / 1500: loss 1.368629 : training accuracy 0.520300, and val accuracy 0.520300\n",
      "epochs 578 / 1500: loss 1.356651 : training accuracy 0.527500, and val accuracy 0.527500\n",
      "epochs 579 / 1500: loss 1.349228 : training accuracy 0.529700, and val accuracy 0.529700\n",
      "epochs 580 / 1500: loss 1.336300 : training accuracy 0.535300, and val accuracy 0.535300\n",
      "epochs 581 / 1500: loss 1.341447 : training accuracy 0.526800, and val accuracy 0.526800\n",
      "epochs 582 / 1500: loss 1.370371 : training accuracy 0.521700, and val accuracy 0.521700\n",
      "epochs 583 / 1500: loss 1.380450 : training accuracy 0.509900, and val accuracy 0.509900\n",
      "epochs 584 / 1500: loss 1.373888 : training accuracy 0.527200, and val accuracy 0.527200\n",
      "epochs 585 / 1500: loss 1.354105 : training accuracy 0.528000, and val accuracy 0.528000\n",
      "epochs 586 / 1500: loss 1.346141 : training accuracy 0.532200, and val accuracy 0.532200\n",
      "epochs 587 / 1500: loss 1.346182 : training accuracy 0.531300, and val accuracy 0.531300\n",
      "epochs 588 / 1500: loss 1.346195 : training accuracy 0.533600, and val accuracy 0.533600\n",
      "epochs 589 / 1500: loss 1.346652 : training accuracy 0.535300, and val accuracy 0.535300\n",
      "epochs 590 / 1500: loss 1.349840 : training accuracy 0.525300, and val accuracy 0.525300\n",
      "epochs 591 / 1500: loss 1.357172 : training accuracy 0.523300, and val accuracy 0.523300\n",
      "epochs 592 / 1500: loss 1.343822 : training accuracy 0.529300, and val accuracy 0.529300\n",
      "epochs 593 / 1500: loss 1.342958 : training accuracy 0.533700, and val accuracy 0.533700\n",
      "epochs 594 / 1500: loss 1.350011 : training accuracy 0.523900, and val accuracy 0.523900\n",
      "epochs 595 / 1500: loss 1.345420 : training accuracy 0.522600, and val accuracy 0.522600\n",
      "epochs 596 / 1500: loss 1.344478 : training accuracy 0.529800, and val accuracy 0.529800\n",
      "epochs 597 / 1500: loss 1.337543 : training accuracy 0.531300, and val accuracy 0.531300\n",
      "epochs 598 / 1500: loss 1.330480 : training accuracy 0.532600, and val accuracy 0.532600\n",
      "epochs 599 / 1500: loss 1.346143 : training accuracy 0.525500, and val accuracy 0.525500\n",
      "epochs 600 / 1500: loss 1.345230 : training accuracy 0.533100, and val accuracy 0.533100\n",
      "epochs 601 / 1500: loss 1.345700 : training accuracy 0.529800, and val accuracy 0.529800\n",
      "epochs 602 / 1500: loss 1.335165 : training accuracy 0.531200, and val accuracy 0.531200\n",
      "epochs 603 / 1500: loss 1.350719 : training accuracy 0.531000, and val accuracy 0.531000\n",
      "epochs 604 / 1500: loss 1.336850 : training accuracy 0.530700, and val accuracy 0.530700\n",
      "epochs 605 / 1500: loss 1.360559 : training accuracy 0.525800, and val accuracy 0.525800\n",
      "epochs 606 / 1500: loss 1.337564 : training accuracy 0.533800, and val accuracy 0.533800\n",
      "epochs 607 / 1500: loss 1.338074 : training accuracy 0.529700, and val accuracy 0.529700\n",
      "epochs 608 / 1500: loss 1.351028 : training accuracy 0.520100, and val accuracy 0.520100\n",
      "epochs 609 / 1500: loss 1.336987 : training accuracy 0.532700, and val accuracy 0.532700\n",
      "epochs 610 / 1500: loss 1.334492 : training accuracy 0.524500, and val accuracy 0.524500\n",
      "epochs 611 / 1500: loss 1.329165 : training accuracy 0.525400, and val accuracy 0.525400\n",
      "epochs 612 / 1500: loss 1.346540 : training accuracy 0.524100, and val accuracy 0.524100\n",
      "epochs 613 / 1500: loss 1.331251 : training accuracy 0.534500, and val accuracy 0.534500\n",
      "epochs 614 / 1500: loss 1.346834 : training accuracy 0.528100, and val accuracy 0.528100\n",
      "epochs 615 / 1500: loss 1.360825 : training accuracy 0.530700, and val accuracy 0.530700\n",
      "epochs 616 / 1500: loss 1.349948 : training accuracy 0.535800, and val accuracy 0.535800\n",
      "epochs 617 / 1500: loss 1.351770 : training accuracy 0.538500, and val accuracy 0.538500\n",
      "epochs 618 / 1500: loss 1.337165 : training accuracy 0.541900, and val accuracy 0.541900\n",
      "epochs 619 / 1500: loss 1.330999 : training accuracy 0.542700, and val accuracy 0.542700\n",
      "epochs 620 / 1500: loss 1.332113 : training accuracy 0.539900, and val accuracy 0.539900\n",
      "epochs 621 / 1500: loss 1.334949 : training accuracy 0.541000, and val accuracy 0.541000\n",
      "epochs 622 / 1500: loss 1.340474 : training accuracy 0.533700, and val accuracy 0.533700\n",
      "epochs 623 / 1500: loss 1.335055 : training accuracy 0.526900, and val accuracy 0.526900\n",
      "epochs 624 / 1500: loss 1.343273 : training accuracy 0.529700, and val accuracy 0.529700\n",
      "epochs 625 / 1500: loss 1.323765 : training accuracy 0.541800, and val accuracy 0.541800\n",
      "epochs 626 / 1500: loss 1.336210 : training accuracy 0.531000, and val accuracy 0.531000\n",
      "epochs 627 / 1500: loss 1.320898 : training accuracy 0.538700, and val accuracy 0.538700\n",
      "epochs 628 / 1500: loss 1.332745 : training accuracy 0.535500, and val accuracy 0.535500\n",
      "epochs 629 / 1500: loss 1.333030 : training accuracy 0.529900, and val accuracy 0.529900\n",
      "epochs 630 / 1500: loss 1.344514 : training accuracy 0.527700, and val accuracy 0.527700\n",
      "epochs 631 / 1500: loss 1.354627 : training accuracy 0.521300, and val accuracy 0.521300\n",
      "epochs 632 / 1500: loss 1.334798 : training accuracy 0.534000, and val accuracy 0.534000\n",
      "epochs 633 / 1500: loss 1.321005 : training accuracy 0.540300, and val accuracy 0.540300\n",
      "epochs 634 / 1500: loss 1.313699 : training accuracy 0.542200, and val accuracy 0.542200\n",
      "epochs 635 / 1500: loss 1.313386 : training accuracy 0.539900, and val accuracy 0.539900\n",
      "epochs 636 / 1500: loss 1.323174 : training accuracy 0.531200, and val accuracy 0.531200\n",
      "epochs 637 / 1500: loss 1.350384 : training accuracy 0.519400, and val accuracy 0.519400\n",
      "epochs 638 / 1500: loss 1.330570 : training accuracy 0.532800, and val accuracy 0.532800\n",
      "epochs 639 / 1500: loss 1.344592 : training accuracy 0.535300, and val accuracy 0.535300\n",
      "epochs 640 / 1500: loss 1.347163 : training accuracy 0.536400, and val accuracy 0.536400\n",
      "epochs 641 / 1500: loss 1.333250 : training accuracy 0.540200, and val accuracy 0.540200\n",
      "epochs 642 / 1500: loss 1.336445 : training accuracy 0.532900, and val accuracy 0.532900\n",
      "epochs 643 / 1500: loss 1.358641 : training accuracy 0.528500, and val accuracy 0.528500\n",
      "epochs 644 / 1500: loss 1.341153 : training accuracy 0.543900, and val accuracy 0.543900\n",
      "epochs 645 / 1500: loss 1.346169 : training accuracy 0.537300, and val accuracy 0.537300\n",
      "epochs 646 / 1500: loss 1.336054 : training accuracy 0.538700, and val accuracy 0.538700\n",
      "epochs 647 / 1500: loss 1.342239 : training accuracy 0.542300, and val accuracy 0.542300\n",
      "epochs 648 / 1500: loss 1.332462 : training accuracy 0.526600, and val accuracy 0.526600\n",
      "epochs 649 / 1500: loss 1.305644 : training accuracy 0.541900, and val accuracy 0.541900\n",
      "epochs 650 / 1500: loss 1.304571 : training accuracy 0.545300, and val accuracy 0.545300\n",
      "epochs 651 / 1500: loss 1.300128 : training accuracy 0.545300, and val accuracy 0.545300\n",
      "epochs 652 / 1500: loss 1.297442 : training accuracy 0.545300, and val accuracy 0.545300\n",
      "epochs 653 / 1500: loss 1.295493 : training accuracy 0.544500, and val accuracy 0.544500\n",
      "epochs 654 / 1500: loss 1.331222 : training accuracy 0.542900, and val accuracy 0.542900\n",
      "epochs 655 / 1500: loss 1.310186 : training accuracy 0.538800, and val accuracy 0.538800\n",
      "epochs 656 / 1500: loss 1.312045 : training accuracy 0.537700, and val accuracy 0.537700\n",
      "epochs 657 / 1500: loss 1.304478 : training accuracy 0.546600, and val accuracy 0.546600\n",
      "epochs 658 / 1500: loss 1.306342 : training accuracy 0.553400, and val accuracy 0.553400\n",
      "epochs 659 / 1500: loss 1.340492 : training accuracy 0.540200, and val accuracy 0.540200\n",
      "epochs 660 / 1500: loss 1.321816 : training accuracy 0.538400, and val accuracy 0.538400\n",
      "epochs 661 / 1500: loss 1.308854 : training accuracy 0.544400, and val accuracy 0.544400\n",
      "epochs 662 / 1500: loss 1.309804 : training accuracy 0.542500, and val accuracy 0.542500\n",
      "epochs 663 / 1500: loss 1.330757 : training accuracy 0.523500, and val accuracy 0.523500\n",
      "epochs 664 / 1500: loss 1.314120 : training accuracy 0.535200, and val accuracy 0.535200\n",
      "epochs 665 / 1500: loss 1.325006 : training accuracy 0.536700, and val accuracy 0.536700\n",
      "epochs 666 / 1500: loss 1.299749 : training accuracy 0.544800, and val accuracy 0.544800\n",
      "epochs 667 / 1500: loss 1.340077 : training accuracy 0.530600, and val accuracy 0.530600\n",
      "epochs 668 / 1500: loss 1.312811 : training accuracy 0.541800, and val accuracy 0.541800\n",
      "epochs 669 / 1500: loss 1.298106 : training accuracy 0.545400, and val accuracy 0.545400\n",
      "epochs 670 / 1500: loss 1.294287 : training accuracy 0.546300, and val accuracy 0.546300\n",
      "epochs 671 / 1500: loss 1.294422 : training accuracy 0.541300, and val accuracy 0.541300\n",
      "epochs 672 / 1500: loss 1.327154 : training accuracy 0.538400, and val accuracy 0.538400\n",
      "epochs 673 / 1500: loss 1.314754 : training accuracy 0.548400, and val accuracy 0.548400\n",
      "epochs 674 / 1500: loss 1.311668 : training accuracy 0.542200, and val accuracy 0.542200\n",
      "epochs 675 / 1500: loss 1.297172 : training accuracy 0.549000, and val accuracy 0.549000\n",
      "epochs 676 / 1500: loss 1.291120 : training accuracy 0.554400, and val accuracy 0.554400\n",
      "epochs 677 / 1500: loss 1.311659 : training accuracy 0.531400, and val accuracy 0.531400\n",
      "epochs 678 / 1500: loss 1.293774 : training accuracy 0.544000, and val accuracy 0.544000\n",
      "epochs 679 / 1500: loss 1.288617 : training accuracy 0.547100, and val accuracy 0.547100\n",
      "epochs 680 / 1500: loss 1.313459 : training accuracy 0.533700, and val accuracy 0.533700\n",
      "epochs 681 / 1500: loss 1.289490 : training accuracy 0.546700, and val accuracy 0.546700\n",
      "epochs 682 / 1500: loss 1.303578 : training accuracy 0.536700, and val accuracy 0.536700\n",
      "epochs 683 / 1500: loss 1.296479 : training accuracy 0.547200, and val accuracy 0.547200\n",
      "epochs 684 / 1500: loss 1.312551 : training accuracy 0.550100, and val accuracy 0.550100\n",
      "epochs 685 / 1500: loss 1.304117 : training accuracy 0.552300, and val accuracy 0.552300\n",
      "epochs 686 / 1500: loss 1.297857 : training accuracy 0.553300, and val accuracy 0.553300\n",
      "epochs 687 / 1500: loss 1.293496 : training accuracy 0.555600, and val accuracy 0.555600\n",
      "epochs 688 / 1500: loss 1.307732 : training accuracy 0.543600, and val accuracy 0.543600\n",
      "epochs 689 / 1500: loss 1.308943 : training accuracy 0.543000, and val accuracy 0.543000\n",
      "epochs 690 / 1500: loss 1.315339 : training accuracy 0.546600, and val accuracy 0.546600\n",
      "epochs 691 / 1500: loss 1.298305 : training accuracy 0.554000, and val accuracy 0.554000\n",
      "epochs 692 / 1500: loss 1.308825 : training accuracy 0.539900, and val accuracy 0.539900\n",
      "epochs 693 / 1500: loss 1.296677 : training accuracy 0.545700, and val accuracy 0.545700\n",
      "epochs 694 / 1500: loss 1.300841 : training accuracy 0.536400, and val accuracy 0.536400\n",
      "epochs 695 / 1500: loss 1.311285 : training accuracy 0.535400, and val accuracy 0.535400\n",
      "epochs 696 / 1500: loss 1.297253 : training accuracy 0.542100, and val accuracy 0.542100\n",
      "epochs 697 / 1500: loss 1.298742 : training accuracy 0.543100, and val accuracy 0.543100\n",
      "epochs 698 / 1500: loss 1.283896 : training accuracy 0.546300, and val accuracy 0.546300\n",
      "epochs 699 / 1500: loss 1.285353 : training accuracy 0.544600, and val accuracy 0.544600\n",
      "epochs 700 / 1500: loss 1.290686 : training accuracy 0.551700, and val accuracy 0.551700\n",
      "epochs 701 / 1500: loss 1.281940 : training accuracy 0.553600, and val accuracy 0.553600\n",
      "epochs 702 / 1500: loss 1.300619 : training accuracy 0.548700, and val accuracy 0.548700\n",
      "epochs 703 / 1500: loss 1.307956 : training accuracy 0.547700, and val accuracy 0.547700\n",
      "epochs 704 / 1500: loss 1.336804 : training accuracy 0.549200, and val accuracy 0.549200\n",
      "epochs 705 / 1500: loss 1.298379 : training accuracy 0.555600, and val accuracy 0.555600\n",
      "epochs 706 / 1500: loss 1.291594 : training accuracy 0.551100, and val accuracy 0.551100\n",
      "epochs 707 / 1500: loss 1.289511 : training accuracy 0.550500, and val accuracy 0.550500\n",
      "epochs 708 / 1500: loss 1.280701 : training accuracy 0.554100, and val accuracy 0.554100\n",
      "epochs 709 / 1500: loss 1.274097 : training accuracy 0.557900, and val accuracy 0.557900\n",
      "epochs 710 / 1500: loss 1.290633 : training accuracy 0.542100, and val accuracy 0.542100\n",
      "epochs 711 / 1500: loss 1.283286 : training accuracy 0.548700, and val accuracy 0.548700\n",
      "epochs 712 / 1500: loss 1.297177 : training accuracy 0.557600, and val accuracy 0.557600\n",
      "epochs 713 / 1500: loss 1.300299 : training accuracy 0.552900, and val accuracy 0.552900\n",
      "epochs 714 / 1500: loss 1.277897 : training accuracy 0.561000, and val accuracy 0.561000\n",
      "epochs 715 / 1500: loss 1.273646 : training accuracy 0.559800, and val accuracy 0.559800\n",
      "epochs 716 / 1500: loss 1.272305 : training accuracy 0.556900, and val accuracy 0.556900\n",
      "epochs 717 / 1500: loss 1.277947 : training accuracy 0.555100, and val accuracy 0.555100\n",
      "epochs 718 / 1500: loss 1.277158 : training accuracy 0.551200, and val accuracy 0.551200\n",
      "epochs 719 / 1500: loss 1.270039 : training accuracy 0.561400, and val accuracy 0.561400\n",
      "epochs 720 / 1500: loss 1.331736 : training accuracy 0.539900, and val accuracy 0.539900\n",
      "epochs 721 / 1500: loss 1.297762 : training accuracy 0.556300, and val accuracy 0.556300\n",
      "epochs 722 / 1500: loss 1.280616 : training accuracy 0.556900, and val accuracy 0.556900\n",
      "epochs 723 / 1500: loss 1.270207 : training accuracy 0.560400, and val accuracy 0.560400\n",
      "epochs 724 / 1500: loss 1.276350 : training accuracy 0.557300, and val accuracy 0.557300\n",
      "epochs 725 / 1500: loss 1.289854 : training accuracy 0.549400, and val accuracy 0.549400\n",
      "epochs 726 / 1500: loss 1.302528 : training accuracy 0.550000, and val accuracy 0.550000\n",
      "epochs 727 / 1500: loss 1.290837 : training accuracy 0.555500, and val accuracy 0.555500\n",
      "epochs 728 / 1500: loss 1.320248 : training accuracy 0.550700, and val accuracy 0.550700\n",
      "epochs 729 / 1500: loss 1.267477 : training accuracy 0.562400, and val accuracy 0.562400\n",
      "epochs 730 / 1500: loss 1.276261 : training accuracy 0.560900, and val accuracy 0.560900\n",
      "epochs 731 / 1500: loss 1.282527 : training accuracy 0.562100, and val accuracy 0.562100\n",
      "epochs 732 / 1500: loss 1.294033 : training accuracy 0.555700, and val accuracy 0.555700\n",
      "epochs 733 / 1500: loss 1.284349 : training accuracy 0.554400, and val accuracy 0.554400\n",
      "epochs 734 / 1500: loss 1.274040 : training accuracy 0.553900, and val accuracy 0.553900\n",
      "epochs 735 / 1500: loss 1.263343 : training accuracy 0.560600, and val accuracy 0.560600\n",
      "epochs 736 / 1500: loss 1.270814 : training accuracy 0.556000, and val accuracy 0.556000\n",
      "epochs 737 / 1500: loss 1.272430 : training accuracy 0.559100, and val accuracy 0.559100\n",
      "epochs 738 / 1500: loss 1.277201 : training accuracy 0.553200, and val accuracy 0.553200\n",
      "epochs 739 / 1500: loss 1.286627 : training accuracy 0.556000, and val accuracy 0.556000\n",
      "epochs 740 / 1500: loss 1.289572 : training accuracy 0.548300, and val accuracy 0.548300\n",
      "epochs 741 / 1500: loss 1.278993 : training accuracy 0.551300, and val accuracy 0.551300\n",
      "epochs 742 / 1500: loss 1.277741 : training accuracy 0.553000, and val accuracy 0.553000\n",
      "epochs 743 / 1500: loss 1.263039 : training accuracy 0.560900, and val accuracy 0.560900\n",
      "epochs 744 / 1500: loss 1.251956 : training accuracy 0.565400, and val accuracy 0.565400\n",
      "epochs 745 / 1500: loss 1.261481 : training accuracy 0.568500, and val accuracy 0.568500\n",
      "epochs 746 / 1500: loss 1.302758 : training accuracy 0.550800, and val accuracy 0.550800\n",
      "epochs 747 / 1500: loss 1.267012 : training accuracy 0.554500, and val accuracy 0.554500\n",
      "epochs 748 / 1500: loss 1.256437 : training accuracy 0.557000, and val accuracy 0.557000\n",
      "epochs 749 / 1500: loss 1.266188 : training accuracy 0.554700, and val accuracy 0.554700\n",
      "epochs 750 / 1500: loss 1.264065 : training accuracy 0.556700, and val accuracy 0.556700\n",
      "epochs 751 / 1500: loss 1.269599 : training accuracy 0.553000, and val accuracy 0.553000\n",
      "epochs 752 / 1500: loss 1.288840 : training accuracy 0.551400, and val accuracy 0.551400\n",
      "epochs 753 / 1500: loss 1.267120 : training accuracy 0.557800, and val accuracy 0.557800\n",
      "epochs 754 / 1500: loss 1.250361 : training accuracy 0.558500, and val accuracy 0.558500\n",
      "epochs 755 / 1500: loss 1.261733 : training accuracy 0.553700, and val accuracy 0.553700\n",
      "epochs 756 / 1500: loss 1.257887 : training accuracy 0.564200, and val accuracy 0.564200\n",
      "epochs 757 / 1500: loss 1.276688 : training accuracy 0.550900, and val accuracy 0.550900\n",
      "epochs 758 / 1500: loss 1.269522 : training accuracy 0.552700, and val accuracy 0.552700\n",
      "epochs 759 / 1500: loss 1.271992 : training accuracy 0.554900, and val accuracy 0.554900\n",
      "epochs 760 / 1500: loss 1.251828 : training accuracy 0.562400, and val accuracy 0.562400\n",
      "epochs 761 / 1500: loss 1.283331 : training accuracy 0.545200, and val accuracy 0.545200\n",
      "epochs 762 / 1500: loss 1.259765 : training accuracy 0.563000, and val accuracy 0.563000\n",
      "epochs 763 / 1500: loss 1.244918 : training accuracy 0.563900, and val accuracy 0.563900\n",
      "epochs 764 / 1500: loss 1.237364 : training accuracy 0.565300, and val accuracy 0.565300\n",
      "epochs 765 / 1500: loss 1.279085 : training accuracy 0.555700, and val accuracy 0.555700\n",
      "epochs 766 / 1500: loss 1.255674 : training accuracy 0.561100, and val accuracy 0.561100\n",
      "epochs 767 / 1500: loss 1.243407 : training accuracy 0.567900, and val accuracy 0.567900\n",
      "epochs 768 / 1500: loss 1.234557 : training accuracy 0.570700, and val accuracy 0.570700\n",
      "epochs 769 / 1500: loss 1.249305 : training accuracy 0.566300, and val accuracy 0.566300\n",
      "epochs 770 / 1500: loss 1.265782 : training accuracy 0.566600, and val accuracy 0.566600\n",
      "epochs 771 / 1500: loss 1.262775 : training accuracy 0.562000, and val accuracy 0.562000\n",
      "epochs 772 / 1500: loss 1.254994 : training accuracy 0.563400, and val accuracy 0.563400\n",
      "epochs 773 / 1500: loss 1.259463 : training accuracy 0.567800, and val accuracy 0.567800\n",
      "epochs 774 / 1500: loss 1.254709 : training accuracy 0.569200, and val accuracy 0.569200\n",
      "epochs 775 / 1500: loss 1.247075 : training accuracy 0.564700, and val accuracy 0.564700\n",
      "epochs 776 / 1500: loss 1.267367 : training accuracy 0.556800, and val accuracy 0.556800\n",
      "epochs 777 / 1500: loss 1.262890 : training accuracy 0.557500, and val accuracy 0.557500\n",
      "epochs 778 / 1500: loss 1.250619 : training accuracy 0.561200, and val accuracy 0.561200\n",
      "epochs 779 / 1500: loss 1.245993 : training accuracy 0.564300, and val accuracy 0.564300\n",
      "epochs 780 / 1500: loss 1.244310 : training accuracy 0.566600, and val accuracy 0.566600\n",
      "epochs 781 / 1500: loss 1.286353 : training accuracy 0.564000, and val accuracy 0.564000\n",
      "epochs 782 / 1500: loss 1.269078 : training accuracy 0.554500, and val accuracy 0.554500\n",
      "epochs 783 / 1500: loss 1.256833 : training accuracy 0.556000, and val accuracy 0.556000\n",
      "epochs 784 / 1500: loss 1.246210 : training accuracy 0.561500, and val accuracy 0.561500\n",
      "epochs 785 / 1500: loss 1.237944 : training accuracy 0.564600, and val accuracy 0.564600\n",
      "epochs 786 / 1500: loss 1.234518 : training accuracy 0.562900, and val accuracy 0.562900\n",
      "epochs 787 / 1500: loss 1.309819 : training accuracy 0.548500, and val accuracy 0.548500\n",
      "epochs 788 / 1500: loss 1.256346 : training accuracy 0.573500, and val accuracy 0.573500\n",
      "epochs 789 / 1500: loss 1.242570 : training accuracy 0.569300, and val accuracy 0.569300\n",
      "epochs 790 / 1500: loss 1.269505 : training accuracy 0.553500, and val accuracy 0.553500\n",
      "epochs 791 / 1500: loss 1.237101 : training accuracy 0.570100, and val accuracy 0.570100\n",
      "epochs 792 / 1500: loss 1.235907 : training accuracy 0.568300, and val accuracy 0.568300\n",
      "epochs 793 / 1500: loss 1.254275 : training accuracy 0.561700, and val accuracy 0.561700\n",
      "epochs 794 / 1500: loss 1.276010 : training accuracy 0.564100, and val accuracy 0.564100\n",
      "epochs 795 / 1500: loss 1.293069 : training accuracy 0.558200, and val accuracy 0.558200\n",
      "epochs 796 / 1500: loss 1.253946 : training accuracy 0.564200, and val accuracy 0.564200\n",
      "epochs 797 / 1500: loss 1.236859 : training accuracy 0.570200, and val accuracy 0.570200\n",
      "epochs 798 / 1500: loss 1.252611 : training accuracy 0.567300, and val accuracy 0.567300\n",
      "epochs 799 / 1500: loss 1.244997 : training accuracy 0.573200, and val accuracy 0.573200\n",
      "epochs 800 / 1500: loss 1.239405 : training accuracy 0.575700, and val accuracy 0.575700\n",
      "epochs 801 / 1500: loss 1.235572 : training accuracy 0.571000, and val accuracy 0.571000\n",
      "epochs 802 / 1500: loss 1.224030 : training accuracy 0.576800, and val accuracy 0.576800\n",
      "epochs 803 / 1500: loss 1.223072 : training accuracy 0.579800, and val accuracy 0.579800\n",
      "epochs 804 / 1500: loss 1.243824 : training accuracy 0.570900, and val accuracy 0.570900\n",
      "epochs 805 / 1500: loss 1.272829 : training accuracy 0.557500, and val accuracy 0.557500\n",
      "epochs 806 / 1500: loss 1.233974 : training accuracy 0.570600, and val accuracy 0.570600\n",
      "epochs 807 / 1500: loss 1.232623 : training accuracy 0.567800, and val accuracy 0.567800\n",
      "epochs 808 / 1500: loss 1.262161 : training accuracy 0.571000, and val accuracy 0.571000\n",
      "epochs 809 / 1500: loss 1.246146 : training accuracy 0.570400, and val accuracy 0.570400\n",
      "epochs 810 / 1500: loss 1.234240 : training accuracy 0.570900, and val accuracy 0.570900\n",
      "epochs 811 / 1500: loss 1.210453 : training accuracy 0.579800, and val accuracy 0.579800\n",
      "epochs 812 / 1500: loss 1.235824 : training accuracy 0.573500, and val accuracy 0.573500\n",
      "epochs 813 / 1500: loss 1.236685 : training accuracy 0.573500, and val accuracy 0.573500\n",
      "epochs 814 / 1500: loss 1.232450 : training accuracy 0.572700, and val accuracy 0.572700\n",
      "epochs 815 / 1500: loss 1.249017 : training accuracy 0.571300, and val accuracy 0.571300\n",
      "epochs 816 / 1500: loss 1.217444 : training accuracy 0.575600, and val accuracy 0.575600\n",
      "epochs 817 / 1500: loss 1.222491 : training accuracy 0.569700, and val accuracy 0.569700\n",
      "epochs 818 / 1500: loss 1.219631 : training accuracy 0.573600, and val accuracy 0.573600\n",
      "epochs 819 / 1500: loss 1.223442 : training accuracy 0.567300, and val accuracy 0.567300\n",
      "epochs 820 / 1500: loss 1.230125 : training accuracy 0.573800, and val accuracy 0.573800\n",
      "epochs 821 / 1500: loss 1.242662 : training accuracy 0.573200, and val accuracy 0.573200\n",
      "epochs 822 / 1500: loss 1.234537 : training accuracy 0.569000, and val accuracy 0.569000\n",
      "epochs 823 / 1500: loss 1.217742 : training accuracy 0.573400, and val accuracy 0.573400\n",
      "epochs 824 / 1500: loss 1.212559 : training accuracy 0.575100, and val accuracy 0.575100\n",
      "epochs 825 / 1500: loss 1.213788 : training accuracy 0.577300, and val accuracy 0.577300\n",
      "epochs 826 / 1500: loss 1.307344 : training accuracy 0.547300, and val accuracy 0.547300\n",
      "epochs 827 / 1500: loss 1.239489 : training accuracy 0.573300, and val accuracy 0.573300\n",
      "epochs 828 / 1500: loss 1.218649 : training accuracy 0.568800, and val accuracy 0.568800\n",
      "epochs 829 / 1500: loss 1.235828 : training accuracy 0.558200, and val accuracy 0.558200\n",
      "epochs 830 / 1500: loss 1.257126 : training accuracy 0.558600, and val accuracy 0.558600\n",
      "epochs 831 / 1500: loss 1.224166 : training accuracy 0.576400, and val accuracy 0.576400\n",
      "epochs 832 / 1500: loss 1.228380 : training accuracy 0.581100, and val accuracy 0.581100\n",
      "epochs 833 / 1500: loss 1.204672 : training accuracy 0.582300, and val accuracy 0.582300\n",
      "epochs 834 / 1500: loss 1.222605 : training accuracy 0.573900, and val accuracy 0.573900\n",
      "epochs 835 / 1500: loss 1.223489 : training accuracy 0.578800, and val accuracy 0.578800\n",
      "epochs 836 / 1500: loss 1.220802 : training accuracy 0.582700, and val accuracy 0.582700\n",
      "epochs 837 / 1500: loss 1.216451 : training accuracy 0.577900, and val accuracy 0.577900\n",
      "epochs 838 / 1500: loss 1.226420 : training accuracy 0.571900, and val accuracy 0.571900\n",
      "epochs 839 / 1500: loss 1.225402 : training accuracy 0.574100, and val accuracy 0.574100\n",
      "epochs 840 / 1500: loss 1.244850 : training accuracy 0.572800, and val accuracy 0.572800\n",
      "epochs 841 / 1500: loss 1.263868 : training accuracy 0.573900, and val accuracy 0.573900\n",
      "epochs 842 / 1500: loss 1.237459 : training accuracy 0.574200, and val accuracy 0.574200\n",
      "epochs 843 / 1500: loss 1.223208 : training accuracy 0.582800, and val accuracy 0.582800\n",
      "epochs 844 / 1500: loss 1.241979 : training accuracy 0.570800, and val accuracy 0.570800\n",
      "epochs 845 / 1500: loss 1.244491 : training accuracy 0.565800, and val accuracy 0.565800\n",
      "epochs 846 / 1500: loss 1.221543 : training accuracy 0.578000, and val accuracy 0.578000\n",
      "epochs 847 / 1500: loss 1.253303 : training accuracy 0.566900, and val accuracy 0.566900\n",
      "epochs 848 / 1500: loss 1.222579 : training accuracy 0.589300, and val accuracy 0.589300\n",
      "epochs 849 / 1500: loss 1.226453 : training accuracy 0.563000, and val accuracy 0.563000\n",
      "epochs 850 / 1500: loss 1.223919 : training accuracy 0.569500, and val accuracy 0.569500\n",
      "epochs 851 / 1500: loss 1.203019 : training accuracy 0.582900, and val accuracy 0.582900\n",
      "epochs 852 / 1500: loss 1.201316 : training accuracy 0.581100, and val accuracy 0.581100\n",
      "epochs 853 / 1500: loss 1.233651 : training accuracy 0.574800, and val accuracy 0.574800\n",
      "epochs 854 / 1500: loss 1.247686 : training accuracy 0.575700, and val accuracy 0.575700\n",
      "epochs 855 / 1500: loss 1.224159 : training accuracy 0.567700, and val accuracy 0.567700\n",
      "epochs 856 / 1500: loss 1.220334 : training accuracy 0.579000, and val accuracy 0.579000\n",
      "epochs 857 / 1500: loss 1.207552 : training accuracy 0.586000, and val accuracy 0.586000\n",
      "epochs 858 / 1500: loss 1.208598 : training accuracy 0.585000, and val accuracy 0.585000\n",
      "epochs 859 / 1500: loss 1.264929 : training accuracy 0.579900, and val accuracy 0.579900\n",
      "epochs 860 / 1500: loss 1.219182 : training accuracy 0.586300, and val accuracy 0.586300\n",
      "epochs 861 / 1500: loss 1.210650 : training accuracy 0.577000, and val accuracy 0.577000\n",
      "epochs 862 / 1500: loss 1.227664 : training accuracy 0.579800, and val accuracy 0.579800\n",
      "epochs 863 / 1500: loss 1.226145 : training accuracy 0.576700, and val accuracy 0.576700\n",
      "epochs 864 / 1500: loss 1.219815 : training accuracy 0.585900, and val accuracy 0.585900\n",
      "epochs 865 / 1500: loss 1.206715 : training accuracy 0.582600, and val accuracy 0.582600\n",
      "epochs 866 / 1500: loss 1.228328 : training accuracy 0.567500, and val accuracy 0.567500\n",
      "epochs 867 / 1500: loss 1.215580 : training accuracy 0.564600, and val accuracy 0.564600\n",
      "epochs 868 / 1500: loss 1.195300 : training accuracy 0.580200, and val accuracy 0.580200\n",
      "epochs 869 / 1500: loss 1.207472 : training accuracy 0.575100, and val accuracy 0.575100\n",
      "epochs 870 / 1500: loss 1.233178 : training accuracy 0.565900, and val accuracy 0.565900\n",
      "epochs 871 / 1500: loss 1.208571 : training accuracy 0.577000, and val accuracy 0.577000\n",
      "epochs 872 / 1500: loss 1.192704 : training accuracy 0.580000, and val accuracy 0.580000\n",
      "epochs 873 / 1500: loss 1.208599 : training accuracy 0.573400, and val accuracy 0.573400\n",
      "epochs 874 / 1500: loss 1.197298 : training accuracy 0.575600, and val accuracy 0.575600\n",
      "epochs 875 / 1500: loss 1.200982 : training accuracy 0.575400, and val accuracy 0.575400\n",
      "epochs 876 / 1500: loss 1.197292 : training accuracy 0.579000, and val accuracy 0.579000\n",
      "epochs 877 / 1500: loss 1.199894 : training accuracy 0.577400, and val accuracy 0.577400\n",
      "epochs 878 / 1500: loss 1.273683 : training accuracy 0.558400, and val accuracy 0.558400\n",
      "epochs 879 / 1500: loss 1.209038 : training accuracy 0.572700, and val accuracy 0.572700\n",
      "epochs 880 / 1500: loss 1.195333 : training accuracy 0.581700, and val accuracy 0.581700\n",
      "epochs 881 / 1500: loss 1.198008 : training accuracy 0.582400, and val accuracy 0.582400\n",
      "epochs 882 / 1500: loss 1.218725 : training accuracy 0.575200, and val accuracy 0.575200\n",
      "epochs 883 / 1500: loss 1.200766 : training accuracy 0.584400, and val accuracy 0.584400\n",
      "epochs 884 / 1500: loss 1.221202 : training accuracy 0.563600, and val accuracy 0.563600\n",
      "epochs 885 / 1500: loss 1.196978 : training accuracy 0.578100, and val accuracy 0.578100\n",
      "epochs 886 / 1500: loss 1.208302 : training accuracy 0.577500, and val accuracy 0.577500\n",
      "epochs 887 / 1500: loss 1.217335 : training accuracy 0.582000, and val accuracy 0.582000\n",
      "epochs 888 / 1500: loss 1.199870 : training accuracy 0.585400, and val accuracy 0.585400\n",
      "epochs 889 / 1500: loss 1.188175 : training accuracy 0.588800, and val accuracy 0.588800\n",
      "epochs 890 / 1500: loss 1.187029 : training accuracy 0.592900, and val accuracy 0.592900\n",
      "epochs 891 / 1500: loss 1.196624 : training accuracy 0.585100, and val accuracy 0.585100\n",
      "epochs 892 / 1500: loss 1.232516 : training accuracy 0.560400, and val accuracy 0.560400\n",
      "epochs 893 / 1500: loss 1.212729 : training accuracy 0.574400, and val accuracy 0.574400\n",
      "epochs 894 / 1500: loss 1.208503 : training accuracy 0.583700, and val accuracy 0.583700\n",
      "epochs 895 / 1500: loss 1.205572 : training accuracy 0.580200, and val accuracy 0.580200\n",
      "epochs 896 / 1500: loss 1.196630 : training accuracy 0.585000, and val accuracy 0.585000\n",
      "epochs 897 / 1500: loss 1.234253 : training accuracy 0.576300, and val accuracy 0.576300\n",
      "epochs 898 / 1500: loss 1.221469 : training accuracy 0.586600, and val accuracy 0.586600\n",
      "epochs 899 / 1500: loss 1.201428 : training accuracy 0.586900, and val accuracy 0.586900\n",
      "epochs 900 / 1500: loss 1.225471 : training accuracy 0.574500, and val accuracy 0.574500\n",
      "epochs 901 / 1500: loss 1.221661 : training accuracy 0.583200, and val accuracy 0.583200\n",
      "epochs 902 / 1500: loss 1.215132 : training accuracy 0.581800, and val accuracy 0.581800\n",
      "epochs 903 / 1500: loss 1.207954 : training accuracy 0.579800, and val accuracy 0.579800\n",
      "epochs 904 / 1500: loss 1.199287 : training accuracy 0.584000, and val accuracy 0.584000\n",
      "epochs 905 / 1500: loss 1.201169 : training accuracy 0.586700, and val accuracy 0.586700\n",
      "epochs 906 / 1500: loss 1.231170 : training accuracy 0.582600, and val accuracy 0.582600\n",
      "epochs 907 / 1500: loss 1.193764 : training accuracy 0.585700, and val accuracy 0.585700\n",
      "epochs 908 / 1500: loss 1.192978 : training accuracy 0.589000, and val accuracy 0.589000\n",
      "epochs 909 / 1500: loss 1.205325 : training accuracy 0.574000, and val accuracy 0.574000\n",
      "epochs 910 / 1500: loss 1.201269 : training accuracy 0.583800, and val accuracy 0.583800\n",
      "epochs 911 / 1500: loss 1.184922 : training accuracy 0.595400, and val accuracy 0.595400\n",
      "epochs 912 / 1500: loss 1.188901 : training accuracy 0.599100, and val accuracy 0.599100\n",
      "epochs 913 / 1500: loss 1.217288 : training accuracy 0.586500, and val accuracy 0.586500\n",
      "epochs 914 / 1500: loss 1.203426 : training accuracy 0.585600, and val accuracy 0.585600\n",
      "epochs 915 / 1500: loss 1.181907 : training accuracy 0.587900, and val accuracy 0.587900\n",
      "epochs 916 / 1500: loss 1.184342 : training accuracy 0.589700, and val accuracy 0.589700\n",
      "epochs 917 / 1500: loss 1.169887 : training accuracy 0.596400, and val accuracy 0.596400\n",
      "epochs 918 / 1500: loss 1.205094 : training accuracy 0.582200, and val accuracy 0.582200\n",
      "epochs 919 / 1500: loss 1.241639 : training accuracy 0.579000, and val accuracy 0.579000\n",
      "epochs 920 / 1500: loss 1.196265 : training accuracy 0.595100, and val accuracy 0.595100\n",
      "epochs 921 / 1500: loss 1.192345 : training accuracy 0.596300, and val accuracy 0.596300\n",
      "epochs 922 / 1500: loss 1.201075 : training accuracy 0.587600, and val accuracy 0.587600\n",
      "epochs 923 / 1500: loss 1.196589 : training accuracy 0.595600, and val accuracy 0.595600\n",
      "epochs 924 / 1500: loss 1.188849 : training accuracy 0.594200, and val accuracy 0.594200\n",
      "epochs 925 / 1500: loss 1.190474 : training accuracy 0.585900, and val accuracy 0.585900\n",
      "epochs 926 / 1500: loss 1.228550 : training accuracy 0.566000, and val accuracy 0.566000\n",
      "epochs 927 / 1500: loss 1.182464 : training accuracy 0.582400, and val accuracy 0.582400\n",
      "epochs 928 / 1500: loss 1.206824 : training accuracy 0.574100, and val accuracy 0.574100\n",
      "epochs 929 / 1500: loss 1.187538 : training accuracy 0.584900, and val accuracy 0.584900\n",
      "epochs 930 / 1500: loss 1.175980 : training accuracy 0.590100, and val accuracy 0.590100\n",
      "epochs 931 / 1500: loss 1.166646 : training accuracy 0.596500, and val accuracy 0.596500\n",
      "epochs 932 / 1500: loss 1.170837 : training accuracy 0.589100, and val accuracy 0.589100\n",
      "epochs 933 / 1500: loss 1.182227 : training accuracy 0.595300, and val accuracy 0.595300\n",
      "epochs 934 / 1500: loss 1.186890 : training accuracy 0.588100, and val accuracy 0.588100\n",
      "epochs 935 / 1500: loss 1.204613 : training accuracy 0.570300, and val accuracy 0.570300\n",
      "epochs 936 / 1500: loss 1.173033 : training accuracy 0.595400, and val accuracy 0.595400\n",
      "epochs 937 / 1500: loss 1.192368 : training accuracy 0.595100, and val accuracy 0.595100\n",
      "epochs 938 / 1500: loss 1.188982 : training accuracy 0.587800, and val accuracy 0.587800\n",
      "epochs 939 / 1500: loss 1.207964 : training accuracy 0.580700, and val accuracy 0.580700\n",
      "epochs 940 / 1500: loss 1.200693 : training accuracy 0.572500, and val accuracy 0.572500\n",
      "epochs 941 / 1500: loss 1.183074 : training accuracy 0.585100, and val accuracy 0.585100\n",
      "epochs 942 / 1500: loss 1.171424 : training accuracy 0.593200, and val accuracy 0.593200\n",
      "epochs 943 / 1500: loss 1.196429 : training accuracy 0.580000, and val accuracy 0.580000\n",
      "epochs 944 / 1500: loss 1.268675 : training accuracy 0.585900, and val accuracy 0.585900\n",
      "epochs 945 / 1500: loss 1.198148 : training accuracy 0.594800, and val accuracy 0.594800\n",
      "epochs 946 / 1500: loss 1.196620 : training accuracy 0.589200, and val accuracy 0.589200\n",
      "epochs 947 / 1500: loss 1.186582 : training accuracy 0.591100, and val accuracy 0.591100\n",
      "epochs 948 / 1500: loss 1.204155 : training accuracy 0.588700, and val accuracy 0.588700\n",
      "epochs 949 / 1500: loss 1.185430 : training accuracy 0.590000, and val accuracy 0.590000\n",
      "epochs 950 / 1500: loss 1.178480 : training accuracy 0.582200, and val accuracy 0.582200\n",
      "epochs 951 / 1500: loss 1.173572 : training accuracy 0.584300, and val accuracy 0.584300\n",
      "epochs 952 / 1500: loss 1.177911 : training accuracy 0.585200, and val accuracy 0.585200\n",
      "epochs 953 / 1500: loss 1.188921 : training accuracy 0.574300, and val accuracy 0.574300\n",
      "epochs 954 / 1500: loss 1.171082 : training accuracy 0.589200, and val accuracy 0.589200\n",
      "epochs 955 / 1500: loss 1.197929 : training accuracy 0.585300, and val accuracy 0.585300\n",
      "epochs 956 / 1500: loss 1.198850 : training accuracy 0.589300, and val accuracy 0.589300\n",
      "epochs 957 / 1500: loss 1.183525 : training accuracy 0.598600, and val accuracy 0.598600\n",
      "epochs 958 / 1500: loss 1.163253 : training accuracy 0.599200, and val accuracy 0.599200\n",
      "epochs 959 / 1500: loss 1.158599 : training accuracy 0.601200, and val accuracy 0.601200\n",
      "epochs 960 / 1500: loss 1.155228 : training accuracy 0.602500, and val accuracy 0.602500\n",
      "epochs 961 / 1500: loss 1.225232 : training accuracy 0.566000, and val accuracy 0.566000\n",
      "epochs 962 / 1500: loss 1.154548 : training accuracy 0.599500, and val accuracy 0.599500\n",
      "epochs 963 / 1500: loss 1.159599 : training accuracy 0.593600, and val accuracy 0.593600\n",
      "epochs 964 / 1500: loss 1.156274 : training accuracy 0.600800, and val accuracy 0.600800\n",
      "epochs 965 / 1500: loss 1.147521 : training accuracy 0.599200, and val accuracy 0.599200\n",
      "epochs 966 / 1500: loss 1.170052 : training accuracy 0.599600, and val accuracy 0.599600\n",
      "epochs 967 / 1500: loss 1.197833 : training accuracy 0.596400, and val accuracy 0.596400\n",
      "epochs 968 / 1500: loss 1.177879 : training accuracy 0.597600, and val accuracy 0.597600\n",
      "epochs 969 / 1500: loss 1.222379 : training accuracy 0.581100, and val accuracy 0.581100\n",
      "epochs 970 / 1500: loss 1.173805 : training accuracy 0.592300, and val accuracy 0.592300\n",
      "epochs 971 / 1500: loss 1.181239 : training accuracy 0.596800, and val accuracy 0.596800\n",
      "epochs 972 / 1500: loss 1.170962 : training accuracy 0.604700, and val accuracy 0.604700\n",
      "epochs 973 / 1500: loss 1.179599 : training accuracy 0.590100, and val accuracy 0.590100\n",
      "epochs 974 / 1500: loss 1.192915 : training accuracy 0.577900, and val accuracy 0.577900\n",
      "epochs 975 / 1500: loss 1.210426 : training accuracy 0.574000, and val accuracy 0.574000\n",
      "epochs 976 / 1500: loss 1.182175 : training accuracy 0.583200, and val accuracy 0.583200\n",
      "epochs 977 / 1500: loss 1.164856 : training accuracy 0.594000, and val accuracy 0.594000\n",
      "epochs 978 / 1500: loss 1.176201 : training accuracy 0.591400, and val accuracy 0.591400\n",
      "epochs 979 / 1500: loss 1.153817 : training accuracy 0.596300, and val accuracy 0.596300\n",
      "epochs 980 / 1500: loss 1.197376 : training accuracy 0.584200, and val accuracy 0.584200\n",
      "epochs 981 / 1500: loss 1.155096 : training accuracy 0.600200, and val accuracy 0.600200\n",
      "epochs 982 / 1500: loss 1.162161 : training accuracy 0.603700, and val accuracy 0.603700\n",
      "epochs 983 / 1500: loss 1.158748 : training accuracy 0.607200, and val accuracy 0.607200\n",
      "epochs 984 / 1500: loss 1.168404 : training accuracy 0.596800, and val accuracy 0.596800\n",
      "epochs 985 / 1500: loss 1.167317 : training accuracy 0.590500, and val accuracy 0.590500\n",
      "epochs 986 / 1500: loss 1.140782 : training accuracy 0.604300, and val accuracy 0.604300\n",
      "epochs 987 / 1500: loss 1.178733 : training accuracy 0.592000, and val accuracy 0.592000\n",
      "epochs 988 / 1500: loss 1.164515 : training accuracy 0.610900, and val accuracy 0.610900\n",
      "epochs 989 / 1500: loss 1.159397 : training accuracy 0.603900, and val accuracy 0.603900\n",
      "epochs 990 / 1500: loss 1.158053 : training accuracy 0.592900, and val accuracy 0.592900\n",
      "epochs 991 / 1500: loss 1.181428 : training accuracy 0.592600, and val accuracy 0.592600\n",
      "epochs 992 / 1500: loss 1.195003 : training accuracy 0.607100, and val accuracy 0.607100\n",
      "epochs 993 / 1500: loss 1.179909 : training accuracy 0.601000, and val accuracy 0.601000\n",
      "epochs 994 / 1500: loss 1.160847 : training accuracy 0.592900, and val accuracy 0.592900\n",
      "epochs 995 / 1500: loss 1.168274 : training accuracy 0.597000, and val accuracy 0.597000\n",
      "epochs 996 / 1500: loss 1.166170 : training accuracy 0.599900, and val accuracy 0.599900\n",
      "epochs 997 / 1500: loss 1.170262 : training accuracy 0.602400, and val accuracy 0.602400\n",
      "epochs 998 / 1500: loss 1.161048 : training accuracy 0.604400, and val accuracy 0.604400\n",
      "epochs 999 / 1500: loss 1.207069 : training accuracy 0.583200, and val accuracy 0.583200\n",
      "epochs 1000 / 1500: loss 1.220777 : training accuracy 0.583800, and val accuracy 0.583800\n",
      "epochs 1001 / 1500: loss 1.169171 : training accuracy 0.598900, and val accuracy 0.598900\n",
      "epochs 1002 / 1500: loss 1.190366 : training accuracy 0.603100, and val accuracy 0.603100\n",
      "epochs 1003 / 1500: loss 1.188880 : training accuracy 0.598200, and val accuracy 0.598200\n",
      "epochs 1004 / 1500: loss 1.172584 : training accuracy 0.596500, and val accuracy 0.596500\n",
      "epochs 1005 / 1500: loss 1.158659 : training accuracy 0.597400, and val accuracy 0.597400\n",
      "epochs 1006 / 1500: loss 1.140779 : training accuracy 0.608900, and val accuracy 0.608900\n",
      "epochs 1007 / 1500: loss 1.159735 : training accuracy 0.610800, and val accuracy 0.610800\n",
      "epochs 1008 / 1500: loss 1.139696 : training accuracy 0.611000, and val accuracy 0.611000\n",
      "epochs 1009 / 1500: loss 1.130919 : training accuracy 0.614200, and val accuracy 0.614200\n",
      "epochs 1010 / 1500: loss 1.173447 : training accuracy 0.600900, and val accuracy 0.600900\n",
      "epochs 1011 / 1500: loss 1.166246 : training accuracy 0.608300, and val accuracy 0.608300\n",
      "epochs 1012 / 1500: loss 1.167489 : training accuracy 0.601100, and val accuracy 0.601100\n",
      "epochs 1013 / 1500: loss 1.181348 : training accuracy 0.595200, and val accuracy 0.595200\n",
      "epochs 1014 / 1500: loss 1.140994 : training accuracy 0.614300, and val accuracy 0.614300\n",
      "epochs 1015 / 1500: loss 1.144519 : training accuracy 0.602600, and val accuracy 0.602600\n",
      "epochs 1016 / 1500: loss 1.137917 : training accuracy 0.601100, and val accuracy 0.601100\n",
      "epochs 1017 / 1500: loss 1.136590 : training accuracy 0.599200, and val accuracy 0.599200\n",
      "epochs 1018 / 1500: loss 1.176796 : training accuracy 0.585400, and val accuracy 0.585400\n",
      "epochs 1019 / 1500: loss 1.152241 : training accuracy 0.605600, and val accuracy 0.605600\n",
      "epochs 1020 / 1500: loss 1.139214 : training accuracy 0.604600, and val accuracy 0.604600\n",
      "epochs 1021 / 1500: loss 1.179905 : training accuracy 0.594800, and val accuracy 0.594800\n",
      "epochs 1022 / 1500: loss 1.154613 : training accuracy 0.580400, and val accuracy 0.580400\n",
      "epochs 1023 / 1500: loss 1.159670 : training accuracy 0.588400, and val accuracy 0.588400\n",
      "epochs 1024 / 1500: loss 1.138282 : training accuracy 0.600500, and val accuracy 0.600500\n",
      "epochs 1025 / 1500: loss 1.122326 : training accuracy 0.618200, and val accuracy 0.618200\n",
      "epochs 1026 / 1500: loss 1.155184 : training accuracy 0.605200, and val accuracy 0.605200\n",
      "epochs 1027 / 1500: loss 1.123244 : training accuracy 0.608100, and val accuracy 0.608100\n",
      "epochs 1028 / 1500: loss 1.143985 : training accuracy 0.608600, and val accuracy 0.608600\n",
      "epochs 1029 / 1500: loss 1.151187 : training accuracy 0.602500, and val accuracy 0.602500\n",
      "epochs 1030 / 1500: loss 1.151238 : training accuracy 0.600700, and val accuracy 0.600700\n",
      "epochs 1031 / 1500: loss 1.165241 : training accuracy 0.598800, and val accuracy 0.598800\n",
      "epochs 1032 / 1500: loss 1.182824 : training accuracy 0.592700, and val accuracy 0.592700\n",
      "epochs 1033 / 1500: loss 1.164400 : training accuracy 0.593000, and val accuracy 0.593000\n",
      "epochs 1034 / 1500: loss 1.140122 : training accuracy 0.605200, and val accuracy 0.605200\n",
      "epochs 1035 / 1500: loss 1.125104 : training accuracy 0.607000, and val accuracy 0.607000\n",
      "epochs 1036 / 1500: loss 1.124790 : training accuracy 0.601100, and val accuracy 0.601100\n",
      "epochs 1037 / 1500: loss 1.186198 : training accuracy 0.580400, and val accuracy 0.580400\n",
      "epochs 1038 / 1500: loss 1.143582 : training accuracy 0.605300, and val accuracy 0.605300\n",
      "epochs 1039 / 1500: loss 1.125137 : training accuracy 0.610200, and val accuracy 0.610200\n",
      "epochs 1040 / 1500: loss 1.129754 : training accuracy 0.597900, and val accuracy 0.597900\n",
      "epochs 1041 / 1500: loss 1.174335 : training accuracy 0.589300, and val accuracy 0.589300\n",
      "epochs 1042 / 1500: loss 1.149608 : training accuracy 0.593000, and val accuracy 0.593000\n",
      "epochs 1043 / 1500: loss 1.170405 : training accuracy 0.589600, and val accuracy 0.589600\n",
      "epochs 1044 / 1500: loss 1.154724 : training accuracy 0.594400, and val accuracy 0.594400\n",
      "epochs 1045 / 1500: loss 1.139037 : training accuracy 0.598700, and val accuracy 0.598700\n",
      "epochs 1046 / 1500: loss 1.113121 : training accuracy 0.605900, and val accuracy 0.605900\n",
      "epochs 1047 / 1500: loss 1.129105 : training accuracy 0.597500, and val accuracy 0.597500\n",
      "epochs 1048 / 1500: loss 1.149306 : training accuracy 0.605400, and val accuracy 0.605400\n",
      "epochs 1049 / 1500: loss 1.142811 : training accuracy 0.601600, and val accuracy 0.601600\n",
      "epochs 1050 / 1500: loss 1.134836 : training accuracy 0.607500, and val accuracy 0.607500\n",
      "epochs 1051 / 1500: loss 1.140365 : training accuracy 0.600100, and val accuracy 0.600100\n",
      "epochs 1052 / 1500: loss 1.141220 : training accuracy 0.601800, and val accuracy 0.601800\n",
      "epochs 1053 / 1500: loss 1.128641 : training accuracy 0.607200, and val accuracy 0.607200\n",
      "epochs 1054 / 1500: loss 1.127384 : training accuracy 0.613800, and val accuracy 0.613800\n",
      "epochs 1055 / 1500: loss 1.141014 : training accuracy 0.605800, and val accuracy 0.605800\n",
      "epochs 1056 / 1500: loss 1.169206 : training accuracy 0.612600, and val accuracy 0.612600\n",
      "epochs 1057 / 1500: loss 1.157878 : training accuracy 0.608500, and val accuracy 0.608500\n",
      "epochs 1058 / 1500: loss 1.133977 : training accuracy 0.614300, and val accuracy 0.614300\n",
      "epochs 1059 / 1500: loss 1.165107 : training accuracy 0.605300, and val accuracy 0.605300\n",
      "epochs 1060 / 1500: loss 1.140850 : training accuracy 0.600700, and val accuracy 0.600700\n",
      "epochs 1061 / 1500: loss 1.167300 : training accuracy 0.598300, and val accuracy 0.598300\n",
      "epochs 1062 / 1500: loss 1.132823 : training accuracy 0.598100, and val accuracy 0.598100\n",
      "epochs 1063 / 1500: loss 1.132939 : training accuracy 0.606300, and val accuracy 0.606300\n",
      "epochs 1064 / 1500: loss 1.150131 : training accuracy 0.608400, and val accuracy 0.608400\n",
      "epochs 1065 / 1500: loss 1.151073 : training accuracy 0.594900, and val accuracy 0.594900\n",
      "epochs 1066 / 1500: loss 1.152454 : training accuracy 0.592700, and val accuracy 0.592700\n",
      "epochs 1067 / 1500: loss 1.141549 : training accuracy 0.606900, and val accuracy 0.606900\n",
      "epochs 1068 / 1500: loss 1.129936 : training accuracy 0.613600, and val accuracy 0.613600\n",
      "epochs 1069 / 1500: loss 1.127340 : training accuracy 0.619300, and val accuracy 0.619300\n",
      "epochs 1070 / 1500: loss 1.222574 : training accuracy 0.592700, and val accuracy 0.592700\n",
      "epochs 1071 / 1500: loss 1.130454 : training accuracy 0.617100, and val accuracy 0.617100\n",
      "epochs 1072 / 1500: loss 1.121043 : training accuracy 0.615500, and val accuracy 0.615500\n",
      "epochs 1073 / 1500: loss 1.155562 : training accuracy 0.609100, and val accuracy 0.609100\n",
      "epochs 1074 / 1500: loss 1.152776 : training accuracy 0.604800, and val accuracy 0.604800\n",
      "epochs 1075 / 1500: loss 1.126699 : training accuracy 0.606600, and val accuracy 0.606600\n",
      "epochs 1076 / 1500: loss 1.112395 : training accuracy 0.610900, and val accuracy 0.610900\n",
      "epochs 1077 / 1500: loss 1.115516 : training accuracy 0.611400, and val accuracy 0.611400\n",
      "epochs 1078 / 1500: loss 1.155256 : training accuracy 0.592000, and val accuracy 0.592000\n",
      "epochs 1079 / 1500: loss 1.138755 : training accuracy 0.612200, and val accuracy 0.612200\n",
      "epochs 1080 / 1500: loss 1.124086 : training accuracy 0.619900, and val accuracy 0.619900\n",
      "epochs 1081 / 1500: loss 1.108072 : training accuracy 0.622800, and val accuracy 0.622800\n",
      "epochs 1082 / 1500: loss 1.138132 : training accuracy 0.614400, and val accuracy 0.614400\n",
      "epochs 1083 / 1500: loss 1.171929 : training accuracy 0.590500, and val accuracy 0.590500\n",
      "epochs 1084 / 1500: loss 1.147841 : training accuracy 0.593100, and val accuracy 0.593100\n",
      "epochs 1085 / 1500: loss 1.127534 : training accuracy 0.598500, and val accuracy 0.598500\n",
      "epochs 1086 / 1500: loss 1.121058 : training accuracy 0.613600, and val accuracy 0.613600\n",
      "epochs 1087 / 1500: loss 1.109934 : training accuracy 0.610200, and val accuracy 0.610200\n",
      "epochs 1088 / 1500: loss 1.128826 : training accuracy 0.603000, and val accuracy 0.603000\n",
      "epochs 1089 / 1500: loss 1.178725 : training accuracy 0.606700, and val accuracy 0.606700\n",
      "epochs 1090 / 1500: loss 1.131440 : training accuracy 0.609700, and val accuracy 0.609700\n",
      "epochs 1091 / 1500: loss 1.124833 : training accuracy 0.611500, and val accuracy 0.611500\n",
      "epochs 1092 / 1500: loss 1.117292 : training accuracy 0.611400, and val accuracy 0.611400\n",
      "epochs 1093 / 1500: loss 1.099924 : training accuracy 0.614500, and val accuracy 0.614500\n",
      "epochs 1094 / 1500: loss 1.131811 : training accuracy 0.596300, and val accuracy 0.596300\n",
      "epochs 1095 / 1500: loss 1.119603 : training accuracy 0.604200, and val accuracy 0.604200\n",
      "epochs 1096 / 1500: loss 1.101375 : training accuracy 0.625700, and val accuracy 0.625700\n",
      "epochs 1097 / 1500: loss 1.128623 : training accuracy 0.599400, and val accuracy 0.599400\n",
      "epochs 1098 / 1500: loss 1.139103 : training accuracy 0.596900, and val accuracy 0.596900\n",
      "epochs 1099 / 1500: loss 1.116961 : training accuracy 0.609000, and val accuracy 0.609000\n",
      "epochs 1100 / 1500: loss 1.095034 : training accuracy 0.618000, and val accuracy 0.618000\n",
      "epochs 1101 / 1500: loss 1.108452 : training accuracy 0.611700, and val accuracy 0.611700\n",
      "epochs 1102 / 1500: loss 1.152322 : training accuracy 0.609600, and val accuracy 0.609600\n",
      "epochs 1103 / 1500: loss 1.099356 : training accuracy 0.617500, and val accuracy 0.617500\n",
      "epochs 1104 / 1500: loss 1.137091 : training accuracy 0.595700, and val accuracy 0.595700\n",
      "epochs 1105 / 1500: loss 1.134154 : training accuracy 0.613500, and val accuracy 0.613500\n",
      "epochs 1106 / 1500: loss 1.119971 : training accuracy 0.622700, and val accuracy 0.622700\n",
      "epochs 1107 / 1500: loss 1.138214 : training accuracy 0.596000, and val accuracy 0.596000\n",
      "epochs 1108 / 1500: loss 1.128849 : training accuracy 0.606100, and val accuracy 0.606100\n",
      "epochs 1109 / 1500: loss 1.122409 : training accuracy 0.608500, and val accuracy 0.608500\n",
      "epochs 1110 / 1500: loss 1.103580 : training accuracy 0.616000, and val accuracy 0.616000\n",
      "epochs 1111 / 1500: loss 1.120523 : training accuracy 0.612300, and val accuracy 0.612300\n",
      "epochs 1112 / 1500: loss 1.118441 : training accuracy 0.614600, and val accuracy 0.614600\n",
      "epochs 1113 / 1500: loss 1.121181 : training accuracy 0.608800, and val accuracy 0.608800\n",
      "epochs 1114 / 1500: loss 1.106954 : training accuracy 0.626000, and val accuracy 0.626000\n",
      "epochs 1115 / 1500: loss 1.186433 : training accuracy 0.599100, and val accuracy 0.599100\n",
      "epochs 1116 / 1500: loss 1.115735 : training accuracy 0.626800, and val accuracy 0.626800\n",
      "epochs 1117 / 1500: loss 1.127162 : training accuracy 0.619600, and val accuracy 0.619600\n",
      "epochs 1118 / 1500: loss 1.127740 : training accuracy 0.618500, and val accuracy 0.618500\n",
      "epochs 1119 / 1500: loss 1.131908 : training accuracy 0.621700, and val accuracy 0.621700\n",
      "epochs 1120 / 1500: loss 1.130649 : training accuracy 0.612100, and val accuracy 0.612100\n",
      "epochs 1121 / 1500: loss 1.106235 : training accuracy 0.625000, and val accuracy 0.625000\n",
      "epochs 1122 / 1500: loss 1.100056 : training accuracy 0.624200, and val accuracy 0.624200\n",
      "epochs 1123 / 1500: loss 1.121229 : training accuracy 0.604500, and val accuracy 0.604500\n",
      "epochs 1124 / 1500: loss 1.073727 : training accuracy 0.618300, and val accuracy 0.618300\n",
      "epochs 1125 / 1500: loss 1.106729 : training accuracy 0.605100, and val accuracy 0.605100\n",
      "epochs 1126 / 1500: loss 1.126199 : training accuracy 0.608600, and val accuracy 0.608600\n",
      "epochs 1127 / 1500: loss 1.101094 : training accuracy 0.611700, and val accuracy 0.611700\n",
      "epochs 1128 / 1500: loss 1.111837 : training accuracy 0.608600, and val accuracy 0.608600\n",
      "epochs 1129 / 1500: loss 1.147496 : training accuracy 0.613800, and val accuracy 0.613800\n",
      "epochs 1130 / 1500: loss 1.113154 : training accuracy 0.622100, and val accuracy 0.622100\n",
      "epochs 1131 / 1500: loss 1.128203 : training accuracy 0.625100, and val accuracy 0.625100\n",
      "epochs 1132 / 1500: loss 1.105472 : training accuracy 0.614900, and val accuracy 0.614900\n",
      "epochs 1133 / 1500: loss 1.119753 : training accuracy 0.619200, and val accuracy 0.619200\n",
      "epochs 1134 / 1500: loss 1.144483 : training accuracy 0.608100, and val accuracy 0.608100\n",
      "epochs 1135 / 1500: loss 1.118799 : training accuracy 0.611400, and val accuracy 0.611400\n",
      "epochs 1136 / 1500: loss 1.106604 : training accuracy 0.624600, and val accuracy 0.624600\n",
      "epochs 1137 / 1500: loss 1.123977 : training accuracy 0.617600, and val accuracy 0.617600\n",
      "epochs 1138 / 1500: loss 1.135936 : training accuracy 0.610600, and val accuracy 0.610600\n",
      "epochs 1139 / 1500: loss 1.111094 : training accuracy 0.617600, and val accuracy 0.617600\n",
      "epochs 1140 / 1500: loss 1.104118 : training accuracy 0.611300, and val accuracy 0.611300\n",
      "epochs 1141 / 1500: loss 1.089071 : training accuracy 0.624500, and val accuracy 0.624500\n",
      "epochs 1142 / 1500: loss 1.120172 : training accuracy 0.607600, and val accuracy 0.607600\n",
      "epochs 1143 / 1500: loss 1.088139 : training accuracy 0.622100, and val accuracy 0.622100\n",
      "epochs 1144 / 1500: loss 1.118081 : training accuracy 0.619900, and val accuracy 0.619900\n",
      "epochs 1145 / 1500: loss 1.110624 : training accuracy 0.609800, and val accuracy 0.609800\n",
      "epochs 1146 / 1500: loss 1.102873 : training accuracy 0.627500, and val accuracy 0.627500\n",
      "epochs 1147 / 1500: loss 1.136578 : training accuracy 0.613600, and val accuracy 0.613600\n",
      "epochs 1148 / 1500: loss 1.129916 : training accuracy 0.621400, and val accuracy 0.621400\n",
      "epochs 1149 / 1500: loss 1.115579 : training accuracy 0.625100, and val accuracy 0.625100\n",
      "epochs 1150 / 1500: loss 1.109375 : training accuracy 0.615800, and val accuracy 0.615800\n",
      "epochs 1151 / 1500: loss 1.119770 : training accuracy 0.628700, and val accuracy 0.628700\n",
      "epochs 1152 / 1500: loss 1.115381 : training accuracy 0.631600, and val accuracy 0.631600\n",
      "epochs 1153 / 1500: loss 1.104767 : training accuracy 0.623500, and val accuracy 0.623500\n",
      "epochs 1154 / 1500: loss 1.130495 : training accuracy 0.615900, and val accuracy 0.615900\n",
      "epochs 1155 / 1500: loss 1.141766 : training accuracy 0.612500, and val accuracy 0.612500\n",
      "epochs 1156 / 1500: loss 1.127817 : training accuracy 0.618900, and val accuracy 0.618900\n",
      "epochs 1157 / 1500: loss 1.135821 : training accuracy 0.606100, and val accuracy 0.606100\n",
      "epochs 1158 / 1500: loss 1.150748 : training accuracy 0.589200, and val accuracy 0.589200\n",
      "epochs 1159 / 1500: loss 1.107409 : training accuracy 0.606400, and val accuracy 0.606400\n",
      "epochs 1160 / 1500: loss 1.112677 : training accuracy 0.614700, and val accuracy 0.614700\n",
      "epochs 1161 / 1500: loss 1.118858 : training accuracy 0.610600, and val accuracy 0.610600\n",
      "epochs 1162 / 1500: loss 1.100439 : training accuracy 0.619300, and val accuracy 0.619300\n",
      "epochs 1163 / 1500: loss 1.116298 : training accuracy 0.617700, and val accuracy 0.617700\n",
      "epochs 1164 / 1500: loss 1.123036 : training accuracy 0.621400, and val accuracy 0.621400\n",
      "epochs 1165 / 1500: loss 1.090858 : training accuracy 0.625700, and val accuracy 0.625700\n",
      "epochs 1166 / 1500: loss 1.095479 : training accuracy 0.622000, and val accuracy 0.622000\n",
      "epochs 1167 / 1500: loss 1.080818 : training accuracy 0.615600, and val accuracy 0.615600\n",
      "epochs 1168 / 1500: loss 1.114504 : training accuracy 0.594600, and val accuracy 0.594600\n",
      "epochs 1169 / 1500: loss 1.114098 : training accuracy 0.618800, and val accuracy 0.618800\n",
      "epochs 1170 / 1500: loss 1.097339 : training accuracy 0.626300, and val accuracy 0.626300\n",
      "epochs 1171 / 1500: loss 1.079235 : training accuracy 0.624700, and val accuracy 0.624700\n",
      "epochs 1172 / 1500: loss 1.107127 : training accuracy 0.612500, and val accuracy 0.612500\n",
      "epochs 1173 / 1500: loss 1.078455 : training accuracy 0.626600, and val accuracy 0.626600\n",
      "epochs 1174 / 1500: loss 1.069062 : training accuracy 0.632000, and val accuracy 0.632000\n",
      "epochs 1175 / 1500: loss 1.066806 : training accuracy 0.629200, and val accuracy 0.629200\n",
      "epochs 1176 / 1500: loss 1.101988 : training accuracy 0.617300, and val accuracy 0.617300\n",
      "epochs 1177 / 1500: loss 1.080044 : training accuracy 0.625300, and val accuracy 0.625300\n",
      "epochs 1178 / 1500: loss 1.069477 : training accuracy 0.629000, and val accuracy 0.629000\n",
      "epochs 1179 / 1500: loss 1.072103 : training accuracy 0.625900, and val accuracy 0.625900\n",
      "epochs 1180 / 1500: loss 1.128047 : training accuracy 0.608100, and val accuracy 0.608100\n",
      "epochs 1181 / 1500: loss 1.109910 : training accuracy 0.604100, and val accuracy 0.604100\n",
      "epochs 1182 / 1500: loss 1.075163 : training accuracy 0.618700, and val accuracy 0.618700\n",
      "epochs 1183 / 1500: loss 1.109539 : training accuracy 0.609500, and val accuracy 0.609500\n",
      "epochs 1184 / 1500: loss 1.097731 : training accuracy 0.608800, and val accuracy 0.608800\n",
      "epochs 1185 / 1500: loss 1.111529 : training accuracy 0.620200, and val accuracy 0.620200\n",
      "epochs 1186 / 1500: loss 1.120379 : training accuracy 0.622500, and val accuracy 0.622500\n",
      "epochs 1187 / 1500: loss 1.094709 : training accuracy 0.623800, and val accuracy 0.623800\n",
      "epochs 1188 / 1500: loss 1.097890 : training accuracy 0.612400, and val accuracy 0.612400\n",
      "epochs 1189 / 1500: loss 1.078760 : training accuracy 0.625000, and val accuracy 0.625000\n",
      "epochs 1190 / 1500: loss 1.072760 : training accuracy 0.624600, and val accuracy 0.624600\n",
      "epochs 1191 / 1500: loss 1.078917 : training accuracy 0.620000, and val accuracy 0.620000\n",
      "epochs 1192 / 1500: loss 1.084082 : training accuracy 0.619300, and val accuracy 0.619300\n",
      "epochs 1193 / 1500: loss 1.096149 : training accuracy 0.623600, and val accuracy 0.623600\n",
      "epochs 1194 / 1500: loss 1.071799 : training accuracy 0.620100, and val accuracy 0.620100\n",
      "epochs 1195 / 1500: loss 1.112248 : training accuracy 0.614400, and val accuracy 0.614400\n",
      "epochs 1196 / 1500: loss 1.101810 : training accuracy 0.611200, and val accuracy 0.611200\n",
      "epochs 1197 / 1500: loss 1.076594 : training accuracy 0.623800, and val accuracy 0.623800\n",
      "epochs 1198 / 1500: loss 1.081274 : training accuracy 0.625800, and val accuracy 0.625800\n",
      "epochs 1199 / 1500: loss 1.083362 : training accuracy 0.614700, and val accuracy 0.614700\n",
      "epochs 1200 / 1500: loss 1.078594 : training accuracy 0.610800, and val accuracy 0.610800\n",
      "epochs 1201 / 1500: loss 1.065119 : training accuracy 0.624900, and val accuracy 0.624900\n",
      "epochs 1202 / 1500: loss 1.095924 : training accuracy 0.617900, and val accuracy 0.617900\n",
      "epochs 1203 / 1500: loss 1.089647 : training accuracy 0.626500, and val accuracy 0.626500\n",
      "epochs 1204 / 1500: loss 1.141892 : training accuracy 0.620800, and val accuracy 0.620800\n",
      "epochs 1205 / 1500: loss 1.106926 : training accuracy 0.619300, and val accuracy 0.619300\n",
      "epochs 1206 / 1500: loss 1.074290 : training accuracy 0.635000, and val accuracy 0.635000\n",
      "epochs 1207 / 1500: loss 1.107649 : training accuracy 0.620700, and val accuracy 0.620700\n",
      "epochs 1208 / 1500: loss 1.112497 : training accuracy 0.621600, and val accuracy 0.621600\n",
      "epochs 1209 / 1500: loss 1.077913 : training accuracy 0.638100, and val accuracy 0.638100\n",
      "epochs 1210 / 1500: loss 1.085392 : training accuracy 0.638500, and val accuracy 0.638500\n",
      "epochs 1211 / 1500: loss 1.125835 : training accuracy 0.619100, and val accuracy 0.619100\n",
      "epochs 1212 / 1500: loss 1.067170 : training accuracy 0.630000, and val accuracy 0.630000\n",
      "epochs 1213 / 1500: loss 1.079030 : training accuracy 0.623400, and val accuracy 0.623400\n",
      "epochs 1214 / 1500: loss 1.080536 : training accuracy 0.631200, and val accuracy 0.631200\n",
      "epochs 1215 / 1500: loss 1.097531 : training accuracy 0.610800, and val accuracy 0.610800\n",
      "epochs 1216 / 1500: loss 1.074309 : training accuracy 0.619400, and val accuracy 0.619400\n",
      "epochs 1217 / 1500: loss 1.048523 : training accuracy 0.624000, and val accuracy 0.624000\n",
      "epochs 1218 / 1500: loss 1.086370 : training accuracy 0.621000, and val accuracy 0.621000\n",
      "epochs 1219 / 1500: loss 1.124149 : training accuracy 0.613400, and val accuracy 0.613400\n",
      "epochs 1220 / 1500: loss 1.092219 : training accuracy 0.631900, and val accuracy 0.631900\n",
      "epochs 1221 / 1500: loss 1.057636 : training accuracy 0.630400, and val accuracy 0.630400\n",
      "epochs 1222 / 1500: loss 1.053608 : training accuracy 0.635000, and val accuracy 0.635000\n",
      "epochs 1223 / 1500: loss 1.075350 : training accuracy 0.621100, and val accuracy 0.621100\n",
      "epochs 1224 / 1500: loss 1.069905 : training accuracy 0.629000, and val accuracy 0.629000\n",
      "epochs 1225 / 1500: loss 1.064294 : training accuracy 0.627700, and val accuracy 0.627700\n",
      "epochs 1226 / 1500: loss 1.120778 : training accuracy 0.624300, and val accuracy 0.624300\n",
      "epochs 1227 / 1500: loss 1.139156 : training accuracy 0.618000, and val accuracy 0.618000\n",
      "epochs 1228 / 1500: loss 1.076182 : training accuracy 0.629800, and val accuracy 0.629800\n",
      "epochs 1229 / 1500: loss 1.080649 : training accuracy 0.633200, and val accuracy 0.633200\n",
      "epochs 1230 / 1500: loss 1.076476 : training accuracy 0.635000, and val accuracy 0.635000\n",
      "epochs 1231 / 1500: loss 1.066655 : training accuracy 0.640600, and val accuracy 0.640600\n",
      "epochs 1232 / 1500: loss 1.074974 : training accuracy 0.636600, and val accuracy 0.636600\n",
      "epochs 1233 / 1500: loss 1.103659 : training accuracy 0.634700, and val accuracy 0.634700\n",
      "epochs 1234 / 1500: loss 1.112899 : training accuracy 0.624500, and val accuracy 0.624500\n",
      "epochs 1235 / 1500: loss 1.079507 : training accuracy 0.628400, and val accuracy 0.628400\n",
      "epochs 1236 / 1500: loss 1.092835 : training accuracy 0.631500, and val accuracy 0.631500\n",
      "epochs 1237 / 1500: loss 1.079816 : training accuracy 0.633000, and val accuracy 0.633000\n",
      "epochs 1238 / 1500: loss 1.082762 : training accuracy 0.623000, and val accuracy 0.623000\n",
      "epochs 1239 / 1500: loss 1.094929 : training accuracy 0.618700, and val accuracy 0.618700\n",
      "epochs 1240 / 1500: loss 1.090845 : training accuracy 0.617900, and val accuracy 0.617900\n",
      "epochs 1241 / 1500: loss 1.106682 : training accuracy 0.613300, and val accuracy 0.613300\n",
      "epochs 1242 / 1500: loss 1.071363 : training accuracy 0.617400, and val accuracy 0.617400\n",
      "epochs 1243 / 1500: loss 1.076508 : training accuracy 0.621200, and val accuracy 0.621200\n",
      "epochs 1244 / 1500: loss 1.104712 : training accuracy 0.618100, and val accuracy 0.618100\n",
      "epochs 1245 / 1500: loss 1.076079 : training accuracy 0.618400, and val accuracy 0.618400\n",
      "epochs 1246 / 1500: loss 1.078685 : training accuracy 0.625100, and val accuracy 0.625100\n",
      "epochs 1247 / 1500: loss 1.060232 : training accuracy 0.628000, and val accuracy 0.628000\n",
      "epochs 1248 / 1500: loss 1.072192 : training accuracy 0.623200, and val accuracy 0.623200\n",
      "epochs 1249 / 1500: loss 1.074472 : training accuracy 0.622800, and val accuracy 0.622800\n",
      "epochs 1250 / 1500: loss 1.052241 : training accuracy 0.629000, and val accuracy 0.629000\n",
      "epochs 1251 / 1500: loss 1.061704 : training accuracy 0.629800, and val accuracy 0.629800\n",
      "epochs 1252 / 1500: loss 1.070635 : training accuracy 0.635700, and val accuracy 0.635700\n",
      "epochs 1253 / 1500: loss 1.076497 : training accuracy 0.631600, and val accuracy 0.631600\n",
      "epochs 1254 / 1500: loss 1.082943 : training accuracy 0.617200, and val accuracy 0.617200\n",
      "epochs 1255 / 1500: loss 1.067948 : training accuracy 0.621300, and val accuracy 0.621300\n",
      "epochs 1256 / 1500: loss 1.047700 : training accuracy 0.632700, and val accuracy 0.632700\n",
      "epochs 1257 / 1500: loss 1.051346 : training accuracy 0.634900, and val accuracy 0.634900\n",
      "epochs 1258 / 1500: loss 1.074087 : training accuracy 0.634900, and val accuracy 0.634900\n",
      "epochs 1259 / 1500: loss 1.105298 : training accuracy 0.600600, and val accuracy 0.600600\n",
      "epochs 1260 / 1500: loss 1.056395 : training accuracy 0.634500, and val accuracy 0.634500\n",
      "epochs 1261 / 1500: loss 1.089436 : training accuracy 0.621300, and val accuracy 0.621300\n",
      "epochs 1262 / 1500: loss 1.052709 : training accuracy 0.641200, and val accuracy 0.641200\n",
      "epochs 1263 / 1500: loss 1.070576 : training accuracy 0.631400, and val accuracy 0.631400\n",
      "epochs 1264 / 1500: loss 1.058188 : training accuracy 0.632700, and val accuracy 0.632700\n",
      "epochs 1265 / 1500: loss 1.037768 : training accuracy 0.633800, and val accuracy 0.633800\n",
      "epochs 1266 / 1500: loss 1.041749 : training accuracy 0.630900, and val accuracy 0.630900\n",
      "epochs 1267 / 1500: loss 1.050879 : training accuracy 0.629900, and val accuracy 0.629900\n",
      "epochs 1268 / 1500: loss 1.081254 : training accuracy 0.629400, and val accuracy 0.629400\n",
      "epochs 1269 / 1500: loss 1.104577 : training accuracy 0.625200, and val accuracy 0.625200\n",
      "epochs 1270 / 1500: loss 1.070085 : training accuracy 0.635800, and val accuracy 0.635800\n",
      "epochs 1271 / 1500: loss 1.059873 : training accuracy 0.640400, and val accuracy 0.640400\n",
      "epochs 1272 / 1500: loss 1.061592 : training accuracy 0.638500, and val accuracy 0.638500\n",
      "epochs 1273 / 1500: loss 1.125059 : training accuracy 0.622500, and val accuracy 0.622500\n",
      "epochs 1274 / 1500: loss 1.099645 : training accuracy 0.626200, and val accuracy 0.626200\n",
      "epochs 1275 / 1500: loss 1.076469 : training accuracy 0.637000, and val accuracy 0.637000\n",
      "epochs 1276 / 1500: loss 1.075764 : training accuracy 0.617300, and val accuracy 0.617300\n",
      "epochs 1277 / 1500: loss 1.047364 : training accuracy 0.624000, and val accuracy 0.624000\n",
      "epochs 1278 / 1500: loss 1.077995 : training accuracy 0.614700, and val accuracy 0.614700\n",
      "epochs 1279 / 1500: loss 1.052132 : training accuracy 0.624500, and val accuracy 0.624500\n",
      "epochs 1280 / 1500: loss 1.077888 : training accuracy 0.620900, and val accuracy 0.620900\n",
      "epochs 1281 / 1500: loss 1.065728 : training accuracy 0.625000, and val accuracy 0.625000\n",
      "epochs 1282 / 1500: loss 1.055620 : training accuracy 0.630600, and val accuracy 0.630600\n",
      "epochs 1283 / 1500: loss 1.052463 : training accuracy 0.647100, and val accuracy 0.647100\n",
      "epochs 1284 / 1500: loss 1.072145 : training accuracy 0.632500, and val accuracy 0.632500\n",
      "epochs 1285 / 1500: loss 1.051224 : training accuracy 0.642200, and val accuracy 0.642200\n",
      "epochs 1286 / 1500: loss 1.057747 : training accuracy 0.635600, and val accuracy 0.635600\n",
      "epochs 1287 / 1500: loss 1.115773 : training accuracy 0.617900, and val accuracy 0.617900\n",
      "epochs 1288 / 1500: loss 1.055488 : training accuracy 0.629200, and val accuracy 0.629200\n",
      "epochs 1289 / 1500: loss 1.099780 : training accuracy 0.636500, and val accuracy 0.636500\n",
      "epochs 1290 / 1500: loss 1.055467 : training accuracy 0.640900, and val accuracy 0.640900\n",
      "epochs 1291 / 1500: loss 1.047959 : training accuracy 0.630900, and val accuracy 0.630900\n",
      "epochs 1292 / 1500: loss 1.019806 : training accuracy 0.638500, and val accuracy 0.638500\n",
      "epochs 1293 / 1500: loss 1.038221 : training accuracy 0.635400, and val accuracy 0.635400\n",
      "epochs 1294 / 1500: loss 1.059071 : training accuracy 0.640500, and val accuracy 0.640500\n",
      "epochs 1295 / 1500: loss 1.048852 : training accuracy 0.630500, and val accuracy 0.630500\n",
      "epochs 1296 / 1500: loss 1.068901 : training accuracy 0.615700, and val accuracy 0.615700\n",
      "epochs 1297 / 1500: loss 1.053660 : training accuracy 0.628800, and val accuracy 0.628800\n",
      "epochs 1298 / 1500: loss 1.029071 : training accuracy 0.638300, and val accuracy 0.638300\n",
      "epochs 1299 / 1500: loss 1.066849 : training accuracy 0.629100, and val accuracy 0.629100\n",
      "epochs 1300 / 1500: loss 1.026188 : training accuracy 0.647100, and val accuracy 0.647100\n",
      "epochs 1301 / 1500: loss 1.058262 : training accuracy 0.630500, and val accuracy 0.630500\n",
      "epochs 1302 / 1500: loss 1.058597 : training accuracy 0.629600, and val accuracy 0.629600\n",
      "epochs 1303 / 1500: loss 1.037363 : training accuracy 0.646100, and val accuracy 0.646100\n",
      "epochs 1304 / 1500: loss 0.993088 : training accuracy 0.665400, and val accuracy 0.665400\n",
      "epochs 1305 / 1500: loss 1.076651 : training accuracy 0.630800, and val accuracy 0.630800\n",
      "epochs 1306 / 1500: loss 1.029809 : training accuracy 0.641300, and val accuracy 0.641300\n",
      "epochs 1307 / 1500: loss 1.050925 : training accuracy 0.634500, and val accuracy 0.634500\n",
      "epochs 1308 / 1500: loss 1.081028 : training accuracy 0.628200, and val accuracy 0.628200\n",
      "epochs 1309 / 1500: loss 1.089301 : training accuracy 0.614800, and val accuracy 0.614800\n",
      "epochs 1310 / 1500: loss 1.060837 : training accuracy 0.619000, and val accuracy 0.619000\n",
      "epochs 1311 / 1500: loss 1.053542 : training accuracy 0.634800, and val accuracy 0.634800\n",
      "epochs 1312 / 1500: loss 1.042352 : training accuracy 0.642600, and val accuracy 0.642600\n",
      "epochs 1313 / 1500: loss 1.057245 : training accuracy 0.629200, and val accuracy 0.629200\n",
      "epochs 1314 / 1500: loss 1.057365 : training accuracy 0.626600, and val accuracy 0.626600\n",
      "epochs 1315 / 1500: loss 1.064122 : training accuracy 0.620200, and val accuracy 0.620200\n",
      "epochs 1316 / 1500: loss 1.044799 : training accuracy 0.627800, and val accuracy 0.627800\n",
      "epochs 1317 / 1500: loss 1.047310 : training accuracy 0.640700, and val accuracy 0.640700\n",
      "epochs 1318 / 1500: loss 1.051978 : training accuracy 0.641800, and val accuracy 0.641800\n",
      "epochs 1319 / 1500: loss 1.027837 : training accuracy 0.649100, and val accuracy 0.649100\n",
      "epochs 1320 / 1500: loss 1.034737 : training accuracy 0.646600, and val accuracy 0.646600\n",
      "epochs 1321 / 1500: loss 1.053438 : training accuracy 0.635000, and val accuracy 0.635000\n",
      "epochs 1322 / 1500: loss 1.065748 : training accuracy 0.637700, and val accuracy 0.637700\n",
      "epochs 1323 / 1500: loss 1.021110 : training accuracy 0.641600, and val accuracy 0.641600\n",
      "epochs 1324 / 1500: loss 1.050606 : training accuracy 0.639500, and val accuracy 0.639500\n",
      "epochs 1325 / 1500: loss 1.087299 : training accuracy 0.632500, and val accuracy 0.632500\n",
      "epochs 1326 / 1500: loss 1.090614 : training accuracy 0.633000, and val accuracy 0.633000\n",
      "epochs 1327 / 1500: loss 1.061330 : training accuracy 0.640400, and val accuracy 0.640400\n",
      "epochs 1328 / 1500: loss 1.063219 : training accuracy 0.641200, and val accuracy 0.641200\n",
      "epochs 1329 / 1500: loss 1.099442 : training accuracy 0.636700, and val accuracy 0.636700\n",
      "epochs 1330 / 1500: loss 1.050796 : training accuracy 0.640300, and val accuracy 0.640300\n",
      "epochs 1331 / 1500: loss 1.033754 : training accuracy 0.646300, and val accuracy 0.646300\n",
      "epochs 1332 / 1500: loss 1.014333 : training accuracy 0.649000, and val accuracy 0.649000\n",
      "epochs 1333 / 1500: loss 1.058909 : training accuracy 0.633900, and val accuracy 0.633900\n",
      "epochs 1334 / 1500: loss 1.038229 : training accuracy 0.638600, and val accuracy 0.638600\n",
      "epochs 1335 / 1500: loss 1.056724 : training accuracy 0.645500, and val accuracy 0.645500\n",
      "epochs 1336 / 1500: loss 1.053027 : training accuracy 0.641500, and val accuracy 0.641500\n",
      "epochs 1337 / 1500: loss 1.056006 : training accuracy 0.621300, and val accuracy 0.621300\n",
      "epochs 1338 / 1500: loss 1.084764 : training accuracy 0.625900, and val accuracy 0.625900\n",
      "epochs 1339 / 1500: loss 1.033680 : training accuracy 0.633800, and val accuracy 0.633800\n",
      "epochs 1340 / 1500: loss 1.028483 : training accuracy 0.641600, and val accuracy 0.641600\n",
      "epochs 1341 / 1500: loss 1.070982 : training accuracy 0.611200, and val accuracy 0.611200\n",
      "epochs 1342 / 1500: loss 1.037969 : training accuracy 0.628900, and val accuracy 0.628900\n",
      "epochs 1343 / 1500: loss 1.031986 : training accuracy 0.642000, and val accuracy 0.642000\n",
      "epochs 1344 / 1500: loss 1.029648 : training accuracy 0.644400, and val accuracy 0.644400\n",
      "epochs 1345 / 1500: loss 1.037720 : training accuracy 0.621600, and val accuracy 0.621600\n",
      "epochs 1346 / 1500: loss 1.108269 : training accuracy 0.613700, and val accuracy 0.613700\n",
      "epochs 1347 / 1500: loss 1.038619 : training accuracy 0.648600, and val accuracy 0.648600\n",
      "epochs 1348 / 1500: loss 1.040291 : training accuracy 0.647800, and val accuracy 0.647800\n",
      "epochs 1349 / 1500: loss 1.026747 : training accuracy 0.634500, and val accuracy 0.634500\n",
      "epochs 1350 / 1500: loss 1.031723 : training accuracy 0.638800, and val accuracy 0.638800\n",
      "epochs 1351 / 1500: loss 1.046934 : training accuracy 0.643500, and val accuracy 0.643500\n",
      "epochs 1352 / 1500: loss 1.046317 : training accuracy 0.643200, and val accuracy 0.643200\n",
      "epochs 1353 / 1500: loss 1.023759 : training accuracy 0.641200, and val accuracy 0.641200\n",
      "epochs 1354 / 1500: loss 1.036146 : training accuracy 0.639400, and val accuracy 0.639400\n",
      "epochs 1355 / 1500: loss 1.037778 : training accuracy 0.645600, and val accuracy 0.645600\n",
      "epochs 1356 / 1500: loss 1.024509 : training accuracy 0.653800, and val accuracy 0.653800\n",
      "epochs 1357 / 1500: loss 1.031349 : training accuracy 0.643000, and val accuracy 0.643000\n",
      "epochs 1358 / 1500: loss 1.024596 : training accuracy 0.648200, and val accuracy 0.648200\n",
      "epochs 1359 / 1500: loss 1.023895 : training accuracy 0.647100, and val accuracy 0.647100\n",
      "epochs 1360 / 1500: loss 1.012600 : training accuracy 0.643200, and val accuracy 0.643200\n",
      "epochs 1361 / 1500: loss 1.056795 : training accuracy 0.625200, and val accuracy 0.625200\n",
      "epochs 1362 / 1500: loss 1.040692 : training accuracy 0.636500, and val accuracy 0.636500\n",
      "epochs 1363 / 1500: loss 1.068249 : training accuracy 0.632200, and val accuracy 0.632200\n",
      "epochs 1364 / 1500: loss 1.116083 : training accuracy 0.630900, and val accuracy 0.630900\n",
      "epochs 1365 / 1500: loss 1.067210 : training accuracy 0.644300, and val accuracy 0.644300\n",
      "epochs 1366 / 1500: loss 1.071865 : training accuracy 0.637600, and val accuracy 0.637600\n",
      "epochs 1367 / 1500: loss 1.029557 : training accuracy 0.647900, and val accuracy 0.647900\n",
      "epochs 1368 / 1500: loss 1.042732 : training accuracy 0.647400, and val accuracy 0.647400\n",
      "epochs 1369 / 1500: loss 1.034936 : training accuracy 0.650200, and val accuracy 0.650200\n",
      "epochs 1370 / 1500: loss 1.020567 : training accuracy 0.655600, and val accuracy 0.655600\n",
      "epochs 1371 / 1500: loss 1.014055 : training accuracy 0.653300, and val accuracy 0.653300\n",
      "epochs 1372 / 1500: loss 1.104665 : training accuracy 0.620800, and val accuracy 0.620800\n",
      "epochs 1373 / 1500: loss 1.017015 : training accuracy 0.654100, and val accuracy 0.654100\n",
      "epochs 1374 / 1500: loss 1.006481 : training accuracy 0.651500, and val accuracy 0.651500\n",
      "epochs 1375 / 1500: loss 1.110133 : training accuracy 0.616800, and val accuracy 0.616800\n",
      "epochs 1376 / 1500: loss 1.054302 : training accuracy 0.639300, and val accuracy 0.639300\n",
      "epochs 1377 / 1500: loss 1.041885 : training accuracy 0.642300, and val accuracy 0.642300\n",
      "epochs 1378 / 1500: loss 1.021922 : training accuracy 0.636400, and val accuracy 0.636400\n",
      "epochs 1379 / 1500: loss 1.001425 : training accuracy 0.648100, and val accuracy 0.648100\n",
      "epochs 1380 / 1500: loss 1.006513 : training accuracy 0.638000, and val accuracy 0.638000\n",
      "epochs 1381 / 1500: loss 1.034680 : training accuracy 0.632900, and val accuracy 0.632900\n",
      "epochs 1382 / 1500: loss 1.057704 : training accuracy 0.643800, and val accuracy 0.643800\n",
      "epochs 1383 / 1500: loss 1.046874 : training accuracy 0.641900, and val accuracy 0.641900\n",
      "epochs 1384 / 1500: loss 1.015965 : training accuracy 0.651800, and val accuracy 0.651800\n",
      "epochs 1385 / 1500: loss 1.039965 : training accuracy 0.651200, and val accuracy 0.651200\n",
      "epochs 1386 / 1500: loss 1.019314 : training accuracy 0.655400, and val accuracy 0.655400\n",
      "epochs 1387 / 1500: loss 1.004696 : training accuracy 0.661200, and val accuracy 0.661200\n",
      "epochs 1388 / 1500: loss 1.012359 : training accuracy 0.648800, and val accuracy 0.648800\n",
      "epochs 1389 / 1500: loss 0.991761 : training accuracy 0.657500, and val accuracy 0.657500\n",
      "epochs 1390 / 1500: loss 1.075052 : training accuracy 0.617200, and val accuracy 0.617200\n",
      "epochs 1391 / 1500: loss 1.084605 : training accuracy 0.637300, and val accuracy 0.637300\n",
      "epochs 1392 / 1500: loss 1.055514 : training accuracy 0.636800, and val accuracy 0.636800\n",
      "epochs 1393 / 1500: loss 1.067558 : training accuracy 0.633200, and val accuracy 0.633200\n",
      "epochs 1394 / 1500: loss 1.057479 : training accuracy 0.647000, and val accuracy 0.647000\n",
      "epochs 1395 / 1500: loss 1.020120 : training accuracy 0.650700, and val accuracy 0.650700\n",
      "epochs 1396 / 1500: loss 1.010979 : training accuracy 0.644800, and val accuracy 0.644800\n",
      "epochs 1397 / 1500: loss 0.996928 : training accuracy 0.650400, and val accuracy 0.650400\n",
      "epochs 1398 / 1500: loss 1.073766 : training accuracy 0.611100, and val accuracy 0.611100\n",
      "epochs 1399 / 1500: loss 1.020322 : training accuracy 0.642000, and val accuracy 0.642000\n",
      "epochs 1400 / 1500: loss 1.057040 : training accuracy 0.626200, and val accuracy 0.626200\n",
      "epochs 1401 / 1500: loss 0.997986 : training accuracy 0.645800, and val accuracy 0.645800\n",
      "epochs 1402 / 1500: loss 0.999231 : training accuracy 0.632600, and val accuracy 0.632600\n",
      "epochs 1403 / 1500: loss 1.065219 : training accuracy 0.619700, and val accuracy 0.619700\n",
      "epochs 1404 / 1500: loss 1.020378 : training accuracy 0.640500, and val accuracy 0.640500\n",
      "epochs 1405 / 1500: loss 0.998684 : training accuracy 0.651200, and val accuracy 0.651200\n",
      "epochs 1406 / 1500: loss 1.000734 : training accuracy 0.647700, and val accuracy 0.647700\n",
      "epochs 1407 / 1500: loss 1.027238 : training accuracy 0.634500, and val accuracy 0.634500\n",
      "epochs 1408 / 1500: loss 1.016503 : training accuracy 0.645900, and val accuracy 0.645900\n",
      "epochs 1409 / 1500: loss 0.995625 : training accuracy 0.650000, and val accuracy 0.650000\n",
      "epochs 1410 / 1500: loss 1.016085 : training accuracy 0.654900, and val accuracy 0.654900\n",
      "epochs 1411 / 1500: loss 1.019401 : training accuracy 0.662600, and val accuracy 0.662600\n",
      "epochs 1412 / 1500: loss 1.146600 : training accuracy 0.623800, and val accuracy 0.623800\n",
      "epochs 1413 / 1500: loss 1.010685 : training accuracy 0.654500, and val accuracy 0.654500\n",
      "epochs 1414 / 1500: loss 1.023370 : training accuracy 0.652200, and val accuracy 0.652200\n",
      "epochs 1415 / 1500: loss 1.053912 : training accuracy 0.637500, and val accuracy 0.637500\n",
      "epochs 1416 / 1500: loss 1.045146 : training accuracy 0.643300, and val accuracy 0.643300\n",
      "epochs 1417 / 1500: loss 1.060479 : training accuracy 0.637100, and val accuracy 0.637100\n",
      "epochs 1418 / 1500: loss 1.024785 : training accuracy 0.629500, and val accuracy 0.629500\n",
      "epochs 1419 / 1500: loss 1.014366 : training accuracy 0.644700, and val accuracy 0.644700\n",
      "epochs 1420 / 1500: loss 1.036983 : training accuracy 0.635000, and val accuracy 0.635000\n",
      "epochs 1421 / 1500: loss 1.039490 : training accuracy 0.631600, and val accuracy 0.631600\n",
      "epochs 1422 / 1500: loss 1.060856 : training accuracy 0.627600, and val accuracy 0.627600\n",
      "epochs 1423 / 1500: loss 0.999637 : training accuracy 0.658000, and val accuracy 0.658000\n",
      "epochs 1424 / 1500: loss 1.007997 : training accuracy 0.649600, and val accuracy 0.649600\n",
      "epochs 1425 / 1500: loss 1.016341 : training accuracy 0.643600, and val accuracy 0.643600\n",
      "epochs 1426 / 1500: loss 0.993917 : training accuracy 0.661200, and val accuracy 0.661200\n",
      "epochs 1427 / 1500: loss 0.990690 : training accuracy 0.656400, and val accuracy 0.656400\n",
      "epochs 1428 / 1500: loss 0.983237 : training accuracy 0.652500, and val accuracy 0.652500\n",
      "epochs 1429 / 1500: loss 1.018031 : training accuracy 0.643200, and val accuracy 0.643200\n",
      "epochs 1430 / 1500: loss 1.024325 : training accuracy 0.638200, and val accuracy 0.638200\n",
      "epochs 1431 / 1500: loss 1.035490 : training accuracy 0.647100, and val accuracy 0.647100\n",
      "epochs 1432 / 1500: loss 1.017338 : training accuracy 0.654500, and val accuracy 0.654500\n",
      "epochs 1433 / 1500: loss 1.025360 : training accuracy 0.646900, and val accuracy 0.646900\n",
      "epochs 1434 / 1500: loss 1.004633 : training accuracy 0.654100, and val accuracy 0.654100\n",
      "epochs 1435 / 1500: loss 1.032027 : training accuracy 0.650700, and val accuracy 0.650700\n",
      "epochs 1436 / 1500: loss 1.000195 : training accuracy 0.657700, and val accuracy 0.657700\n",
      "epochs 1437 / 1500: loss 1.005101 : training accuracy 0.656300, and val accuracy 0.656300\n",
      "epochs 1438 / 1500: loss 1.002924 : training accuracy 0.660600, and val accuracy 0.660600\n",
      "epochs 1439 / 1500: loss 1.007427 : training accuracy 0.661800, and val accuracy 0.661800\n",
      "epochs 1440 / 1500: loss 1.040917 : training accuracy 0.642300, and val accuracy 0.642300\n",
      "epochs 1441 / 1500: loss 1.009499 : training accuracy 0.643700, and val accuracy 0.643700\n",
      "epochs 1442 / 1500: loss 0.996371 : training accuracy 0.638400, and val accuracy 0.638400\n",
      "epochs 1443 / 1500: loss 1.018546 : training accuracy 0.641000, and val accuracy 0.641000\n",
      "epochs 1444 / 1500: loss 0.980238 : training accuracy 0.657400, and val accuracy 0.657400\n",
      "epochs 1445 / 1500: loss 0.991620 : training accuracy 0.650400, and val accuracy 0.650400\n",
      "epochs 1446 / 1500: loss 1.168463 : training accuracy 0.612400, and val accuracy 0.612400\n",
      "epochs 1447 / 1500: loss 0.999394 : training accuracy 0.653700, and val accuracy 0.653700\n",
      "epochs 1448 / 1500: loss 1.029386 : training accuracy 0.636000, and val accuracy 0.636000\n",
      "epochs 1449 / 1500: loss 0.987780 : training accuracy 0.655100, and val accuracy 0.655100\n",
      "epochs 1450 / 1500: loss 1.012205 : training accuracy 0.630900, and val accuracy 0.630900\n",
      "epochs 1451 / 1500: loss 1.012235 : training accuracy 0.640600, and val accuracy 0.640600\n",
      "epochs 1452 / 1500: loss 1.062315 : training accuracy 0.611900, and val accuracy 0.611900\n",
      "epochs 1453 / 1500: loss 1.020467 : training accuracy 0.631800, and val accuracy 0.631800\n",
      "epochs 1454 / 1500: loss 0.986413 : training accuracy 0.650100, and val accuracy 0.650100\n",
      "epochs 1455 / 1500: loss 1.008402 : training accuracy 0.651700, and val accuracy 0.651700\n",
      "epochs 1456 / 1500: loss 1.010933 : training accuracy 0.649500, and val accuracy 0.649500\n",
      "epochs 1457 / 1500: loss 0.992022 : training accuracy 0.656300, and val accuracy 0.656300\n",
      "epochs 1458 / 1500: loss 0.989050 : training accuracy 0.662300, and val accuracy 0.662300\n",
      "epochs 1459 / 1500: loss 0.980277 : training accuracy 0.666700, and val accuracy 0.666700\n",
      "epochs 1460 / 1500: loss 0.970624 : training accuracy 0.672700, and val accuracy 0.672700\n",
      "epochs 1461 / 1500: loss 1.000890 : training accuracy 0.644500, and val accuracy 0.644500\n",
      "epochs 1462 / 1500: loss 0.982798 : training accuracy 0.658600, and val accuracy 0.658600\n",
      "epochs 1463 / 1500: loss 1.057207 : training accuracy 0.633600, and val accuracy 0.633600\n",
      "epochs 1464 / 1500: loss 1.000389 : training accuracy 0.655000, and val accuracy 0.655000\n",
      "epochs 1465 / 1500: loss 0.973202 : training accuracy 0.656800, and val accuracy 0.656800\n",
      "epochs 1466 / 1500: loss 0.977744 : training accuracy 0.656500, and val accuracy 0.656500\n",
      "epochs 1467 / 1500: loss 1.007126 : training accuracy 0.646900, and val accuracy 0.646900\n",
      "epochs 1468 / 1500: loss 1.055054 : training accuracy 0.621000, and val accuracy 0.621000\n",
      "epochs 1469 / 1500: loss 1.009296 : training accuracy 0.646500, and val accuracy 0.646500\n",
      "epochs 1470 / 1500: loss 0.992883 : training accuracy 0.637300, and val accuracy 0.637300\n",
      "epochs 1471 / 1500: loss 1.026075 : training accuracy 0.637800, and val accuracy 0.637800\n",
      "epochs 1472 / 1500: loss 1.024160 : training accuracy 0.635400, and val accuracy 0.635400\n",
      "epochs 1473 / 1500: loss 1.005640 : training accuracy 0.663400, and val accuracy 0.663400\n",
      "epochs 1474 / 1500: loss 0.974932 : training accuracy 0.657000, and val accuracy 0.657000\n",
      "epochs 1475 / 1500: loss 0.972222 : training accuracy 0.654600, and val accuracy 0.654600\n",
      "epochs 1476 / 1500: loss 0.990835 : training accuracy 0.655800, and val accuracy 0.655800\n",
      "epochs 1477 / 1500: loss 1.024407 : training accuracy 0.636300, and val accuracy 0.636300\n",
      "epochs 1478 / 1500: loss 1.063294 : training accuracy 0.654000, and val accuracy 0.654000\n",
      "epochs 1479 / 1500: loss 1.018793 : training accuracy 0.661700, and val accuracy 0.661700\n",
      "epochs 1480 / 1500: loss 0.991662 : training accuracy 0.650100, and val accuracy 0.650100\n",
      "epochs 1481 / 1500: loss 0.956748 : training accuracy 0.662200, and val accuracy 0.662200\n",
      "epochs 1482 / 1500: loss 0.972820 : training accuracy 0.654700, and val accuracy 0.654700\n",
      "epochs 1483 / 1500: loss 1.017204 : training accuracy 0.638700, and val accuracy 0.638700\n",
      "epochs 1484 / 1500: loss 0.993433 : training accuracy 0.637400, and val accuracy 0.637400\n",
      "epochs 1485 / 1500: loss 1.000125 : training accuracy 0.659300, and val accuracy 0.659300\n",
      "epochs 1486 / 1500: loss 1.009739 : training accuracy 0.642600, and val accuracy 0.642600\n",
      "epochs 1487 / 1500: loss 1.036136 : training accuracy 0.643700, and val accuracy 0.643700\n",
      "epochs 1488 / 1500: loss 0.985865 : training accuracy 0.644700, and val accuracy 0.644700\n",
      "epochs 1489 / 1500: loss 0.949118 : training accuracy 0.656900, and val accuracy 0.656900\n",
      "epochs 1490 / 1500: loss 0.980170 : training accuracy 0.659400, and val accuracy 0.659400\n",
      "epochs 1491 / 1500: loss 1.004812 : training accuracy 0.653000, and val accuracy 0.653000\n",
      "epochs 1492 / 1500: loss 0.974553 : training accuracy 0.652900, and val accuracy 0.652900\n",
      "epochs 1493 / 1500: loss 0.958297 : training accuracy 0.659400, and val accuracy 0.659400\n",
      "epochs 1494 / 1500: loss 0.960844 : training accuracy 0.664900, and val accuracy 0.664900\n",
      "epochs 1495 / 1500: loss 0.971128 : training accuracy 0.659600, and val accuracy 0.659600\n",
      "epochs 1496 / 1500: loss 1.001218 : training accuracy 0.656100, and val accuracy 0.656100\n",
      "epochs 1497 / 1500: loss 0.985499 : training accuracy 0.661900, and val accuracy 0.661900\n",
      "epochs 1498 / 1500: loss 0.977374 : training accuracy 0.653600, and val accuracy 0.653600\n",
      "epochs 1499 / 1500: loss 0.987296 : training accuracy 0.648000, and val accuracy 0.648000\n"
     ]
    }
   ],
   "source": [
    "net.train(x_train,y_train,x_train,y_train,learning_rate=1e-4,learning_rate_decay=1,num_epochs=1500,reg=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1=net.params['w1']\n",
    "b1=net.params['b1']\n",
    "w2=net.params['w2']\n",
    "b2=net.params['b2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) \n",
      "(10000,) \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x_test = np.transpose(unpickle('cifar-10/data_batch_4')['data'])\n",
    "#x_train=np.transpose(x_train)[0:20,:]\n",
    "x_test=np.transpose(x_test)\n",
    "y_test = unpickle('cifar-10/data_batch_4')['labels']\n",
    "#y_train =y_train[0:20]\n",
    "\n",
    "print np.shape(x_test),'\\n',np.shape(y_test),'\\n',y_test[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(X, y):\n",
    "        \n",
    "    y_pred = None\n",
    "        \n",
    "    z1 = np.dot(X,w1) + b1\n",
    "    a1 = np.maximum(0, z1) # pass through ReLU activation function\n",
    "    scores = np.dot(a1,w2) + b2\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    acc = (y_pred == y).mean()\n",
    "    \n",
    "    return y_pred,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3962\n"
     ]
    }
   ],
   "source": [
    "pred,acc=accuracy(x_test,y_test)\n",
    "print acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 6 4 ... 3 4 8]\n"
     ]
    }
   ],
   "source": [
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name(n):\n",
    "    if(n == 0): return 'airplane'\n",
    "    elif(n == 1): return 'Automobile'\n",
    "    elif(n == 2): return 'Bird'\n",
    "    elif(n == 3): return 'Cat'\n",
    "    elif(n == 4): return 'Deer'\n",
    "    elif(n == 5): return 'Dog'\n",
    "    elif(n == 6): return 'Frog'\n",
    "    elif(n == 7): return 'Horse'\n",
    "    elif(n == 8): return 'Ship'\n",
    "    elif(n == 9): return 'Truck'\n",
    "    else: return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ship\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAUICh0qAkMChEiAkcKgECAmc/wcJCrM8i7pSgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a1594d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "single_img=x_test[0]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[0]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb229e215d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[1]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deer\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4FAECAkcigAhgfP/Y1FEFSQLmkoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb229f0db10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[2]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a086b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[3]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horse\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a4f9150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[4]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deer\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnqUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a2c3250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[5]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truck\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a1de310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[6]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[6]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb254019c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[7]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[7]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deer\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/D+xtDKMFf7jIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a84f850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[8]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[8]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a559a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[9]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[9]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVAKEBA6VACGB838f1XlPhqV6vgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a424f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[10]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyAgIEXNkBISIOTICQsQcGQEhYs7/A4TDwAlvTrZ7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a41e610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[11]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[11]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICRw/g9okdm6D/unbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a2a0110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[139]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[12]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c13a01bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[13]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[13]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truck\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c1344a790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[14]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[14]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c14af1dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[15]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[15]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHHxJREFUeJztnVuMXWd1x/9rn8vMmbvHHjt2nOAEjJPgQi6jkCgRooSglCLgAapQVEVVqkgVrUBUgqSVqiL1AfpA0ycqi0DzAIQ0QIkiBEQhUUVVQmycgB3n4jhOPPX9MvfLua0+nD17rX1yJnPGnrMdz/f/Sfass8/e+/vOnpk1a33fuoiqghASLtHFngAh5OJCJUBI4FAJEBI4VAKEBA6VACGBQyVASOBckBIQkbtE5GUROSgi96/WpAgh2SHnGycgIjkArwC4E8AYgOcAfE5VX1y96RFCOs2FWAI3AzioqodUtQzgEQCfWp1pEUKyIn8B114O4Ih7PQbgg80nich9AO5ryLipq5Brfr+l7Iki01W5nLs+dX7ra98yHyer1hO5Xq8te6f2jSZ1kt2tUrPPUa/ZOf5zR+7afM3m5D93LUrPUNXOi2r2mWpOx9fyhUTeufOaNj4DuRTZs2fPaVUdWck1F6IEWv2uvOXXRFV3AdgFAKWuvF69ZTD1vv/hLhQK7rj9APf0lhK5f3BdIkc5Ox+ylHIApG7TisTkysJsIs/PTtnYYr9I/hPVa3Zf70Y1u1ReuVRRTORjkybPTNs5+YJ9G3rrC4k8MjWRyOsG7LlNlOw+AFCpTCdycdI+x6T2mDy8NZF37/41yNpERN5Y6TUXogTGAFzhXm8FcPTtLogQoVTvTf9lh/3yStV+yerur5v7nUK96v5iu198yTmLAmlrI+fecyLcH2YUiqZQ8u7yykLV7ut/2dXdqFkJOKWTz9sjLnV3J/LcjCkgcX/xu/L2C17M2fn+I+Uj90AAdEc2Rr3ulEvOPUPMg5BWXMiawHMAtovIVSJSBHA3gMdXZ1qEkKw4b0tAVasi8jcAfoHG36nvqOr+VZsZISQTLsQdgKr+DMDP2j0/inLo7e9DsWgmr18TqDpTv6plm6Szz2vunJrzn3PO7JYobeDUnGuRd4tqOScPbtiQyH195kuPT0wm8sz0XCJXKhUbr2kNws+31Ge+vI6bqT43bf5+rWouR827PpGNUZux5zFcsHURAMjX7bOfde5ArWDP5/TpFbuKJBAYMUhI4FAJEBI4F+QOrBQVRbVQRbGnKznmV/UjtwVXrJvL0FOyLUJ1e3aVmpnR6s3gmpnRAFAv28p4zZnqIyPmArznve9J5KF1Zm7Pz9u1MzMmz83Z6n7zFqHfEch32WcdOH3Orp86kcjHTpy1MVz8wHCXfW6Ztc9UmDK3BAAqVTe+22atRvZ8cpq+hpBFaAkQEjhUAoQETqbugIggV8yjWjcz1auhfNGt8LtV8vlZM8PFRRKWei2Yplo3c7lcsd0AIB0l2O+uueJKi6LbdNllds7ggM2pYG5JJDa2dwGqFfd5AJQrfi62qt/X35vI02dPJvKZCXMNvMsx4wKH+nrNzJ8qp92datHchunIzpudm0nkwTz1PWkNfzIICRwqAUICJ1t3AIK8FhDVXXCNs2z9InvO5QXUnKnvLPKUeR65gJ1aNW0ud7kdiC2bzezfuNGSrYpdLmbfBTN1lyxwKJfLO9nm5xOGAKC8YOPP+l0El1OwY/uORJ4umzux9w8HEvnYpF1bdm5Jr9stAYBZt0tyZsaCkEpuJ6VUby/LkoQHLQFCAodKgJDAydQdQF2BuTLUmdV+Hb/mtgTqbjXbx+LnxK6tzLtgIXHpuLn0x7pi66ZEfte7rkzkUsl2CoouyKbgVuW9W5LzBVBcPkJzKnHe72C4gJ9qr32+TVs2J/J7yxbjf/iNY4n85kmTZ2ZtpT+abgoW8gVK1FyRfN5cGa2y3RxpDS0BQgKHSoCQwMnWHQAQqaRWycVVKZNIWh6PfBUft8pdL5tJ3jtgZveWLb7gEbDlCtsRKLrqPqkKQM6F8IFAdZfbm65PaK6Ir0/YeNMFFbmNg8jtUuSLdk5XwY4PdLudiaLlHVTKbh5NlYV6uuxe3bk+k8WeSXe+C4S0gpYAIYFDJUBI4GSbSgygIjlE3tT3AT9OJ+WdSZ1P6SrnDrjVeh/gU+rtgafubPK5eVuJH3Ar9z4KqeZcgFykTnYBSa44aK2Wzh0Qv4NRcSnOLo9AXbpzf5/NY+fOaxO5pziUyFW7FPmmsu3qvou++KqvOCQMFiJLQEuAkMChEiAkcDJ1B+qqmFlYQMGtxPe4yjuR3x1IL8XbOe4NdXLVFetcqKRzB+YXzJbuL9rugPigJb9j4aoUVVxaci5yprYLKGruTKTOlak6F2Bu1oJ85lyarzh3ZdClGw8OmDxxzvIImnMjfOZ0DfYcIifnqO/JEvAng5DAWVYJiMh3ROSkiOxzx4ZF5EkReTX+uu7t7kEIeefSjiXwHwDuajp2P4CnVHU7gKfi14SQS5Bl1wRU9b9FZFvT4U8B+HAsPwzgGQBfXe5e5UoFY8eOoa/HtsS2uvx+3wOw7tYH6s53F5fH75t+LDjfe3LKmnICQI/bgvPrCN6Xr7s1Ab8+4EP+UvUO/Jyamo/4bUHfUKXs6gbMu63KqWlrKLqwYMdn523dYHLS6gSUq+kIxbm6b4Dq+y76bs70/EhrzndhcJOqHgMAVT0mIhuXOtG3Jo+WaD1OCLl4dPzPg6ruUtVRVR2lEiDkncf5WgInRGRzbAVsBnBy2SsAAIpavQq4KLyqmBkfuS24mk/ocaW1fM0BdYk7eRfBV2naIlRnIldSvQydGe1cgHm3lTc/O57Iw4PWV3BoqD+Rc02VfP1W4pE3rVv7G0cOJ/L6EatoXHaJULNzVm343Li5ALMLNqep2XSb8bPTtn1Y93rd1TwosNowWYLz/cl4HMA9sXwPgJ+uznQIIVnTzhbhDwD8L4AdIjImIvcC+DqAO0XkVQB3xq8JIZcg7ewOfG6Jt+5Y6WAiglJ3EevWmykcdbl1gqI71zUigXMHpsZtJb3sSm4ND1l0XW+vyQBQdFGJfuG/7JqGzJ2zPoF7976QyOfO2PHbb701kXM5q1kwPZPejRgaWG/zdSv/L71yMJFvGvyAm689jxOnzAWYmrHPNzBsoRgTbgcBACan7bxc3sqkiWvyogWux5DW0FEkJHCoBAgJnGzLi4kAkaBYdOW0Bs10H9gwnMh1X+W3brpq8ujxRJ47ayv3+XVWVqurqZ4Act7PcElDrmyZDxaamDDzvuzamneV7D77X34lkX/zm2dTw+3c+f5E3rHjmkT2JrmIM9XdtSVX/mzdsLkJuYI7vmEDPLPV1uXaaq6KcV5YbZi0hpYAIYFDJUBI4GRebbiei1Jmao8LCto4ZGbunAv4qbnc+5FuO3/doJn9JVfJt9nwlZytmMMF8vgW5j09dq/bb7/NjjsXYHDAAoSedz0Dn/+9yQBQVRvj2p07E/mKKy9P5OPHj9j83LW9vZbnMDRo4x08ZI1I8l3p3Y/Rm26wsV3Q1KFXXk3kdEEyQgxaAoQEDpUAIYGTvTsgQN2tZuddVmyv2wWI3Ir+ZH3SjovFyQ/0ujyAirkMlZl0bH1xi7kD3a6t95RLOZ6ft9j89Rss2KfoUpenJizwpzJnYw/12a4GAExP2r1OHLe0iuFhM+OjyKX5urTpcZcyPD1j96nOWY5FLR0rhP5uc2WmXSv0o0ctb8GXbiPEQ0uAkMChEiAkcDJ3B6BA2aXwzrjKuXOurXbVmeGTc67yjpoJX8jZfebLvsFI2h2ouxXzri5zDVTNPJ+cNJfjzJlTidzXbSv042dsHvMTNkYpskAeAKjP23jHxtyqfrTBya7Zia9u7Fwl7w74BizNlYPLc3ae/6wLrsqyb+9OiIeWACGBQyVASOBchNbkwIIL5zldMVO2p2Lmdq5uZvvUjB2vuvTYirNwx2fNTSjOplN7a+qbiZjL4V2Dvj5bYT/8+uFEnnQuQK7igo6c+7Flw6bUeKcmTyfygRdfTOSBfsspWL/eqhSNu12K3l5zP/oHLX24Om3PrLKQ7n3oi5aWBiyHYv162+VoblhCyCK0BAgJHCoBQgLnIuwOKKpmSePshK3KVw+/nshF18+v5qr79Ls+3JGLiI9cU4CurvRqfd4V2ay4/gT+4w+4vIB1rorPvr2WF9BVs/tevXVbIvf2pFOX979mLkDfejPPN20cSeRa3czzvMttmJ010/7sOXs26nYTypp2B3wR0g195pq8+91XJ/LE+DgIaQUtAUICh0qAkMChEiAkcDJdExAR5ItF1BZcrYBZF4Xnmmr09ZmPPpgzv3q4YNteBZcU0zVgawIjA7b9BgAFFy3ne/LVXfOSuova27JlcyK//4/el8hHXhpL5LzLx5Fa2kcvddljve6a7Ync65OXZuyagdR8TyTS2XPmx1fLvr9herzaSbtGC/ZZ1a2T9DStWxCySDt9B64QkadF5ICI7BeRL8bH2Z6ckDVAO+5AFcDfqeq1AG4B8AURuQ5sT07ImqCd5iPHACx2IJ4SkQMALsd5tCfv6u7C9mu2Y/yEma9wDTaGu8xcHi6aC9CTs+YhxXprvdXf48qAlfpS7/ntQ9+Tr1q17cKZacvj73Lz8CXBTrxhlY7HJ+z8/r70eKUe20rsG7B7wVX8LfpEJlfybMBF/PkSADOuxBqkKYHo7Fk3L9tWhEtGyrGeAFmCFS0Misg2ADcAeBZN7ckBtGxPLiL3ichuEdldqVRbnUIIuYi0rQREpA/AjwB8SVUnlzt/Ed+avFDIPjaJEPL2tPVbKSIFNBTA91T1x/HhFbcnjyRCV1cxtbqtNVv1PjdpJu/khOuv53SVuCYh4kz7bde+J5G7+q1pR2NcF1nojvtu3TVXeXjW9Q+suLkOX2Y7E0cPWZLQhMvnb5xndQokZ/Mt11y0onMNFPYM1m+wnYItmy3CcHzckoz8TkZ8g4Sq26nwZdxE6A6Q1rSzOyAAHgJwQFW/6d5ie3JC1gDtWAK3AfgLAH8QkefjY3+PRjvyR+NW5W8C+GxnpkgI6STt7A78Gun+GJ4VtSefm5/Dvn37UZm1irjdLpCn6Fa9XQkA1JzLoM7czbsV9m2up2HUnw6MKbj+fjkf5eNKcUWRyfPzbtdg1kz9ktsFyJXMPJ+cSC+RXLluyObrzPCFBQuGKrjW6z6AyTccufEma19+5pyNd+SIVRGOZ+8+hw8Wcm7UW1qyENKAYcOEBA6VACGBk+meXa1Ww+TkFLpcfHvNBbHUfNy7b7HtChDUXAx9sdsChIq95g5UmryXur9XzZXpcqvn8y6fwfcorLr25XNup2DatywfSPcGLA3a7sSkC4YqORegu2QBUJHbphDn+mzaZLsR11zz3kQ+fsKqIQPAnMu5iCK/c0AdT5aHPyWEBA6VACGBk3EIn0AkQs2Z2L4RiU/tzaVKhzlz2V3b7WL8fXOOhZl08I4r1Itul+YLt3JfcW7CjCvxJa51eq5oJny+x+RC0VUhBrDg3IaFSTPVc27Xolq1zypuK0Td5/MNQ0ZGzDXo7raxAWBqyoKbfJ6E+CArBguRJaAlQEjgUAkQEjgXodqwQNxqvTfvXQg91AW3aN3O8RWG56fN7D/40qt2flNcTOmqbTZEsXUATRSZSX/smKU6z8/brkFvvwUBDbv25XMLafdj/wGrULx5o7Ut9ynO5QX7sD6nIMrbPCZc/sTJU0unZvhqyr6akO9LuHS8FwkdWgKEBA6VACGBk7k7IBCfRQvxpr4L3qm7k6rOT/CL3LWyreK/fuTNRC5LOtW2f8jKHw6pxeZLZGP4FfZDr7+RyCeOWmBOrysIGhUtUGlm1lX9AVB2rdSrs1aZaNOQpQaXSrZTMDFp5585Z41WTpy0ikEHXnotkWs+JRlAT0/Jved2W9yOhw+SIsRDS4CQwKESICRwst8dkOS/Bs5K9enDKXcg8hVy3Iq+v63zE06dPQfP7194IZEv33KZ3bdmK/+nT1uloDNOrrnU5QlnqlfUVwZKm9p5N8cTx22nYd/+/Yk8OGT5BeOTdt9TZ864+bk8h7LNNWoqNJrLOZfKpxK7qkbNxYgIWYSWACGBQyVASOBQCRASONn2IkTcUMMHCfrlAb//58RccwhgcoqdlHMVhevlWuq8sTdsy2/ijPn7VRdRV15w224uuafgypFVXaXjXM2vU6R1adFFJfrPMfZ/RxL5zSPmpM+XbWw/RpTz3x6XUOX8fgBQl3jlAzC9ivdrKYR4aAkQEjhUAoQEzkVvCbSUkZra/kNrN2GpG0VNufMFZ1ZXvNnvzivkWz8K38wjcoPkND1Dj9+ujCLvsrjEKe/uRDa2Hy9l26fqBCw9nn/HP1t6A2Qp2mk+0i0ivxWRF+LW5F+Lj18lIs/Grcl/KCLF5e5FCHnn0Y47sADgI6r6AQDXA7hLRG4B8A0A/xq3Jj8H4N7OTZMQ0inaaT6iABYzXArxPwXwEQB/Hh9/GMA/AfjW6k2ttemcOmOJN5pX631zj1zUWu/5PHzvTkjkS5v5HYHWc239+q1E7r6+jJjP8/EWvI+mbP7c/l661E4KywmQJWhrYVBEcnELspMAngTwGoBxVV10YMcAXL7U9YSQdy5tKQFVranq9QC2ArgZwLWtTmt1rYjcJyK7RWR3fYm/UoSQi8eKdgdUdVxEngFwC4AhEcnH1sBWAM0N8hav2QVgFwAU8jlV1ZTJ2k4V3KXO97JXL82tu32VrSU3F1IJQak33PHUentb8/VTqXuVu4Q+FLeb4OsBLGXmtwvdAbIU7ewOjIjIUCyXAHwUwAEATwP4THwaW5MTconSjiWwGcDDIpJDQ2k8qqpPiMiLAB4RkX8GsBfAQx2cJyGkQ8iFmpkrYXR0VHfv3p3ZeISEhojsUdXRlVzDsGFCAodKgJDAoRIgJHCoBAgJHCoBQgKHSoCQwKESICRwqAQICRwqAUICh0qAkMChEiAkcKgECAkcKgFCAodKgJDAoRIgJHCoBAgJHCoBQgKHSoCQwKESICRwqAQICRwqAUICh0qAkMBpWwnE/Qj3isgT8Wu2JidkDbASS+CLaHQeWoStyQlZA7TblXgrgD8F8O34taDRmvyx+JSHAXy6ExMkhHSWdi2BBwF8BcBie831YGtyQtYE7TQk/QSAk6q6xx9uceqyrclPnTp1ntMkhHSKdiyB2wB8UkQOA3gEDTfgQcStyeNz3rY1uaqOquroyMjIKkyZELKaLKsEVPUBVd2qqtsA3A3gV6r6ebA1OSFrgguJE/gqgC+LyEE01gjYmpyQS5D88qcYqvoMgGdi+RCAm1d/SoSQLGHEICGBQyVASOBQCRASOFQChAQOlQAhgUMlQEjgUAkQEjhUAoQEDpUAIYFDJUBI4FAJEBI4VAKEBA6VACGBQyVASOBQCRASOFQChAQOlQAhgUMlQEjgUAkQEjhUAoQEDpUAIYFDJUBI4FAJEBI4bfUdiFuQTQGoAaiq6qiIDAP4IYBtAA4D+DNVPdeZaRJCOsVKLIE/VtXrVXU0fn0/gKdUdTuAp+LXhJBLjAtxBz4F4OFYfhjApy98OoSQrGlXCSiAX4rIHhG5Lz62SVWPAUD8dWOrC9manJB3Nu32IrxNVY+KyEYAT4rIS+0OoKq7AOwCgNHRUT2PORJCOkhbloCqHo2/ngTwEzQakZ4Qkc0AEH892alJEkI6x7JKQER6RaR/UQbwMQD7ADwO4J74tHsA/LRTkySEdI523IFNAH4iIovnf19Vfy4izwF4VETuBfAmgM92bpqEkE6xrBJQ1UMAPtDi+BkAd3RiUoSQ7GDEICGBQyVASOBQCRASOFQChAQOlQAhgUMlQEjgUAkQEjhUAoQEDpUAIYFDJUBI4FAJEBI4VAKEBA6VACGBQyVASOBQCRASOFQChAQOlQAhgUMlQEjgUAkQEjhUAoQEDpUAIYFDJUBI4LSlBERkSEQeE5GXROSAiNwqIsMi8qSIvBp/XdfpyRJCVp92LYF/A/BzVb0GjR4EB8DW5ISsCdppQzYA4EMAHgIAVS2r6jjYmpyQNUE7lsDVAE4B+K6I7BWRb8c9CdmanJA1QDtKIA/gRgDfUtUbAMxgBaa/qu5S1VFVHR0ZGTnPaRJCOkU7SmAMwJiqPhu/fgwNpcDW5ISsAZZVAqp6HMAREdkRH7oDwItga3JC1gTttCYHgL8F8D0RKQI4BOAv0VAgbE1OyCVOW0pAVZ8HMNriLbYmJ+QShxGDhAQOlQAhgUMlQEjgUAkQEjhUAoQEDpUAIYFDJUBI4FAJEBI4VAKEBA6VACGBQyVASOBQCRASOFQChAQOlQAhgUMlQEjgUAkQEjhUAoQEDpUAIYFDJUBI4FAJEBI4VAKEBA6VACGB005D0h0i8rz7NykiX2JrckLWBu10IHpZVa9X1esB3ARgFsBPwNbkhKwJVuoO3AHgNVV9A2xNTsiaoN02ZIvcDeAHsZxqTS4iS7YmB3Bf/HJBRPad10wvnA0ATnNsjr3Gx96x/ClpRFXbO7HRh/AogPep6gkRGVfVIff+OVV923UBEdmtqq3amXUcjs2xOXZrVuIO/AmA36nqifg1W5MTsgZYiRL4HMwVANianJA1QVtKQER6ANwJ4Mfu8NcB3Ckir8bvfb2NW+1a8QxXD47NsTl2C9peEyCErE0YMUhI4FAJEBI4mSgBEblLRF4WkYMi0vHIQhH5joic9DEJWYQ5i8gVIvK0iBwQkf0i8sUMx+4Wkd+KyAvx2F+Lj18lIs/GY/8w3urtCCKSE5G9IvLERRj7sIj8IQ5t3x0fyyS0XUSGROQxEXkp/t7fmtH3fHVC+lW1o/8A5AC8BuBqAEUALwC4rsNjfgjAjQD2uWP/AuD+WL4fwDc6MO5mADfGcj+AVwBcl9HYAqAvlgsAngVwC4BHAdwdH/93AH/dwef+ZQDfB/BE/DrLsQ8D2NB0rOPPPb73wwD+KpaLAIayGtvNIQfgOIB3rXTsjk3KTe5WAL9wrx8A8EAG425rUgIvA9gcy5sBvJzBHH6Kxs5JpmMD6AHwOwAfRCNyLd/qe7HKY25FI4fkIwCeiJVSJmPH92+lBDr+3AEMAHgd8SL7xfp5A/AxAP9zPmNn4Q5cDuCIez0WH8uaVJgzgJZhzquFiGwDcAMaf5EzGTs2x59HI3DrSTQssHFVrcandPLZPwjgKwDq8ev1GY4NAArglyKyJw5VB7J57lcDOAXgu7Er9G0R6c1obM+SIf3LjZ2FEpAWx9b0vqSI9AH4EYAvqepkVuOqak0b2Z5bAdwM4NpWp632uCLyCQAnVXWPP5zF2I7bVPVGNCJbvyAiH+rgWJ48Gq7nt1T1BgAzyDijNl5r+SSA/zyf67NQAmMArnCvt6KRg5A1mYQ5i0gBDQXwPVVdDK7KNMRaVccBPIPGmsCQiCwminXq2d8G4JMichjAI2i4BA9mNDYAQFWPxl9PopHqfjOyee5jAMZU9dn49WNoKIUsv+cXFNKfhRJ4DsD2eKW4iIbZ8ngG4zbT8TBnEREADwE4oKrfzHjsEREZiuUSgI8COADgaQCf6eTYqvqAqm5V1W1ofH9/paqfz2JsABCRXhHpX5TR8I/3IYPnrqrHARwRkcXsvTsAvJjF2I4LC+nv5GKFW7T4OBor5a8B+IcMxvsBgGMAKmho6nvR8FGfAvBq/HW4A+PejobJ+3sAz8f/Pp7R2O8HsDceex+Af4yPXw3gtwAOomEudnX42X8YtjuQydjxOC/E//Yv/oxl8dzjca4HsDt+9v8FYF2GY/cAOANg0B1b0dgMGyYkcBgxSEjgUAkQEjhUAoQEDpUAIYFDJUBI4FAJEBI4VAKEBM7/A/aTRKHfDg+bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c1340ffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[16]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[16]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deer\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c13cb50d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[17]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[17]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c133b3250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[18]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[18]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c133d4610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[19]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[19]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deer\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHf9JREFUeJztnVmMXOd15/+n9qru6o1sUhSbFiWblmUbNiV3FBsaeDJWlDgL7DwkA9vBwAgUCAiSwEYCJPLMYDAB5sGZh4znyQPCdkYPjpcocWwI2QRZQuJkIouypIlMWqZEU2aLSzd779qXMw91u865PSWxmmRdmf39fwDRp27fe7/v3m6ePuf7ziKqCkJIuKTe7AkQQt5cqAQICRwqAUICh0qAkMChEiAkcKgECAmc61ICIvJhEXlJRF4WkYdv1KQIIckh1xonICJpAD8E8ACABQDPAPi4qp66cdMjhIya67EE7gXwsqqeVdUmgK8C+OiNmRYhJCky13HtYQDn3ecFAD+98yQReQjAQwCQzabfN72vjOLYvv73u62mye12X06lTT91um13TtVunpa+2Gq5a1PZ2BzSaXvMdCrdl7Vr57Tarb7c7pqcydq1fk7tdsPkjo0NAF1331Taxsum8n1Z3LMKzBrrukllC2Wbq5u3vz8A5Apjbl4td56N0WrZRXccPgSyN3n22WevqOrsbq65HiUgA479f76Fqp4AcAIADh6a1k88eD/ec+8n+t/funixL9evLPblXLnYlzfrS325svRcX06VC335wtLlvlwqxn/Jy+WDfXmyZP+xWlX7j3F5+VJfXt6yOe07vL8vFyfs2uXVsyYvX4mNV2vaayhNTPTlQ6W39eXsFXumtJhCqTVMKd7y9g/25c74ZF+ubsVf81ve8VN9+cqqvcPKps3rwkVTnl/77H8G2ZuIyKu7veZ6lMACgCPu8xyAC298iULRwuLCyf4RaXT6cqVj/zFS9ZIdryz05ZpW+nJnZb0vr27Yf6TNqp0PAI1NZz3kzUroNt1fYzU5O2Z/dSuNTZNXfuyexObdbMb/NDcb9lrLafdXPmfXV8ZtvmNdUy63HrzLxtja6svrr9k7u9I25QcAYxNmYays2DUvPPPtvjw1/VYQMojrWRN4BsAxEbldRHIAPgbgWzdmWoSQpLhmS0BV2yLyOwD+DkAawJdU9fs3bGaEkES4HncAqvrXAP562PPbnRYWVxbQdIuBxZz5updXzK+fnjETGVLvizV3bU7M1C6Wzcyvd8zUBoCNqvnJnaqdl3Vm/KWuuQz1MVvu0FUzlrqweUxMmbuSycUXBovuc7ttbsPSxobNo2ZjN1Dry/mtnN0ofUtf3Npas/M75q4AwKVTZvY3YYuElWXnRi0vg5BBMGKQkMChEiAkcK7LHdgtXe2i3q5ibdV2ATbVVrMvLdlq/8aGbW910+YCjBfNFC4VbfrNjpndfs8fAJp503XVlrkKzYrdd915EBln3audgnbGTPhmy9yHTNbGBoBUys6rrtvNmuZNoONckcakXV+omktUr5gJny26/f+muQ8AsLps7y2XNXfn9iO2a7C2FXeRCNmGlgAhgUMlQEjgJOoOAD2t0+yaOdvcMjM33TWdtLJo59QbZkeXJ83ETWXsWsm6AMZsPJhxPDPVl6dcJOKZTYv629ywMY7dcpvNCeZ+nFu0KOn6lpnwU/vjr1Hda61uOLO/ZrsZKee+dPMW/JPtmiwuMAriXZG4u7NSc7sWWXuH2UlzDSb2U9+TwfA3g5DAoRIgJHCS3R1oC2rLOYjLiEsXTA+pM++rV8xcbrjF8G7HZ9mZeT2zz4J3ZmZmYuPOFMb7cipr1zTOmdsw6TLxDk7d2pcXl2z3orJoY4+VLahnc2VHsFDavpdzWYhtcc9XtW2HA3lLeJosWYblyVdfsHu63YRKJ55AtLVh43fcTkjW7VpkqO/J68DfDEICh0qAkMBJdndABdrMIp0xc7m5aSZyZdWZ1W1XRMOl7Y6N2/FCyeRcx+65LxevqTA5aebzeVe/oFAwd+DtR4/15YOTZpKfftl2EAqTdv7EjMtViMfuYOWCC8zpevPcDqecy7C2ZPOru+CpRZfnoC53ID8Z193tjH1er5jbkEvZfNM7AqgI2YaWACGBQyVASOAk6g5Iqle1pzxtATtbS2bST7pAIGRW++KtR9w0W2ZGV1xcftW5DOd+HC/39fNH7u7LSwVzB0pW+QvHj8/ZnKzSGJrOjD6w70BfblfMhD8wEa/0c+Aum8vmps0xX7BnbdbteGXdzPblilUymp61482c22WoxXW3utSFVtvcgVLX3IlUswhCBkFLgJDAoRIgJHASdQdS6RRKU3k01Uze7JSZrFMuAKfcsIpDWw1LlR2bMHO3PGkBPhlXDly78eV6TZtJP16w+06rHZ8VM5ebMFP9XW97u92oaCv3NReg06rHU4klZa+1cNDlOmTNHaiu2jVX1u356q48erNrY09NWN6CpsxVAoAJl1K9uWljTJcsgOrg7DgIGQQtAUICh0qAkMBJ1B0oZjN496F9yKQGd/URV9JHXSeeTttW5bN5WzEX1//k8OH39uXymLkJAHD6X77Tl2cq5n4cu/0dfflQ3gqbHpt3TULGn+/Lr61Z4FDLmfa5HanLrYbPezC56HYjGq5XwNLadF/eHLfnmHzLO/vy2yb/sS+P11+Mjdft2L3UpSiPF8y9mp4ogZBB0BIgJHCuqgRE5EsisigiL7pjMyLyuIicib5Ov9E9CCE/uQxjCfxvAB/ecexhAE+o6jEAT0SfCSE3IVddE1DVfxCRozsOfxTAz0TyIwCeAvCHV7tXMZ/Hu247FusSLCm3JuD69vmuv359QMRtBbpuwIdusbaI7U58O0yr5jNPFc03LrjHP/rWd/XlJRddl03bluIt43faTUs2V0F8i7BSsbJgFfd82bytI8zNWaPTdNHm9/jzVmegsPWKDefGy2Xiax6pkkUsdl34YLVuW6Dnz62AkEFc65rAQVW9CADR1wOvd6KIPCQiJ0Xk5MZm7fVOI4S8SYx8YVBVT6jqvKrOT5QZv07ITxrXukV4WUQOqepFETkEYPGqVwDoKlBRQLtm8mbTtmWX8aa+S75X13PQl+tKqW3NrdQsom7xYtziWK7afVOusm9jwxqIXFk3E36hbq5IDdY/MF0w83rCmeC+NgAA6KY932Te1kyzzhUp5s3NSOfMhP/u09+zMdK2p+jaI2Is4/o0AkiXzT3IjrtaDS5JKV+qg5BBXKsl8C0An4zkTwL45o2ZDiEkaYbZIvwKgP8D4E4RWRCRBwF8FsADInIGwAPRZ0LITcgwuwMff51v3b/bwdrdLlYrldiOQNaZwqm07QKkxEUGOlWV6Tp3wO8sNMyEr3bi7kCuYKvvnbaZxT7RaOHsy335ktp9Nyp2fsbNL593Ow4u+g8AZnJ2fc1VGpuaMNeg5FyDTs1qHPzbn7Jdg0Lezjkya2b+eCoeliF5eyfZks0lm3FVnTXezpyQbRgxSEjgUAkQEjiJJhA1my386EeXUCybmVuesG3DjDOjs66nXtqtjDvvAemMTb/j+vNtrcSbc0zkbKehuv6ajeEan3RrtjvQrNv1Z09ZG/Vb58zU3jdtz7CzN6CvbVDIuUQotVbjza7VJhiftt2Ed7/PrfS7RKuckzsaL2fmS6DV2q5WQ9uuSWvibSfJTQItAUICh0qAkMBJ1EZsd7tYqW+hkDLzt5m15fOSmJlbcKb+RNEF5rjc/ZTLKeimXI/C8Xh+PxquuvGynXeo4IN8zAXQLZvfFRdz31y0FfYp58ZM3BkPFsq4eyHvTHVXJq1et3uVshb8k3E7Hp2OzaPacrkUOxb6xY3nXlssyKrhrifEQ0uAkMChEiAkcBJ1B3LFDI6++wByLoilkDdTuuSCerKu2YaLG4KL44m1KW+6vnuFo3F34AevvdqXFy5bpd7bXfmutmvUke5YvkAhbyb1lYvWD/D8KRvjwFw8MarlchoaXXMH3GFknE2/1rU5dV1AEpxrAN9XMBX/seVT9g4zGXuOBsyd6LTYi5AMhpYAIYFDJUBI4CTqDmTTKRyYHkPLNffw+QKStuNddSvsLkCo6yoMF7JmBk+I6/u9Y/W8XLYDB6csPbfdtnttVs1c9tWQ5/ZZlaLzFUs93lwzuen6/wFA2u1ONJt235TLe4DYQ1WalsPQbriVfqeis2l7Bt2Rutx0uy3i3Ix2w+7b6carHxGyDS0BQgKHSoCQwEk8oFw7ihR8cVEznduucGjabQmkYtsDvimJmcEt5z3kJJ7aWypYnH+jZGZ8111Tb5q5LL4A6UG7dmvV5rG+YTH6rR0r79mcC/4Rt8vhXJyucyFyLmU479yETtqZ8O4dNNXeEwC03X2l4VwDtzPRaNEdIIOhJUBI4FAJEBI4yboDCkhHUHQBQj4gRuFW2X1gjQuA6bpV7lbLTN9MzlyA2M4CgPKUpecuqqUGV6q2ep5xqcvdll/td26JSxHuup5/jXbcPM87Uz/lfA4VF9zkYvl9wVT/fOLG81FSvt8gAKRc6SV/r6bLKegi/k4I2YaWACGBQyVASOBQCRASOImuCagCnZai6bbBUm3zVYs5VzbLneN3xFJd83lz6qIEmy7BJh1PIMq7hKXLq5YEVFuzEl9jRZfIVLQowYlJaxKSclF7KTd0LheP4POJTR23XuC3C1NdVz7NzbeTsfeR9r6+Px/x52t1vL9vctZFUfoKz4R4huk7cEREnhSR0yLyfRH5VHSc7ckJ2QMM8+ehDeD3VfUuAO8H8Nsi8k6wPTkhe4Jhmo9cBLDdgXhTRE4DOIxrbE/eFYE4k17cFloj5RKLxG+D2Tl+my7v+hh2nAme7sYTejLuvMV12xZ8bcXKfd12YKovHznsKiC77UkfuDgx5foKluL1BNLiknjcs/oGIClXHyCVdee37B2IazOuTsZO0159tKNzl5yOz6ZYbZgMZleOoogcBXA3gKcxZHty35p8a4tNMQn5SWNoJSAi4wD+AsCnVXXjaudv41uTj48Xrn4BISRRhrIRRSSLngL4sqr+ZXR41+3JRQS5rMRKinVdvr26JhrqkomyuXhC0DYNV41Xfe9Cjeu2rFt9zxXM9H51xZKApsedC+Daovt6AC1nqu8/bOugPtlp53k++SlXstedabv+iC56sF2zZ0qX7D2lnVuiO3YHMllzlzptcw18QlaqE3eRCNlmmN0BAfBFAKdV9U/ct9ienJA9wDCWwH0A/gOAfxWR56Nj/xG9duRfj1qV/xjAr41mioSQUTLM7sB3gB32p7Gr9uQZAfZlMkhnXHBNxtYJOi5xp+ly4cW12/aJN+2mydlcyR2PJ9h0XdXd2996S18+c8r6EkrG3IGOe9x63cZoOJN6wpUpy0q8nlnbuTWFtGsb7oKW8m68ZsqOT7jSYVl3LbyZ340nLLVc45X1hi2+1tru/bSZQEQGwzAyQgKHSoCQwEk0gkQgyEka9bqV+CqNWZz+THmfnevKiG26ugG2ng8Us+YmpOFcBon33au4hfEjb7VwhtsOz/Tl8pjVHPDlwlquXFfR5RfsmzV3YKxo1wJAzq3kF1ySQatpZnytYw1H6jYEpibsHVS37Gmry1fsnmOWzwDEexZ2mr6Ss+n4SqMCQgZBS4CQwKESICRwEg4oVyg6aLQsfr9ZMdO97lazp91qv19hh9tZ6PgyYm7FPJ2P6zZt2nmtst3r8BEzvdsr5gJsbZm7UqvaavvEtK3oTx6wYKHOjtX6yuv0H1T3Yb1i960696hed/dy5cjg+jS2s/HnW6+7HAOX1typ2X0zGk93JmQbWgKEBA6VACGB86bkl5YLtrLuTWlfNVfh8gtc5ZxS3sziypateKecy5Daods6bXM/bp+d7ctnpi705X96/nRfnp60scsl55bMWGDTuuvzl8/Em4801NyadRe4lPaVkFw6cMe5OJsbtmuQdrsfPv261dgRu9Wx8fIub6LkmrDU02w+QgZDS4CQwKESICRwknUHRJBKZ9FomPk8M26BNtmsmdtry2YWN9xqe27cXImpsgUaFTJm+m5UfUgR0N5yPfnUHvmOuYN9ufYeC7ipqK2qr75m1YfqNXMBWh1zRcaaNg8AmCjY58WGBfnUXB5CN+XcHbch4GqZounM/KwPQCrFU6t9i/VK00o9+GYnVde+nBAPLQFCAodKgJDASbzvQKupKPhimDmzfzeb1gdgpe1M+rydX2uZef7aK8t23CUIbNXi7sDWit338K37+3LaFTC9bc52DSopu371ko23uW5uwoRzP/I7gneqTdcjsevSh93uR6fg3AFXbHVrzYKItG5yetzOX2la7wQg3ua84/utu6pG+SxLu5HB0BIgJHCoBAgJHCoBQgIn0TWBVCqF4ngezYpt2V1wW4GStek0OubPHp6yrbwtl3z/N09+py9fvGR+f1vjCT1wZcvednipL982a3n5B6YP9eX9ZWtEMnfY1gqWls0X77ikpOyMRRUCQLtmc8kUXGOSjM2rkLNtRJf2jwlXbm3LJQA1GrZdmN6hu3214VzKxiumXXRlYwuEDIKWACGBQyVASOAk6g50tIv1Vh1rriX4ubMX+3K15ra6XL5Lu2rRci+ds6SfM2csGq/ecjn1iFfW9f35XnzVrlnZMBP73YdtK69YdfK4mdSFmsmrq2aq77/Nth2BeAJToW2vWGHPkXaJPhnfOCXrtkNdnYG0q52Q39FXsOnKixXcdqXzrtBtsdowGcwwzUcKIvJdEXkhak3+R9Hx20Xk6ag1+ddEhFUrCLkJGcYdaAD4kKq+F8BxAB8WkfcD+GMA/yNqTb4K4MHRTZMQMiqGaT6iALbt92z0TwF8CMAnouOPAPivAD7/hjfrAp1aF1nXbOPsuZW+fH7JknJ8ksypH1lk4Nq6naNu+hn/JN147nzaVfz1fQN9cJ1fuS9NWVJTO2UnjcNW7q+sW6LO3Ea8km9DLNIv7+oi1FwDkFrbJSAV3O6Cn3rHdlGmSja2uucBgIxzLcaLdq8a3DOlyiBkEEMtDIpIOmpBtgjgcQCvAFhT7e/FLQA4PJopEkJGyVBKQFU7qnocwByAewHcNei0QdeKyEMiclJETm5s1gadQgh5E9nV7oCqronIUwDeD2BKRDKRNTAH4MLrXHMCwAkAuOOOAwpRTJTNZH3vXW/py8fvMZPX5drgn//lh315rGRVflXNdq66SsWdeLUv1Kuuj5/rS+gX2YvO/Zg+aKbz2pbtZBSmvBnu3IpqfMCKmqm/0bEV/k7Kxui61f5c2+aUcyXI8jl7Hy3XgEUkrrvLrhmJit23UrFEqHaHuwNkMMPsDsyKyFQkFwH8LIDTAJ4E8KvRaWxNTshNyjCWwCEAj4hIGj2l8XVVfUxETgH4qoj8NwDPAfjiCOdJCBkRopqcmTg/P68nT55MbDxCQkNEnlXV+d1cw7BhQgKHSoCQwKESICRwqAQICRwqAUICh0qAkMChEiAkcKgECAkcKgFCAodKgJDAoRIgJHCoBAgJHCoBQgKHSoCQwKESICRwqAQICRwqAUICh0qAkMChEiAkcKgECAkcKgFCAodKgJDAGVoJRP0InxORx6LPbE1OyB5gN5bAp9DrPLQNW5MTsgcYtivxHIBfAvCF6LOg15r80eiURwD8yigmSAgZLcNaAp8D8AcAutHnfWBrckL2BMM0JP1lAIuq+qw/PODUq7YmX1pausZpEkJGxTCWwH0APiIi5wB8FT034HOIWpNH57xha3JVnVfV+dnZ2RswZULIjeSqSkBVP6Oqc6p6FMDHAHxbVX8dbE1OyJ7geuIE/hDA74nIy+itEbA1OSE3IZmrn2Ko6lMAnorkswDuvfFTIoQkCSMGCQkcKgFCAodKgJDAoRIgJHCoBAgJHCoBQgKHSoCQwKESICRwqAQICRwqAUICh0qAkMChEiAkcKgECAkcKgFCAodKgJDAoRIgJHCoBAgJHCoBQgKHSoCQwKESICRwqAQICRwqAUICh0qAkMAZqu9A1IJsE0AHQFtV50VkBsDXABwFcA7Av1fV1dFMkxAyKnZjCfw7VT2uqvPR54cBPKGqxwA8EX0mhNxkXI878FEAj0TyIwB+5fqnQwhJmmGVgAL4exF5VkQeio4dVNWLABB9PTDoQrYmJ+Qnm2F7Ed6nqhdE5ACAx0XkB8MOoKonAJwAgPn5eb2GORJCRshQloCqXoi+LgL4BnqNSC+LyCEAiL4ujmqShJDRcVUlICJjIlLelgH8HIAXAXwLwCej0z4J4JujmiQhZHQM4w4cBPANEdk+/89U9W9F5BkAXxeRBwH8GMCvjW6ahJBRcVUloKpnAbx3wPFlAPePYlKEkORgxCAhgUMlQEjgUAkQEjhUAoQEDpUAIYFDJUBI4FAJEBI4VAKEBA6VACGBQyVASOBQCRASOFQChAQOlQAhgUMlQEjgUAkQEjhUAoQEDpUAIYFDJUBI4FAJEBI4VAKEBA6VACGBQyVASOAMpQREZEpEHhWRH4jIaRH5gIjMiMjjInIm+jo96skSQm48w1oC/xPA36rqO9DrQXAabE1OyJ5gmDZkEwA+COCLAKCqTVVdA1uTE7InGMYSuAPAEoA/FZHnROQLUU9CtiYnZA8wjBLIALgHwOdV9W4AFezC9FfVE6o6r6rzs7Oz1zhNQsioGEYJLABYUNWno8+PoqcU2JqckD3AVZWAql4CcF5E7owO3Q/gFNianJA9wTCtyQHgdwF8WURyAM4C+A30FAhbkxNykzOUElDV5wHMD/gWW5MTcpPDiEFCAodKgJDAoRIgJHCoBAgJHCoBQgKHSoCQwKESICRwqAQICRwqAUICh0qAkMChEiAkcKgECAkcKgFCAodKgJDAoRIgJHCoBAgJHCoBQgKHSoCQwKESICRwqAQICRwqAUICh0qAkMAZpiHpnSLyvPu3ISKfZmtyQvYGw3QgeklVj6vqcQDvA1AF8A2wNTkhe4LdugP3A3hFVV8FW5MTsicYtg3ZNh8D8JVIjrUmF5HXbU0O4KHoY0NEXrymmV4/+wFc4dgce4+PfefVT4kjqjrcib0+hBcAvEtVL4vImqpOue+vquobrguIyElVHdTObORwbI7NsQezG3fgFwB8T1UvR5/ZmpyQPcBulMDHYa4AwNbkhOwJhlICIlIC8ACAv3SHPwvgARE5E33vs0Pc6sSuZ3jj4Ngcm2MPYOg1AULI3oQRg4QEDpUAIYGTiBIQkQ+LyEsi8rKIjDyyUES+JCKLPiYhiTBnETkiIk+KyGkR+b6IfCrBsQsi8l0ReSEa+4+i47eLyNPR2F+LtnpHgoikReQ5EXnsTRj7nIj8axTafjI6lkhou4hMicijIvKD6Gf/gYR+5jcmpF9VR/oPQBrAKwDuAJAD8AKAd454zA8CuAfAi+7YfwfwcCQ/DOCPRzDuIQD3RHIZwA8BvDOhsQXAeCRnATwN4P0Avg7gY9Hx/wXgt0b43n8PwJ8BeCz6nOTY5wDs33Fs5O89uvcjAH4zknMAppIa280hDeASgNt2O/bIJuUm9wEAf+c+fwbAZxIY9+gOJfASgEORfAjASwnM4Zvo7ZwkOjaAEoDvAfhp9CLXMoN+Fjd4zDn0ckg+BOCxSCklMnZ0/0FKYOTvHcAEgB8hWmR/s37fAPwcgH+6lrGTcAcOAzjvPi9Ex5ImFuYMYGCY841CRI4CuBu9v8iJjB2Z48+jF7j1OHoW2JqqtqNTRvnuPwfgDwB0o8/7EhwbABTA34vIs1GoOpDMe78DwBKAP41coS+IyFhCY3teN6T/amMnoQRkwLE9vS8pIuMA/gLAp1V1I6lxVbWjvWzPOQD3Arhr0Gk3elwR+WUAi6r6rD+cxNiO+1T1HvQiW39bRD44wrE8GfRcz8+r6t0AKkg4ozZaa/kIgD+/luuTUAILAI64z3Po5SAkTSJhziKSRU8BfFlVt4OrEg2xVtU1AE+htyYwJSLbiWKjevf3AfiIiJwD8FX0XILPJTQ2AEBVL0RfF9FLdb8Xybz3BQALqvp09PlR9JRCkj/z6wrpT0IJPAPgWLRSnEPPbPlWAuPuZORhziIiAL4I4LSq/knCY8+KyFQkFwH8LIDTAJ4E8KujHFtVP6Oqc6p6FL2f77dV9deTGBsARGRMRMrbMnr+8YtI4L2r6iUA50VkO3vvfgCnkhjbcX0h/aNcrHCLFr+I3kr5KwD+UwLjfQXARQAt9DT1g+j5qE8AOBN9nRnBuP8GPZP3/wJ4Pvr3iwmN/R4Az0Vjvwjgv0TH7wDwXQAvo2cu5kf87n8GtjuQyNjROC9E/76//TuWxHuPxjkO4GT07v8KwHSCY5cALAOYdMd2NTbDhgkJHEYMEhI4VAKEBA6VACGBQyVASOBQCRASOFQChAQOlQAhgfP/AD8ytgTzV9L7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c142ace10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[20]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[20]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a00ebd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[291]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[21]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-42-0348c57876c8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-0348c57876c8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    single_img=x_test[]\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "single_img=x_test[]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[22]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nChyTAFHg/h+P75jZXpcY0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb22a11ef50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_img=x_test[23]\n",
    "#single_img_reshaped = single_img.reshape(32,32,3)\n",
    "single_img_reshaped1 = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "plt.imshow(single_img_reshaped1,clim=(0.0, 0.5))\n",
    "plt.axis([0,70,70,0])\n",
    "print name(pred[23]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3025945583820104\n",
      "epochs 0 / 2500: loss 2.302595 : training accuracy 0.104500, and val accuracy 0.104500\n",
      "epochs 1 / 2500: loss 2.302542 : training accuracy 0.102900, and val accuracy 0.102900\n",
      "epochs 2 / 2500: loss 2.302480 : training accuracy 0.102700, and val accuracy 0.102700\n",
      "epochs 3 / 2500: loss 2.302394 : training accuracy 0.102700, and val accuracy 0.102700\n",
      "epochs 4 / 2500: loss 2.302263 : training accuracy 0.105400, and val accuracy 0.105400\n",
      "epochs 5 / 2500: loss 2.302059 : training accuracy 0.116900, and val accuracy 0.116900\n",
      "epochs 6 / 2500: loss 2.301742 : training accuracy 0.129100, and val accuracy 0.129100\n",
      "epochs 7 / 2500: loss 2.301246 : training accuracy 0.122800, and val accuracy 0.122800\n",
      "epochs 8 / 2500: loss 2.300488 : training accuracy 0.112400, and val accuracy 0.112400\n",
      "epochs 9 / 2500: loss 2.299365 : training accuracy 0.105500, and val accuracy 0.105500\n",
      "epochs 10 / 2500: loss 2.297786 : training accuracy 0.103000, and val accuracy 0.103000\n",
      "epochs 11 / 2500: loss 2.295704 : training accuracy 0.103500, and val accuracy 0.103500\n",
      "epochs 12 / 2500: loss 2.293115 : training accuracy 0.113200, and val accuracy 0.113200\n",
      "epochs 13 / 2500: loss 2.289986 : training accuracy 0.129800, and val accuracy 0.129800\n",
      "epochs 14 / 2500: loss 2.286157 : training accuracy 0.119900, and val accuracy 0.119900\n",
      "epochs 15 / 2500: loss 2.281375 : training accuracy 0.104700, and val accuracy 0.104700\n",
      "epochs 16 / 2500: loss 2.275506 : training accuracy 0.106900, and val accuracy 0.106900\n",
      "epochs 17 / 2500: loss 2.268625 : training accuracy 0.114000, and val accuracy 0.114000\n",
      "epochs 18 / 2500: loss 2.260861 : training accuracy 0.125800, and val accuracy 0.125800\n",
      "epochs 19 / 2500: loss 2.252226 : training accuracy 0.133200, and val accuracy 0.133200\n",
      "epochs 20 / 2500: loss 2.242539 : training accuracy 0.141400, and val accuracy 0.141400\n",
      "epochs 21 / 2500: loss 2.231489 : training accuracy 0.153000, and val accuracy 0.153000\n",
      "epochs 22 / 2500: loss 2.218804 : training accuracy 0.161500, and val accuracy 0.161500\n",
      "epochs 23 / 2500: loss 2.204602 : training accuracy 0.171100, and val accuracy 0.171100\n",
      "epochs 24 / 2500: loss 2.189544 : training accuracy 0.177400, and val accuracy 0.177400\n",
      "epochs 25 / 2500: loss 2.174550 : training accuracy 0.180600, and val accuracy 0.180600\n",
      "epochs 26 / 2500: loss 2.160411 : training accuracy 0.183500, and val accuracy 0.183500\n",
      "epochs 27 / 2500: loss 2.147615 : training accuracy 0.185900, and val accuracy 0.185900\n",
      "epochs 28 / 2500: loss 2.136340 : training accuracy 0.189100, and val accuracy 0.189100\n",
      "epochs 29 / 2500: loss 2.126529 : training accuracy 0.190800, and val accuracy 0.190800\n",
      "epochs 30 / 2500: loss 2.117976 : training accuracy 0.191900, and val accuracy 0.191900\n",
      "epochs 31 / 2500: loss 2.110454 : training accuracy 0.193600, and val accuracy 0.193600\n",
      "epochs 32 / 2500: loss 2.103735 : training accuracy 0.195000, and val accuracy 0.195000\n",
      "epochs 33 / 2500: loss 2.097612 : training accuracy 0.197400, and val accuracy 0.197400\n",
      "epochs 34 / 2500: loss 2.091925 : training accuracy 0.200400, and val accuracy 0.200400\n",
      "epochs 35 / 2500: loss 2.086537 : training accuracy 0.205600, and val accuracy 0.205600\n",
      "epochs 36 / 2500: loss 2.081350 : training accuracy 0.212100, and val accuracy 0.212100\n",
      "epochs 37 / 2500: loss 2.076279 : training accuracy 0.216000, and val accuracy 0.216000\n",
      "epochs 38 / 2500: loss 2.071264 : training accuracy 0.223300, and val accuracy 0.223300\n",
      "epochs 39 / 2500: loss 2.066261 : training accuracy 0.230200, and val accuracy 0.230200\n",
      "epochs 40 / 2500: loss 2.061242 : training accuracy 0.234600, and val accuracy 0.234600\n",
      "epochs 41 / 2500: loss 2.056194 : training accuracy 0.239000, and val accuracy 0.239000\n",
      "epochs 42 / 2500: loss 2.051114 : training accuracy 0.241900, and val accuracy 0.241900\n",
      "epochs 43 / 2500: loss 2.046013 : training accuracy 0.246600, and val accuracy 0.246600\n",
      "epochs 44 / 2500: loss 2.040919 : training accuracy 0.250400, and val accuracy 0.250400\n",
      "epochs 45 / 2500: loss 2.035860 : training accuracy 0.252600, and val accuracy 0.252600\n",
      "epochs 46 / 2500: loss 2.030864 : training accuracy 0.253600, and val accuracy 0.253600\n",
      "epochs 47 / 2500: loss 2.025958 : training accuracy 0.254600, and val accuracy 0.254600\n",
      "epochs 48 / 2500: loss 2.021164 : training accuracy 0.255100, and val accuracy 0.255100\n",
      "epochs 49 / 2500: loss 2.016498 : training accuracy 0.255200, and val accuracy 0.255200\n",
      "epochs 50 / 2500: loss 2.011968 : training accuracy 0.255900, and val accuracy 0.255900\n",
      "epochs 51 / 2500: loss 2.007570 : training accuracy 0.258300, and val accuracy 0.258300\n",
      "epochs 52 / 2500: loss 2.003298 : training accuracy 0.260300, and val accuracy 0.260300\n",
      "epochs 53 / 2500: loss 1.999139 : training accuracy 0.264100, and val accuracy 0.264100\n",
      "epochs 54 / 2500: loss 1.995078 : training accuracy 0.265100, and val accuracy 0.265100\n",
      "epochs 55 / 2500: loss 1.991106 : training accuracy 0.266700, and val accuracy 0.266700\n",
      "epochs 56 / 2500: loss 1.987210 : training accuracy 0.269400, and val accuracy 0.269400\n",
      "epochs 57 / 2500: loss 1.983384 : training accuracy 0.271100, and val accuracy 0.271100\n",
      "epochs 58 / 2500: loss 1.979618 : training accuracy 0.273200, and val accuracy 0.273200\n",
      "epochs 59 / 2500: loss 1.975898 : training accuracy 0.274900, and val accuracy 0.274900\n",
      "epochs 60 / 2500: loss 1.972228 : training accuracy 0.277000, and val accuracy 0.277000\n",
      "epochs 61 / 2500: loss 1.968610 : training accuracy 0.278000, and val accuracy 0.278000\n",
      "epochs 62 / 2500: loss 1.965045 : training accuracy 0.280500, and val accuracy 0.280500\n",
      "epochs 63 / 2500: loss 1.961542 : training accuracy 0.281900, and val accuracy 0.281900\n",
      "epochs 64 / 2500: loss 1.958096 : training accuracy 0.283100, and val accuracy 0.283100\n",
      "epochs 65 / 2500: loss 1.954714 : training accuracy 0.284200, and val accuracy 0.284200\n",
      "epochs 66 / 2500: loss 1.951397 : training accuracy 0.287800, and val accuracy 0.287800\n",
      "epochs 67 / 2500: loss 1.948146 : training accuracy 0.289300, and val accuracy 0.289300\n",
      "epochs 68 / 2500: loss 1.944972 : training accuracy 0.291800, and val accuracy 0.291800\n",
      "epochs 69 / 2500: loss 1.941879 : training accuracy 0.292800, and val accuracy 0.292800\n",
      "epochs 70 / 2500: loss 1.938865 : training accuracy 0.294800, and val accuracy 0.294800\n",
      "epochs 71 / 2500: loss 1.935930 : training accuracy 0.295600, and val accuracy 0.295600\n",
      "epochs 72 / 2500: loss 1.933068 : training accuracy 0.297400, and val accuracy 0.297400\n",
      "epochs 73 / 2500: loss 1.930279 : training accuracy 0.298900, and val accuracy 0.298900\n",
      "epochs 74 / 2500: loss 1.927556 : training accuracy 0.299600, and val accuracy 0.299600\n",
      "epochs 75 / 2500: loss 1.924899 : training accuracy 0.301100, and val accuracy 0.301100\n",
      "epochs 76 / 2500: loss 1.922300 : training accuracy 0.302900, and val accuracy 0.302900\n",
      "epochs 77 / 2500: loss 1.919752 : training accuracy 0.304600, and val accuracy 0.304600\n",
      "epochs 78 / 2500: loss 1.917252 : training accuracy 0.305600, and val accuracy 0.305600\n",
      "epochs 79 / 2500: loss 1.914797 : training accuracy 0.306600, and val accuracy 0.306600\n",
      "epochs 80 / 2500: loss 1.912379 : training accuracy 0.308000, and val accuracy 0.308000\n",
      "epochs 81 / 2500: loss 1.909992 : training accuracy 0.309000, and val accuracy 0.309000\n",
      "epochs 82 / 2500: loss 1.907627 : training accuracy 0.310400, and val accuracy 0.310400\n",
      "epochs 83 / 2500: loss 1.905283 : training accuracy 0.312100, and val accuracy 0.312100\n",
      "epochs 84 / 2500: loss 1.902953 : training accuracy 0.312900, and val accuracy 0.312900\n",
      "epochs 85 / 2500: loss 1.900635 : training accuracy 0.313700, and val accuracy 0.313700\n",
      "epochs 86 / 2500: loss 1.898333 : training accuracy 0.314900, and val accuracy 0.314900\n",
      "epochs 87 / 2500: loss 1.896038 : training accuracy 0.316500, and val accuracy 0.316500\n",
      "epochs 88 / 2500: loss 1.893747 : training accuracy 0.316800, and val accuracy 0.316800\n",
      "epochs 89 / 2500: loss 1.891461 : training accuracy 0.316700, and val accuracy 0.316700\n",
      "epochs 90 / 2500: loss 1.889173 : training accuracy 0.317500, and val accuracy 0.317500\n",
      "epochs 91 / 2500: loss 1.886886 : training accuracy 0.318300, and val accuracy 0.318300\n",
      "epochs 92 / 2500: loss 1.884598 : training accuracy 0.319100, and val accuracy 0.319100\n",
      "epochs 93 / 2500: loss 1.882308 : training accuracy 0.320000, and val accuracy 0.320000\n",
      "epochs 94 / 2500: loss 1.880017 : training accuracy 0.321200, and val accuracy 0.321200\n",
      "epochs 95 / 2500: loss 1.877725 : training accuracy 0.323000, and val accuracy 0.323000\n",
      "epochs 96 / 2500: loss 1.875425 : training accuracy 0.324200, and val accuracy 0.324200\n",
      "epochs 97 / 2500: loss 1.873119 : training accuracy 0.325800, and val accuracy 0.325800\n",
      "epochs 98 / 2500: loss 1.870810 : training accuracy 0.326800, and val accuracy 0.326800\n",
      "epochs 99 / 2500: loss 1.868495 : training accuracy 0.327800, and val accuracy 0.327800\n",
      "epochs 100 / 2500: loss 1.866182 : training accuracy 0.328800, and val accuracy 0.328800\n",
      "epochs 101 / 2500: loss 1.863868 : training accuracy 0.329600, and val accuracy 0.329600\n",
      "epochs 102 / 2500: loss 1.861555 : training accuracy 0.331200, and val accuracy 0.331200\n",
      "epochs 103 / 2500: loss 1.859243 : training accuracy 0.333200, and val accuracy 0.333200\n",
      "epochs 104 / 2500: loss 1.856937 : training accuracy 0.334300, and val accuracy 0.334300\n",
      "epochs 105 / 2500: loss 1.854633 : training accuracy 0.335300, and val accuracy 0.335300\n",
      "epochs 106 / 2500: loss 1.852336 : training accuracy 0.336900, and val accuracy 0.336900\n",
      "epochs 107 / 2500: loss 1.850051 : training accuracy 0.336900, and val accuracy 0.336900\n",
      "epochs 108 / 2500: loss 1.847771 : training accuracy 0.338600, and val accuracy 0.338600\n",
      "epochs 109 / 2500: loss 1.845501 : training accuracy 0.340200, and val accuracy 0.340200\n",
      "epochs 110 / 2500: loss 1.843245 : training accuracy 0.340000, and val accuracy 0.340000\n",
      "epochs 111 / 2500: loss 1.841002 : training accuracy 0.341300, and val accuracy 0.341300\n",
      "epochs 112 / 2500: loss 1.838781 : training accuracy 0.342300, and val accuracy 0.342300\n",
      "epochs 113 / 2500: loss 1.836570 : training accuracy 0.343400, and val accuracy 0.343400\n",
      "epochs 114 / 2500: loss 1.834374 : training accuracy 0.345000, and val accuracy 0.345000\n",
      "epochs 115 / 2500: loss 1.832196 : training accuracy 0.345900, and val accuracy 0.345900\n",
      "epochs 116 / 2500: loss 1.830032 : training accuracy 0.346700, and val accuracy 0.346700\n",
      "epochs 117 / 2500: loss 1.827888 : training accuracy 0.347600, and val accuracy 0.347600\n",
      "epochs 118 / 2500: loss 1.825764 : training accuracy 0.349500, and val accuracy 0.349500\n",
      "epochs 119 / 2500: loss 1.823662 : training accuracy 0.351700, and val accuracy 0.351700\n",
      "epochs 120 / 2500: loss 1.821574 : training accuracy 0.352300, and val accuracy 0.352300\n",
      "epochs 121 / 2500: loss 1.819493 : training accuracy 0.352600, and val accuracy 0.352600\n",
      "epochs 122 / 2500: loss 1.817427 : training accuracy 0.353100, and val accuracy 0.353100\n",
      "epochs 123 / 2500: loss 1.815392 : training accuracy 0.353800, and val accuracy 0.353800\n",
      "epochs 124 / 2500: loss 1.813376 : training accuracy 0.355600, and val accuracy 0.355600\n",
      "epochs 125 / 2500: loss 1.811386 : training accuracy 0.355000, and val accuracy 0.355000\n",
      "epochs 126 / 2500: loss 1.809421 : training accuracy 0.355400, and val accuracy 0.355400\n",
      "epochs 127 / 2500: loss 1.807481 : training accuracy 0.356600, and val accuracy 0.356600\n",
      "epochs 128 / 2500: loss 1.805562 : training accuracy 0.357700, and val accuracy 0.357700\n",
      "epochs 129 / 2500: loss 1.803669 : training accuracy 0.357800, and val accuracy 0.357800\n",
      "epochs 130 / 2500: loss 1.801799 : training accuracy 0.359200, and val accuracy 0.359200\n",
      "epochs 131 / 2500: loss 1.799948 : training accuracy 0.359700, and val accuracy 0.359700\n",
      "epochs 132 / 2500: loss 1.798113 : training accuracy 0.360100, and val accuracy 0.360100\n",
      "epochs 133 / 2500: loss 1.796296 : training accuracy 0.359600, and val accuracy 0.359600\n",
      "epochs 134 / 2500: loss 1.794496 : training accuracy 0.359700, and val accuracy 0.359700\n",
      "epochs 135 / 2500: loss 1.792708 : training accuracy 0.360100, and val accuracy 0.360100\n",
      "epochs 136 / 2500: loss 1.790936 : training accuracy 0.361700, and val accuracy 0.361700\n",
      "epochs 137 / 2500: loss 1.789177 : training accuracy 0.361700, and val accuracy 0.361700\n",
      "epochs 138 / 2500: loss 1.787429 : training accuracy 0.362100, and val accuracy 0.362100\n",
      "epochs 139 / 2500: loss 1.785695 : training accuracy 0.363200, and val accuracy 0.363200\n",
      "epochs 140 / 2500: loss 1.783978 : training accuracy 0.363400, and val accuracy 0.363400\n",
      "epochs 141 / 2500: loss 1.782295 : training accuracy 0.363400, and val accuracy 0.363400\n",
      "epochs 142 / 2500: loss 1.780687 : training accuracy 0.363600, and val accuracy 0.363600\n",
      "epochs 143 / 2500: loss 1.779295 : training accuracy 0.363800, and val accuracy 0.363800\n",
      "epochs 144 / 2500: loss 1.778740 : training accuracy 0.364400, and val accuracy 0.364400\n",
      "epochs 145 / 2500: loss 1.781076 : training accuracy 0.362700, and val accuracy 0.362700\n",
      "epochs 146 / 2500: loss 1.789284 : training accuracy 0.358300, and val accuracy 0.358300\n",
      "epochs 147 / 2500: loss 1.794996 : training accuracy 0.355500, and val accuracy 0.355500\n",
      "epochs 148 / 2500: loss 1.790247 : training accuracy 0.357000, and val accuracy 0.357000\n",
      "epochs 149 / 2500: loss 1.784369 : training accuracy 0.359800, and val accuracy 0.359800\n",
      "epochs 150 / 2500: loss 1.779942 : training accuracy 0.359800, and val accuracy 0.359800\n",
      "epochs 151 / 2500: loss 1.776270 : training accuracy 0.362100, and val accuracy 0.362100\n",
      "epochs 152 / 2500: loss 1.773244 : training accuracy 0.364800, and val accuracy 0.364800\n",
      "epochs 153 / 2500: loss 1.770719 : training accuracy 0.365800, and val accuracy 0.365800\n",
      "epochs 154 / 2500: loss 1.768538 : training accuracy 0.367300, and val accuracy 0.367300\n",
      "epochs 155 / 2500: loss 1.766352 : training accuracy 0.367200, and val accuracy 0.367200\n",
      "epochs 156 / 2500: loss 1.764103 : training accuracy 0.367400, and val accuracy 0.367400\n",
      "epochs 157 / 2500: loss 1.762167 : training accuracy 0.368000, and val accuracy 0.368000\n",
      "epochs 158 / 2500: loss 1.760519 : training accuracy 0.367900, and val accuracy 0.367900\n",
      "epochs 159 / 2500: loss 1.759038 : training accuracy 0.369000, and val accuracy 0.369000\n",
      "epochs 160 / 2500: loss 1.757603 : training accuracy 0.370200, and val accuracy 0.370200\n",
      "epochs 161 / 2500: loss 1.756253 : training accuracy 0.370700, and val accuracy 0.370700\n",
      "epochs 162 / 2500: loss 1.754752 : training accuracy 0.371100, and val accuracy 0.371100\n",
      "epochs 163 / 2500: loss 1.753510 : training accuracy 0.370700, and val accuracy 0.370700\n",
      "epochs 164 / 2500: loss 1.752467 : training accuracy 0.370800, and val accuracy 0.370800\n",
      "epochs 165 / 2500: loss 1.751395 : training accuracy 0.370600, and val accuracy 0.370600\n",
      "epochs 166 / 2500: loss 1.750161 : training accuracy 0.371700, and val accuracy 0.371700\n",
      "epochs 167 / 2500: loss 1.748850 : training accuracy 0.371400, and val accuracy 0.371400\n",
      "epochs 168 / 2500: loss 1.747370 : training accuracy 0.372400, and val accuracy 0.372400\n",
      "epochs 169 / 2500: loss 1.745539 : training accuracy 0.373000, and val accuracy 0.373000\n",
      "epochs 170 / 2500: loss 1.743827 : training accuracy 0.374000, and val accuracy 0.374000\n",
      "epochs 171 / 2500: loss 1.742007 : training accuracy 0.374800, and val accuracy 0.374800\n",
      "epochs 172 / 2500: loss 1.740274 : training accuracy 0.375400, and val accuracy 0.375400\n",
      "epochs 173 / 2500: loss 1.738941 : training accuracy 0.376300, and val accuracy 0.376300\n",
      "epochs 174 / 2500: loss 1.738031 : training accuracy 0.376900, and val accuracy 0.376900\n",
      "epochs 175 / 2500: loss 1.736722 : training accuracy 0.378000, and val accuracy 0.378000\n",
      "epochs 176 / 2500: loss 1.735045 : training accuracy 0.378600, and val accuracy 0.378600\n",
      "epochs 177 / 2500: loss 1.733396 : training accuracy 0.379800, and val accuracy 0.379800\n",
      "epochs 178 / 2500: loss 1.731380 : training accuracy 0.379900, and val accuracy 0.379900\n",
      "epochs 179 / 2500: loss 1.729364 : training accuracy 0.381200, and val accuracy 0.381200\n",
      "epochs 180 / 2500: loss 1.727514 : training accuracy 0.382100, and val accuracy 0.382100\n",
      "epochs 181 / 2500: loss 1.725899 : training accuracy 0.382900, and val accuracy 0.382900\n",
      "epochs 182 / 2500: loss 1.724237 : training accuracy 0.384400, and val accuracy 0.384400\n",
      "epochs 183 / 2500: loss 1.722720 : training accuracy 0.385500, and val accuracy 0.385500\n",
      "epochs 184 / 2500: loss 1.721263 : training accuracy 0.385600, and val accuracy 0.385600\n",
      "epochs 185 / 2500: loss 1.719707 : training accuracy 0.386300, and val accuracy 0.386300\n",
      "epochs 186 / 2500: loss 1.717784 : training accuracy 0.386500, and val accuracy 0.386500\n",
      "epochs 187 / 2500: loss 1.715662 : training accuracy 0.387100, and val accuracy 0.387100\n",
      "epochs 188 / 2500: loss 1.713881 : training accuracy 0.387900, and val accuracy 0.387900\n",
      "epochs 189 / 2500: loss 1.712569 : training accuracy 0.388600, and val accuracy 0.388600\n",
      "epochs 190 / 2500: loss 1.711249 : training accuracy 0.388700, and val accuracy 0.388700\n",
      "epochs 191 / 2500: loss 1.709807 : training accuracy 0.389600, and val accuracy 0.389600\n",
      "epochs 192 / 2500: loss 1.708601 : training accuracy 0.390100, and val accuracy 0.390100\n",
      "epochs 193 / 2500: loss 1.707424 : training accuracy 0.390400, and val accuracy 0.390400\n",
      "epochs 194 / 2500: loss 1.705971 : training accuracy 0.391600, and val accuracy 0.391600\n",
      "epochs 195 / 2500: loss 1.704285 : training accuracy 0.393000, and val accuracy 0.393000\n",
      "epochs 196 / 2500: loss 1.702809 : training accuracy 0.393300, and val accuracy 0.393300\n",
      "epochs 197 / 2500: loss 1.701315 : training accuracy 0.393600, and val accuracy 0.393600\n",
      "epochs 198 / 2500: loss 1.699673 : training accuracy 0.395100, and val accuracy 0.395100\n",
      "epochs 199 / 2500: loss 1.697943 : training accuracy 0.396200, and val accuracy 0.396200\n",
      "epochs 200 / 2500: loss 1.696327 : training accuracy 0.398300, and val accuracy 0.398300\n",
      "epochs 201 / 2500: loss 1.695393 : training accuracy 0.399200, and val accuracy 0.399200\n",
      "epochs 202 / 2500: loss 1.695316 : training accuracy 0.400500, and val accuracy 0.400500\n",
      "epochs 203 / 2500: loss 1.696457 : training accuracy 0.398700, and val accuracy 0.398700\n",
      "epochs 204 / 2500: loss 1.698824 : training accuracy 0.397200, and val accuracy 0.397200\n",
      "epochs 205 / 2500: loss 1.701373 : training accuracy 0.395700, and val accuracy 0.395700\n",
      "epochs 206 / 2500: loss 1.700725 : training accuracy 0.397500, and val accuracy 0.397500\n",
      "epochs 207 / 2500: loss 1.696492 : training accuracy 0.399700, and val accuracy 0.399700\n",
      "epochs 208 / 2500: loss 1.692503 : training accuracy 0.401600, and val accuracy 0.401600\n",
      "epochs 209 / 2500: loss 1.689638 : training accuracy 0.404200, and val accuracy 0.404200\n",
      "epochs 210 / 2500: loss 1.687740 : training accuracy 0.404500, and val accuracy 0.404500\n",
      "epochs 211 / 2500: loss 1.685706 : training accuracy 0.404900, and val accuracy 0.404900\n",
      "epochs 212 / 2500: loss 1.683369 : training accuracy 0.406000, and val accuracy 0.406000\n",
      "epochs 213 / 2500: loss 1.681174 : training accuracy 0.406600, and val accuracy 0.406600\n",
      "epochs 214 / 2500: loss 1.679152 : training accuracy 0.407800, and val accuracy 0.407800\n",
      "epochs 215 / 2500: loss 1.677101 : training accuracy 0.408000, and val accuracy 0.408000\n",
      "epochs 216 / 2500: loss 1.675288 : training accuracy 0.408700, and val accuracy 0.408700\n",
      "epochs 217 / 2500: loss 1.674143 : training accuracy 0.407300, and val accuracy 0.407300\n",
      "epochs 218 / 2500: loss 1.674838 : training accuracy 0.405300, and val accuracy 0.405300\n",
      "epochs 219 / 2500: loss 1.682219 : training accuracy 0.404600, and val accuracy 0.404600\n",
      "epochs 220 / 2500: loss 1.698248 : training accuracy 0.392500, and val accuracy 0.392500\n",
      "epochs 221 / 2500: loss 1.694129 : training accuracy 0.393400, and val accuracy 0.393400\n",
      "epochs 222 / 2500: loss 1.680512 : training accuracy 0.400800, and val accuracy 0.400800\n",
      "epochs 223 / 2500: loss 1.671901 : training accuracy 0.404800, and val accuracy 0.404800\n",
      "epochs 224 / 2500: loss 1.667591 : training accuracy 0.407000, and val accuracy 0.407000\n",
      "epochs 225 / 2500: loss 1.665220 : training accuracy 0.409900, and val accuracy 0.409900\n",
      "epochs 226 / 2500: loss 1.664768 : training accuracy 0.410000, and val accuracy 0.410000\n",
      "epochs 227 / 2500: loss 1.665704 : training accuracy 0.411700, and val accuracy 0.411700\n",
      "epochs 228 / 2500: loss 1.665627 : training accuracy 0.411300, and val accuracy 0.411300\n",
      "epochs 229 / 2500: loss 1.663689 : training accuracy 0.411500, and val accuracy 0.411500\n",
      "epochs 230 / 2500: loss 1.660897 : training accuracy 0.412600, and val accuracy 0.412600\n",
      "epochs 231 / 2500: loss 1.658346 : training accuracy 0.413600, and val accuracy 0.413600\n",
      "epochs 232 / 2500: loss 1.656618 : training accuracy 0.414500, and val accuracy 0.414500\n",
      "epochs 233 / 2500: loss 1.656527 : training accuracy 0.412400, and val accuracy 0.412400\n",
      "epochs 234 / 2500: loss 1.658310 : training accuracy 0.408200, and val accuracy 0.408200\n",
      "epochs 235 / 2500: loss 1.661461 : training accuracy 0.404200, and val accuracy 0.404200\n",
      "epochs 236 / 2500: loss 1.663639 : training accuracy 0.403900, and val accuracy 0.403900\n",
      "epochs 237 / 2500: loss 1.660400 : training accuracy 0.406100, and val accuracy 0.406100\n",
      "epochs 238 / 2500: loss 1.654577 : training accuracy 0.407000, and val accuracy 0.407000\n",
      "epochs 239 / 2500: loss 1.651210 : training accuracy 0.408600, and val accuracy 0.408600\n",
      "epochs 240 / 2500: loss 1.648529 : training accuracy 0.410200, and val accuracy 0.410200\n",
      "epochs 241 / 2500: loss 1.646045 : training accuracy 0.410800, and val accuracy 0.410800\n",
      "epochs 242 / 2500: loss 1.643714 : training accuracy 0.412000, and val accuracy 0.412000\n",
      "epochs 243 / 2500: loss 1.641698 : training accuracy 0.413500, and val accuracy 0.413500\n",
      "epochs 244 / 2500: loss 1.640356 : training accuracy 0.414800, and val accuracy 0.414800\n",
      "epochs 245 / 2500: loss 1.640361 : training accuracy 0.414300, and val accuracy 0.414300\n",
      "epochs 246 / 2500: loss 1.644172 : training accuracy 0.411200, and val accuracy 0.411200\n",
      "epochs 247 / 2500: loss 1.663014 : training accuracy 0.404900, and val accuracy 0.404900\n",
      "epochs 248 / 2500: loss 1.668223 : training accuracy 0.407600, and val accuracy 0.407600\n",
      "epochs 249 / 2500: loss 1.646744 : training accuracy 0.417400, and val accuracy 0.417400\n",
      "epochs 250 / 2500: loss 1.639560 : training accuracy 0.419600, and val accuracy 0.419600\n",
      "epochs 251 / 2500: loss 1.637123 : training accuracy 0.421100, and val accuracy 0.421100\n",
      "epochs 252 / 2500: loss 1.636113 : training accuracy 0.421900, and val accuracy 0.421900\n",
      "epochs 253 / 2500: loss 1.635259 : training accuracy 0.421800, and val accuracy 0.421800\n",
      "epochs 254 / 2500: loss 1.634461 : training accuracy 0.420900, and val accuracy 0.420900\n",
      "epochs 255 / 2500: loss 1.634759 : training accuracy 0.420000, and val accuracy 0.420000\n",
      "epochs 256 / 2500: loss 1.636229 : training accuracy 0.417300, and val accuracy 0.417300\n",
      "epochs 257 / 2500: loss 1.634618 : training accuracy 0.418100, and val accuracy 0.418100\n",
      "epochs 258 / 2500: loss 1.629875 : training accuracy 0.420200, and val accuracy 0.420200\n",
      "epochs 259 / 2500: loss 1.628040 : training accuracy 0.418500, and val accuracy 0.418500\n",
      "epochs 260 / 2500: loss 1.630729 : training accuracy 0.415600, and val accuracy 0.415600\n",
      "epochs 261 / 2500: loss 1.628746 : training accuracy 0.420300, and val accuracy 0.420300\n",
      "epochs 262 / 2500: loss 1.625370 : training accuracy 0.421100, and val accuracy 0.421100\n",
      "epochs 263 / 2500: loss 1.623039 : training accuracy 0.423200, and val accuracy 0.423200\n",
      "epochs 264 / 2500: loss 1.621427 : training accuracy 0.425000, and val accuracy 0.425000\n",
      "epochs 265 / 2500: loss 1.619515 : training accuracy 0.426200, and val accuracy 0.426200\n",
      "epochs 266 / 2500: loss 1.618738 : training accuracy 0.428000, and val accuracy 0.428000\n",
      "epochs 267 / 2500: loss 1.618793 : training accuracy 0.427200, and val accuracy 0.427200\n",
      "epochs 268 / 2500: loss 1.625431 : training accuracy 0.425700, and val accuracy 0.425700\n",
      "epochs 269 / 2500: loss 1.647682 : training accuracy 0.413800, and val accuracy 0.413800\n",
      "epochs 270 / 2500: loss 1.623984 : training accuracy 0.428200, and val accuracy 0.428200\n",
      "epochs 271 / 2500: loss 1.611045 : training accuracy 0.431400, and val accuracy 0.431400\n",
      "epochs 272 / 2500: loss 1.610123 : training accuracy 0.431100, and val accuracy 0.431100\n",
      "epochs 273 / 2500: loss 1.612302 : training accuracy 0.431200, and val accuracy 0.431200\n",
      "epochs 274 / 2500: loss 1.611811 : training accuracy 0.431000, and val accuracy 0.431000\n",
      "epochs 275 / 2500: loss 1.609615 : training accuracy 0.431700, and val accuracy 0.431700\n",
      "epochs 276 / 2500: loss 1.607507 : training accuracy 0.432800, and val accuracy 0.432800\n",
      "epochs 277 / 2500: loss 1.607344 : training accuracy 0.434700, and val accuracy 0.434700\n",
      "epochs 278 / 2500: loss 1.609477 : training accuracy 0.432900, and val accuracy 0.432900\n",
      "epochs 279 / 2500: loss 1.608145 : training accuracy 0.432100, and val accuracy 0.432100\n",
      "epochs 280 / 2500: loss 1.606613 : training accuracy 0.431000, and val accuracy 0.431000\n",
      "epochs 281 / 2500: loss 1.606375 : training accuracy 0.429100, and val accuracy 0.429100\n",
      "epochs 282 / 2500: loss 1.616708 : training accuracy 0.422000, and val accuracy 0.422000\n",
      "epochs 283 / 2500: loss 1.613419 : training accuracy 0.425400, and val accuracy 0.425400\n",
      "epochs 284 / 2500: loss 1.599506 : training accuracy 0.432300, and val accuracy 0.432300\n",
      "epochs 285 / 2500: loss 1.595234 : training accuracy 0.434200, and val accuracy 0.434200\n",
      "epochs 286 / 2500: loss 1.594829 : training accuracy 0.434400, and val accuracy 0.434400\n",
      "epochs 287 / 2500: loss 1.594846 : training accuracy 0.435000, and val accuracy 0.435000\n",
      "epochs 288 / 2500: loss 1.593140 : training accuracy 0.434700, and val accuracy 0.434700\n",
      "epochs 289 / 2500: loss 1.591193 : training accuracy 0.435600, and val accuracy 0.435600\n",
      "epochs 290 / 2500: loss 1.589104 : training accuracy 0.437400, and val accuracy 0.437400\n",
      "epochs 291 / 2500: loss 1.587340 : training accuracy 0.438100, and val accuracy 0.438100\n",
      "epochs 292 / 2500: loss 1.585890 : training accuracy 0.436500, and val accuracy 0.436500\n",
      "epochs 293 / 2500: loss 1.590132 : training accuracy 0.436600, and val accuracy 0.436600\n",
      "epochs 294 / 2500: loss 1.598555 : training accuracy 0.436200, and val accuracy 0.436200\n",
      "epochs 295 / 2500: loss 1.595126 : training accuracy 0.439100, and val accuracy 0.439100\n",
      "epochs 296 / 2500: loss 1.590860 : training accuracy 0.440500, and val accuracy 0.440500\n",
      "epochs 297 / 2500: loss 1.588216 : training accuracy 0.442300, and val accuracy 0.442300\n",
      "epochs 298 / 2500: loss 1.585238 : training accuracy 0.443800, and val accuracy 0.443800\n",
      "epochs 299 / 2500: loss 1.582778 : training accuracy 0.443700, and val accuracy 0.443700\n",
      "epochs 300 / 2500: loss 1.581185 : training accuracy 0.441500, and val accuracy 0.441500\n",
      "epochs 301 / 2500: loss 1.583955 : training accuracy 0.438600, and val accuracy 0.438600\n",
      "epochs 302 / 2500: loss 1.595360 : training accuracy 0.438900, and val accuracy 0.438900\n",
      "epochs 303 / 2500: loss 1.586489 : training accuracy 0.443700, and val accuracy 0.443700\n",
      "epochs 304 / 2500: loss 1.580001 : training accuracy 0.446000, and val accuracy 0.446000\n",
      "epochs 305 / 2500: loss 1.577164 : training accuracy 0.447500, and val accuracy 0.447500\n",
      "epochs 306 / 2500: loss 1.575811 : training accuracy 0.447700, and val accuracy 0.447700\n",
      "epochs 307 / 2500: loss 1.574243 : training accuracy 0.446300, and val accuracy 0.446300\n",
      "epochs 308 / 2500: loss 1.573521 : training accuracy 0.443900, and val accuracy 0.443900\n",
      "epochs 309 / 2500: loss 1.573396 : training accuracy 0.440700, and val accuracy 0.440700\n",
      "epochs 310 / 2500: loss 1.569940 : training accuracy 0.441600, and val accuracy 0.441600\n",
      "epochs 311 / 2500: loss 1.567583 : training accuracy 0.441000, and val accuracy 0.441000\n",
      "epochs 312 / 2500: loss 1.565718 : training accuracy 0.443900, and val accuracy 0.443900\n",
      "epochs 313 / 2500: loss 1.565963 : training accuracy 0.443400, and val accuracy 0.443400\n",
      "epochs 314 / 2500: loss 1.575988 : training accuracy 0.436800, and val accuracy 0.436800\n",
      "epochs 315 / 2500: loss 1.574687 : training accuracy 0.437700, and val accuracy 0.437700\n",
      "epochs 316 / 2500: loss 1.565072 : training accuracy 0.443400, and val accuracy 0.443400\n",
      "epochs 317 / 2500: loss 1.561962 : training accuracy 0.444300, and val accuracy 0.444300\n",
      "epochs 318 / 2500: loss 1.561054 : training accuracy 0.444800, and val accuracy 0.444800\n",
      "epochs 319 / 2500: loss 1.559707 : training accuracy 0.445600, and val accuracy 0.445600\n",
      "epochs 320 / 2500: loss 1.558234 : training accuracy 0.446700, and val accuracy 0.446700\n",
      "epochs 321 / 2500: loss 1.555698 : training accuracy 0.447500, and val accuracy 0.447500\n",
      "epochs 322 / 2500: loss 1.553198 : training accuracy 0.448400, and val accuracy 0.448400\n",
      "epochs 323 / 2500: loss 1.550957 : training accuracy 0.450100, and val accuracy 0.450100\n",
      "epochs 324 / 2500: loss 1.549280 : training accuracy 0.450500, and val accuracy 0.450500\n",
      "epochs 325 / 2500: loss 1.548564 : training accuracy 0.450600, and val accuracy 0.450600\n",
      "epochs 326 / 2500: loss 1.548508 : training accuracy 0.450500, and val accuracy 0.450500\n",
      "epochs 327 / 2500: loss 1.548031 : training accuracy 0.450900, and val accuracy 0.450900\n",
      "epochs 328 / 2500: loss 1.561001 : training accuracy 0.451500, and val accuracy 0.451500\n",
      "epochs 329 / 2500: loss 1.563884 : training accuracy 0.451000, and val accuracy 0.451000\n",
      "epochs 330 / 2500: loss 1.556266 : training accuracy 0.453500, and val accuracy 0.453500\n",
      "epochs 331 / 2500: loss 1.553011 : training accuracy 0.454500, and val accuracy 0.454500\n",
      "epochs 332 / 2500: loss 1.551097 : training accuracy 0.456500, and val accuracy 0.456500\n",
      "epochs 333 / 2500: loss 1.550127 : training accuracy 0.456800, and val accuracy 0.456800\n",
      "epochs 334 / 2500: loss 1.548643 : training accuracy 0.454800, and val accuracy 0.454800\n",
      "epochs 335 / 2500: loss 1.547020 : training accuracy 0.455100, and val accuracy 0.455100\n",
      "epochs 336 / 2500: loss 1.544607 : training accuracy 0.456500, and val accuracy 0.456500\n",
      "epochs 337 / 2500: loss 1.540978 : training accuracy 0.459500, and val accuracy 0.459500\n",
      "epochs 338 / 2500: loss 1.536968 : training accuracy 0.461100, and val accuracy 0.461100\n",
      "epochs 339 / 2500: loss 1.535393 : training accuracy 0.462400, and val accuracy 0.462400\n",
      "epochs 340 / 2500: loss 1.539276 : training accuracy 0.459900, and val accuracy 0.459900\n",
      "epochs 341 / 2500: loss 1.550703 : training accuracy 0.452400, and val accuracy 0.452400\n",
      "epochs 342 / 2500: loss 1.547499 : training accuracy 0.455300, and val accuracy 0.455300\n",
      "epochs 343 / 2500: loss 1.540728 : training accuracy 0.453100, and val accuracy 0.453100\n",
      "epochs 344 / 2500: loss 1.561369 : training accuracy 0.437000, and val accuracy 0.437000\n",
      "epochs 345 / 2500: loss 1.547767 : training accuracy 0.446100, and val accuracy 0.446100\n",
      "epochs 346 / 2500: loss 1.536158 : training accuracy 0.452500, and val accuracy 0.452500\n",
      "epochs 347 / 2500: loss 1.532510 : training accuracy 0.455400, and val accuracy 0.455400\n",
      "epochs 348 / 2500: loss 1.530254 : training accuracy 0.455800, and val accuracy 0.455800\n",
      "epochs 349 / 2500: loss 1.529138 : training accuracy 0.456400, and val accuracy 0.456400\n",
      "epochs 350 / 2500: loss 1.528410 : training accuracy 0.457300, and val accuracy 0.457300\n",
      "epochs 351 / 2500: loss 1.528033 : training accuracy 0.457200, and val accuracy 0.457200\n",
      "epochs 352 / 2500: loss 1.526582 : training accuracy 0.459500, and val accuracy 0.459500\n",
      "epochs 353 / 2500: loss 1.524555 : training accuracy 0.463500, and val accuracy 0.463500\n",
      "epochs 354 / 2500: loss 1.526710 : training accuracy 0.463000, and val accuracy 0.463000\n",
      "epochs 355 / 2500: loss 1.537938 : training accuracy 0.461100, and val accuracy 0.461100\n",
      "epochs 356 / 2500: loss 1.527017 : training accuracy 0.464300, and val accuracy 0.464300\n",
      "epochs 357 / 2500: loss 1.524069 : training accuracy 0.467100, and val accuracy 0.467100\n",
      "epochs 358 / 2500: loss 1.522892 : training accuracy 0.467200, and val accuracy 0.467200\n",
      "epochs 359 / 2500: loss 1.522873 : training accuracy 0.468500, and val accuracy 0.468500\n",
      "epochs 360 / 2500: loss 1.522608 : training accuracy 0.469900, and val accuracy 0.469900\n",
      "epochs 361 / 2500: loss 1.522791 : training accuracy 0.466100, and val accuracy 0.466100\n",
      "epochs 362 / 2500: loss 1.519408 : training accuracy 0.466700, and val accuracy 0.466700\n",
      "epochs 363 / 2500: loss 1.514930 : training accuracy 0.469100, and val accuracy 0.469100\n",
      "epochs 364 / 2500: loss 1.512523 : training accuracy 0.472400, and val accuracy 0.472400\n",
      "epochs 365 / 2500: loss 1.510417 : training accuracy 0.473400, and val accuracy 0.473400\n",
      "epochs 366 / 2500: loss 1.510331 : training accuracy 0.472700, and val accuracy 0.472700\n",
      "epochs 367 / 2500: loss 1.518285 : training accuracy 0.466200, and val accuracy 0.466200\n",
      "epochs 368 / 2500: loss 1.520617 : training accuracy 0.466800, and val accuracy 0.466800\n",
      "epochs 369 / 2500: loss 1.512364 : training accuracy 0.470200, and val accuracy 0.470200\n",
      "epochs 370 / 2500: loss 1.507930 : training accuracy 0.472700, and val accuracy 0.472700\n",
      "epochs 371 / 2500: loss 1.509690 : training accuracy 0.471200, and val accuracy 0.471200\n",
      "epochs 372 / 2500: loss 1.520672 : training accuracy 0.462200, and val accuracy 0.462200\n",
      "epochs 373 / 2500: loss 1.544509 : training accuracy 0.442600, and val accuracy 0.442600\n",
      "epochs 374 / 2500: loss 1.520286 : training accuracy 0.456300, and val accuracy 0.456300\n",
      "epochs 375 / 2500: loss 1.512547 : training accuracy 0.460800, and val accuracy 0.460800\n",
      "epochs 376 / 2500: loss 1.509480 : training accuracy 0.463400, and val accuracy 0.463400\n",
      "epochs 377 / 2500: loss 1.506439 : training accuracy 0.465000, and val accuracy 0.465000\n",
      "epochs 378 / 2500: loss 1.502952 : training accuracy 0.468500, and val accuracy 0.468500\n",
      "epochs 379 / 2500: loss 1.500616 : training accuracy 0.470600, and val accuracy 0.470600\n",
      "epochs 380 / 2500: loss 1.498155 : training accuracy 0.470700, and val accuracy 0.470700\n",
      "epochs 381 / 2500: loss 1.495916 : training accuracy 0.474700, and val accuracy 0.474700\n",
      "epochs 382 / 2500: loss 1.495587 : training accuracy 0.474900, and val accuracy 0.474900\n",
      "epochs 383 / 2500: loss 1.500357 : training accuracy 0.473400, and val accuracy 0.473400\n",
      "epochs 384 / 2500: loss 1.507993 : training accuracy 0.471200, and val accuracy 0.471200\n",
      "epochs 385 / 2500: loss 1.498216 : training accuracy 0.475000, and val accuracy 0.475000\n",
      "epochs 386 / 2500: loss 1.494356 : training accuracy 0.476200, and val accuracy 0.476200\n",
      "epochs 387 / 2500: loss 1.497184 : training accuracy 0.477500, and val accuracy 0.477500\n",
      "epochs 388 / 2500: loss 1.501192 : training accuracy 0.473700, and val accuracy 0.473700\n",
      "epochs 389 / 2500: loss 1.504729 : training accuracy 0.472800, and val accuracy 0.472800\n",
      "epochs 390 / 2500: loss 1.496006 : training accuracy 0.475800, and val accuracy 0.475800\n",
      "epochs 391 / 2500: loss 1.489092 : training accuracy 0.477000, and val accuracy 0.477000\n",
      "epochs 392 / 2500: loss 1.487000 : training accuracy 0.479500, and val accuracy 0.479500\n",
      "epochs 393 / 2500: loss 1.486869 : training accuracy 0.478000, and val accuracy 0.478000\n",
      "epochs 394 / 2500: loss 1.506326 : training accuracy 0.464900, and val accuracy 0.464900\n",
      "epochs 395 / 2500: loss 1.513338 : training accuracy 0.469100, and val accuracy 0.469100\n",
      "epochs 396 / 2500: loss 1.498086 : training accuracy 0.475100, and val accuracy 0.475100\n",
      "epochs 397 / 2500: loss 1.490698 : training accuracy 0.477100, and val accuracy 0.477100\n",
      "epochs 398 / 2500: loss 1.487286 : training accuracy 0.478000, and val accuracy 0.478000\n",
      "epochs 399 / 2500: loss 1.484471 : training accuracy 0.480500, and val accuracy 0.480500\n",
      "epochs 400 / 2500: loss 1.481259 : training accuracy 0.481800, and val accuracy 0.481800\n",
      "epochs 401 / 2500: loss 1.480570 : training accuracy 0.481100, and val accuracy 0.481100\n",
      "epochs 402 / 2500: loss 1.486473 : training accuracy 0.480400, and val accuracy 0.480400\n",
      "epochs 403 / 2500: loss 1.508809 : training accuracy 0.473700, and val accuracy 0.473700\n",
      "epochs 404 / 2500: loss 1.501593 : training accuracy 0.475500, and val accuracy 0.475500\n",
      "epochs 405 / 2500: loss 1.484257 : training accuracy 0.479600, and val accuracy 0.479600\n",
      "epochs 406 / 2500: loss 1.479166 : training accuracy 0.482900, and val accuracy 0.482900\n",
      "epochs 407 / 2500: loss 1.475459 : training accuracy 0.482000, and val accuracy 0.482000\n",
      "epochs 408 / 2500: loss 1.472759 : training accuracy 0.482700, and val accuracy 0.482700\n",
      "epochs 409 / 2500: loss 1.471180 : training accuracy 0.480900, and val accuracy 0.480900\n",
      "epochs 410 / 2500: loss 1.471365 : training accuracy 0.479700, and val accuracy 0.479700\n",
      "epochs 411 / 2500: loss 1.475245 : training accuracy 0.476100, and val accuracy 0.476100\n",
      "epochs 412 / 2500: loss 1.483973 : training accuracy 0.475500, and val accuracy 0.475500\n",
      "epochs 413 / 2500: loss 1.480639 : training accuracy 0.477500, and val accuracy 0.477500\n",
      "epochs 414 / 2500: loss 1.474049 : training accuracy 0.481400, and val accuracy 0.481400\n",
      "epochs 415 / 2500: loss 1.477097 : training accuracy 0.475200, and val accuracy 0.475200\n",
      "epochs 416 / 2500: loss 1.475250 : training accuracy 0.476500, and val accuracy 0.476500\n",
      "epochs 417 / 2500: loss 1.477548 : training accuracy 0.475400, and val accuracy 0.475400\n",
      "epochs 418 / 2500: loss 1.473871 : training accuracy 0.480300, and val accuracy 0.480300\n",
      "epochs 419 / 2500: loss 1.464978 : training accuracy 0.485100, and val accuracy 0.485100\n",
      "epochs 420 / 2500: loss 1.462786 : training accuracy 0.485400, and val accuracy 0.485400\n",
      "epochs 421 / 2500: loss 1.459588 : training accuracy 0.485100, and val accuracy 0.485100\n",
      "epochs 422 / 2500: loss 1.459000 : training accuracy 0.484300, and val accuracy 0.484300\n",
      "epochs 423 / 2500: loss 1.458532 : training accuracy 0.486800, and val accuracy 0.486800\n",
      "epochs 424 / 2500: loss 1.463520 : training accuracy 0.481200, and val accuracy 0.481200\n",
      "epochs 425 / 2500: loss 1.463433 : training accuracy 0.484800, and val accuracy 0.484800\n",
      "epochs 426 / 2500: loss 1.463204 : training accuracy 0.486500, and val accuracy 0.486500\n",
      "epochs 427 / 2500: loss 1.481458 : training accuracy 0.479600, and val accuracy 0.479600\n",
      "epochs 428 / 2500: loss 1.471761 : training accuracy 0.485800, and val accuracy 0.485800\n",
      "epochs 429 / 2500: loss 1.468962 : training accuracy 0.485600, and val accuracy 0.485600\n",
      "epochs 430 / 2500: loss 1.465987 : training accuracy 0.479900, and val accuracy 0.479900\n",
      "epochs 431 / 2500: loss 1.464201 : training accuracy 0.475900, and val accuracy 0.475900\n",
      "epochs 432 / 2500: loss 1.456545 : training accuracy 0.483500, and val accuracy 0.483500\n",
      "epochs 433 / 2500: loss 1.451407 : training accuracy 0.485300, and val accuracy 0.485300\n",
      "epochs 434 / 2500: loss 1.449602 : training accuracy 0.486500, and val accuracy 0.486500\n",
      "epochs 435 / 2500: loss 1.449241 : training accuracy 0.486700, and val accuracy 0.486700\n",
      "epochs 436 / 2500: loss 1.464286 : training accuracy 0.480200, and val accuracy 0.480200\n",
      "epochs 437 / 2500: loss 1.459920 : training accuracy 0.485800, and val accuracy 0.485800\n",
      "epochs 438 / 2500: loss 1.457441 : training accuracy 0.485600, and val accuracy 0.485600\n",
      "epochs 439 / 2500: loss 1.462956 : training accuracy 0.487500, and val accuracy 0.487500\n",
      "epochs 440 / 2500: loss 1.458928 : training accuracy 0.490900, and val accuracy 0.490900\n",
      "epochs 441 / 2500: loss 1.451230 : training accuracy 0.493500, and val accuracy 0.493500\n",
      "epochs 442 / 2500: loss 1.449745 : training accuracy 0.492000, and val accuracy 0.492000\n",
      "epochs 443 / 2500: loss 1.451515 : training accuracy 0.494300, and val accuracy 0.494300\n",
      "epochs 444 / 2500: loss 1.469062 : training accuracy 0.483700, and val accuracy 0.483700\n",
      "epochs 445 / 2500: loss 1.456411 : training accuracy 0.490300, and val accuracy 0.490300\n",
      "epochs 446 / 2500: loss 1.449830 : training accuracy 0.492700, and val accuracy 0.492700\n",
      "epochs 447 / 2500: loss 1.447557 : training accuracy 0.494100, and val accuracy 0.494100\n",
      "epochs 448 / 2500: loss 1.453037 : training accuracy 0.493200, and val accuracy 0.493200\n",
      "epochs 449 / 2500: loss 1.457967 : training accuracy 0.493100, and val accuracy 0.493100\n",
      "epochs 450 / 2500: loss 1.451762 : training accuracy 0.494800, and val accuracy 0.494800\n",
      "epochs 451 / 2500: loss 1.445031 : training accuracy 0.497000, and val accuracy 0.497000\n",
      "epochs 452 / 2500: loss 1.440838 : training accuracy 0.497800, and val accuracy 0.497800\n",
      "epochs 453 / 2500: loss 1.438133 : training accuracy 0.499400, and val accuracy 0.499400\n",
      "epochs 454 / 2500: loss 1.436323 : training accuracy 0.499800, and val accuracy 0.499800\n",
      "epochs 455 / 2500: loss 1.445478 : training accuracy 0.493500, and val accuracy 0.493500\n",
      "epochs 456 / 2500: loss 1.464414 : training accuracy 0.488800, and val accuracy 0.488800\n",
      "epochs 457 / 2500: loss 1.445369 : training accuracy 0.498400, and val accuracy 0.498400\n",
      "epochs 458 / 2500: loss 1.451019 : training accuracy 0.488700, and val accuracy 0.488700\n",
      "epochs 459 / 2500: loss 1.443151 : training accuracy 0.492300, and val accuracy 0.492300\n",
      "epochs 460 / 2500: loss 1.458036 : training accuracy 0.474300, and val accuracy 0.474300\n",
      "epochs 461 / 2500: loss 1.438009 : training accuracy 0.490700, and val accuracy 0.490700\n",
      "epochs 462 / 2500: loss 1.432109 : training accuracy 0.493400, and val accuracy 0.493400\n",
      "epochs 463 / 2500: loss 1.429440 : training accuracy 0.495100, and val accuracy 0.495100\n",
      "epochs 464 / 2500: loss 1.427454 : training accuracy 0.496800, and val accuracy 0.496800\n",
      "epochs 465 / 2500: loss 1.429633 : training accuracy 0.497500, and val accuracy 0.497500\n",
      "epochs 466 / 2500: loss 1.438428 : training accuracy 0.493200, and val accuracy 0.493200\n",
      "epochs 467 / 2500: loss 1.432599 : training accuracy 0.496800, and val accuracy 0.496800\n",
      "epochs 468 / 2500: loss 1.427673 : training accuracy 0.498700, and val accuracy 0.498700\n",
      "epochs 469 / 2500: loss 1.425861 : training accuracy 0.502400, and val accuracy 0.502400\n",
      "epochs 470 / 2500: loss 1.433592 : training accuracy 0.500200, and val accuracy 0.500200\n",
      "epochs 471 / 2500: loss 1.460797 : training accuracy 0.494600, and val accuracy 0.494600\n",
      "epochs 472 / 2500: loss 1.436643 : training accuracy 0.497700, and val accuracy 0.497700\n",
      "epochs 473 / 2500: loss 1.422854 : training accuracy 0.499700, and val accuracy 0.499700\n",
      "epochs 474 / 2500: loss 1.417094 : training accuracy 0.500600, and val accuracy 0.500600\n",
      "epochs 475 / 2500: loss 1.416334 : training accuracy 0.500100, and val accuracy 0.500100\n",
      "epochs 476 / 2500: loss 1.426525 : training accuracy 0.497900, and val accuracy 0.497900\n",
      "epochs 477 / 2500: loss 1.437205 : training accuracy 0.494500, and val accuracy 0.494500\n",
      "epochs 478 / 2500: loss 1.427890 : training accuracy 0.501300, and val accuracy 0.501300\n",
      "epochs 479 / 2500: loss 1.421494 : training accuracy 0.503600, and val accuracy 0.503600\n",
      "epochs 480 / 2500: loss 1.418439 : training accuracy 0.501400, and val accuracy 0.501400\n",
      "epochs 481 / 2500: loss 1.440331 : training accuracy 0.484700, and val accuracy 0.484700\n",
      "epochs 482 / 2500: loss 1.433391 : training accuracy 0.490000, and val accuracy 0.490000\n",
      "epochs 483 / 2500: loss 1.427141 : training accuracy 0.493400, and val accuracy 0.493400\n",
      "epochs 484 / 2500: loss 1.415972 : training accuracy 0.499800, and val accuracy 0.499800\n",
      "epochs 485 / 2500: loss 1.415750 : training accuracy 0.499800, and val accuracy 0.499800\n",
      "epochs 486 / 2500: loss 1.421257 : training accuracy 0.496700, and val accuracy 0.496700\n",
      "epochs 487 / 2500: loss 1.430193 : training accuracy 0.500400, and val accuracy 0.500400\n",
      "epochs 488 / 2500: loss 1.442372 : training accuracy 0.495100, and val accuracy 0.495100\n",
      "epochs 489 / 2500: loss 1.417503 : training accuracy 0.506900, and val accuracy 0.506900\n",
      "epochs 490 / 2500: loss 1.420368 : training accuracy 0.503200, and val accuracy 0.503200\n",
      "epochs 491 / 2500: loss 1.438883 : training accuracy 0.491200, and val accuracy 0.491200\n",
      "epochs 492 / 2500: loss 1.429281 : training accuracy 0.492200, and val accuracy 0.492200\n",
      "epochs 493 / 2500: loss 1.413315 : training accuracy 0.502900, and val accuracy 0.502900\n",
      "epochs 494 / 2500: loss 1.404585 : training accuracy 0.505600, and val accuracy 0.505600\n",
      "epochs 495 / 2500: loss 1.427509 : training accuracy 0.491900, and val accuracy 0.491900\n",
      "epochs 496 / 2500: loss 1.404549 : training accuracy 0.501100, and val accuracy 0.501100\n",
      "epochs 497 / 2500: loss 1.413176 : training accuracy 0.502200, and val accuracy 0.502200\n",
      "epochs 498 / 2500: loss 1.403790 : training accuracy 0.506400, and val accuracy 0.506400\n",
      "epochs 499 / 2500: loss 1.401808 : training accuracy 0.510300, and val accuracy 0.510300\n",
      "epochs 500 / 2500: loss 1.405559 : training accuracy 0.507700, and val accuracy 0.507700\n",
      "epochs 501 / 2500: loss 1.430639 : training accuracy 0.496700, and val accuracy 0.496700\n",
      "epochs 502 / 2500: loss 1.410987 : training accuracy 0.501900, and val accuracy 0.501900\n",
      "epochs 503 / 2500: loss 1.406782 : training accuracy 0.502500, and val accuracy 0.502500\n",
      "epochs 504 / 2500: loss 1.406446 : training accuracy 0.504000, and val accuracy 0.504000\n",
      "epochs 505 / 2500: loss 1.441675 : training accuracy 0.501200, and val accuracy 0.501200\n",
      "epochs 506 / 2500: loss 1.414069 : training accuracy 0.510600, and val accuracy 0.510600\n",
      "epochs 507 / 2500: loss 1.409525 : training accuracy 0.505400, and val accuracy 0.505400\n",
      "epochs 508 / 2500: loss 1.408112 : training accuracy 0.502700, and val accuracy 0.502700\n",
      "epochs 509 / 2500: loss 1.406654 : training accuracy 0.499100, and val accuracy 0.499100\n",
      "epochs 510 / 2500: loss 1.402614 : training accuracy 0.503000, and val accuracy 0.503000\n",
      "epochs 511 / 2500: loss 1.396514 : training accuracy 0.506100, and val accuracy 0.506100\n",
      "epochs 512 / 2500: loss 1.391864 : training accuracy 0.511300, and val accuracy 0.511300\n",
      "epochs 513 / 2500: loss 1.390928 : training accuracy 0.511900, and val accuracy 0.511900\n",
      "epochs 514 / 2500: loss 1.393886 : training accuracy 0.511600, and val accuracy 0.511600\n",
      "epochs 515 / 2500: loss 1.409209 : training accuracy 0.507300, and val accuracy 0.507300\n",
      "epochs 516 / 2500: loss 1.410579 : training accuracy 0.510300, and val accuracy 0.510300\n",
      "epochs 517 / 2500: loss 1.401741 : training accuracy 0.512600, and val accuracy 0.512600\n",
      "epochs 518 / 2500: loss 1.397946 : training accuracy 0.514200, and val accuracy 0.514200\n",
      "epochs 519 / 2500: loss 1.394348 : training accuracy 0.516200, and val accuracy 0.516200\n",
      "epochs 520 / 2500: loss 1.390681 : training accuracy 0.517300, and val accuracy 0.517300\n",
      "epochs 521 / 2500: loss 1.387126 : training accuracy 0.515900, and val accuracy 0.515900\n",
      "epochs 522 / 2500: loss 1.387756 : training accuracy 0.509400, and val accuracy 0.509400\n",
      "epochs 523 / 2500: loss 1.411234 : training accuracy 0.500500, and val accuracy 0.500500\n",
      "epochs 524 / 2500: loss 1.405879 : training accuracy 0.510000, and val accuracy 0.510000\n",
      "epochs 525 / 2500: loss 1.406326 : training accuracy 0.512600, and val accuracy 0.512600\n",
      "epochs 526 / 2500: loss 1.409483 : training accuracy 0.508600, and val accuracy 0.508600\n",
      "epochs 527 / 2500: loss 1.412630 : training accuracy 0.502200, and val accuracy 0.502200\n",
      "epochs 528 / 2500: loss 1.404556 : training accuracy 0.505100, and val accuracy 0.505100\n",
      "epochs 529 / 2500: loss 1.403324 : training accuracy 0.508000, and val accuracy 0.508000\n",
      "epochs 530 / 2500: loss 1.392202 : training accuracy 0.513500, and val accuracy 0.513500\n",
      "epochs 531 / 2500: loss 1.392495 : training accuracy 0.512800, and val accuracy 0.512800\n",
      "epochs 532 / 2500: loss 1.399117 : training accuracy 0.515500, and val accuracy 0.515500\n",
      "epochs 533 / 2500: loss 1.400130 : training accuracy 0.514100, and val accuracy 0.514100\n",
      "epochs 534 / 2500: loss 1.425514 : training accuracy 0.501600, and val accuracy 0.501600\n",
      "epochs 535 / 2500: loss 1.400223 : training accuracy 0.509400, and val accuracy 0.509400\n",
      "epochs 536 / 2500: loss 1.395289 : training accuracy 0.512200, and val accuracy 0.512200\n",
      "epochs 537 / 2500: loss 1.391291 : training accuracy 0.518500, and val accuracy 0.518500\n",
      "epochs 538 / 2500: loss 1.384704 : training accuracy 0.522400, and val accuracy 0.522400\n",
      "epochs 539 / 2500: loss 1.380365 : training accuracy 0.512300, and val accuracy 0.512300\n",
      "epochs 540 / 2500: loss 1.375537 : training accuracy 0.514000, and val accuracy 0.514000\n",
      "epochs 541 / 2500: loss 1.377138 : training accuracy 0.514900, and val accuracy 0.514900\n",
      "epochs 542 / 2500: loss 1.376242 : training accuracy 0.513500, and val accuracy 0.513500\n",
      "epochs 543 / 2500: loss 1.372835 : training accuracy 0.514500, and val accuracy 0.514500\n",
      "epochs 544 / 2500: loss 1.400562 : training accuracy 0.505600, and val accuracy 0.505600\n",
      "epochs 545 / 2500: loss 1.391975 : training accuracy 0.509000, and val accuracy 0.509000\n",
      "epochs 546 / 2500: loss 1.397347 : training accuracy 0.512100, and val accuracy 0.512100\n",
      "epochs 547 / 2500: loss 1.375084 : training accuracy 0.520300, and val accuracy 0.520300\n",
      "epochs 548 / 2500: loss 1.397645 : training accuracy 0.505500, and val accuracy 0.505500\n",
      "epochs 549 / 2500: loss 1.404937 : training accuracy 0.498700, and val accuracy 0.498700\n",
      "epochs 550 / 2500: loss 1.385330 : training accuracy 0.508700, and val accuracy 0.508700\n",
      "epochs 551 / 2500: loss 1.380039 : training accuracy 0.513200, and val accuracy 0.513200\n",
      "epochs 552 / 2500: loss 1.373162 : training accuracy 0.515300, and val accuracy 0.515300\n",
      "epochs 553 / 2500: loss 1.370101 : training accuracy 0.516100, and val accuracy 0.516100\n",
      "epochs 554 / 2500: loss 1.366334 : training accuracy 0.517800, and val accuracy 0.517800\n",
      "epochs 555 / 2500: loss 1.364261 : training accuracy 0.519200, and val accuracy 0.519200\n",
      "epochs 556 / 2500: loss 1.364482 : training accuracy 0.521000, and val accuracy 0.521000\n",
      "epochs 557 / 2500: loss 1.383929 : training accuracy 0.513700, and val accuracy 0.513700\n",
      "epochs 558 / 2500: loss 1.375329 : training accuracy 0.521500, and val accuracy 0.521500\n",
      "epochs 559 / 2500: loss 1.380739 : training accuracy 0.518100, and val accuracy 0.518100\n",
      "epochs 560 / 2500: loss 1.449697 : training accuracy 0.505500, and val accuracy 0.505500\n",
      "epochs 561 / 2500: loss 1.388990 : training accuracy 0.523700, and val accuracy 0.523700\n",
      "epochs 562 / 2500: loss 1.383271 : training accuracy 0.521500, and val accuracy 0.521500\n",
      "epochs 563 / 2500: loss 1.395604 : training accuracy 0.507600, and val accuracy 0.507600\n",
      "epochs 564 / 2500: loss 1.369890 : training accuracy 0.522100, and val accuracy 0.522100\n",
      "epochs 565 / 2500: loss 1.380990 : training accuracy 0.517700, and val accuracy 0.517700\n",
      "epochs 566 / 2500: loss 1.363874 : training accuracy 0.522000, and val accuracy 0.522000\n",
      "epochs 567 / 2500: loss 1.360141 : training accuracy 0.523200, and val accuracy 0.523200\n",
      "epochs 568 / 2500: loss 1.372725 : training accuracy 0.515100, and val accuracy 0.515100\n",
      "epochs 569 / 2500: loss 1.367866 : training accuracy 0.518200, and val accuracy 0.518200\n",
      "epochs 570 / 2500: loss 1.370945 : training accuracy 0.515800, and val accuracy 0.515800\n",
      "epochs 571 / 2500: loss 1.360316 : training accuracy 0.525600, and val accuracy 0.525600\n",
      "epochs 572 / 2500: loss 1.372828 : training accuracy 0.522300, and val accuracy 0.522300\n",
      "epochs 573 / 2500: loss 1.368848 : training accuracy 0.517100, and val accuracy 0.517100\n",
      "epochs 574 / 2500: loss 1.367478 : training accuracy 0.516500, and val accuracy 0.516500\n",
      "epochs 575 / 2500: loss 1.382927 : training accuracy 0.519100, and val accuracy 0.519100\n",
      "epochs 576 / 2500: loss 1.368364 : training accuracy 0.523100, and val accuracy 0.523100\n",
      "epochs 577 / 2500: loss 1.400129 : training accuracy 0.506300, and val accuracy 0.506300\n",
      "epochs 578 / 2500: loss 1.387717 : training accuracy 0.514000, and val accuracy 0.514000\n",
      "epochs 579 / 2500: loss 1.391535 : training accuracy 0.518200, and val accuracy 0.518200\n",
      "epochs 580 / 2500: loss 1.382705 : training accuracy 0.526900, and val accuracy 0.526900\n",
      "epochs 581 / 2500: loss 1.376575 : training accuracy 0.523600, and val accuracy 0.523600\n",
      "epochs 582 / 2500: loss 1.364589 : training accuracy 0.524500, and val accuracy 0.524500\n",
      "epochs 583 / 2500: loss 1.357948 : training accuracy 0.525100, and val accuracy 0.525100\n",
      "epochs 584 / 2500: loss 1.358472 : training accuracy 0.525800, and val accuracy 0.525800\n",
      "epochs 585 / 2500: loss 1.352941 : training accuracy 0.530200, and val accuracy 0.530200\n",
      "epochs 586 / 2500: loss 1.380848 : training accuracy 0.522900, and val accuracy 0.522900\n",
      "epochs 587 / 2500: loss 1.360977 : training accuracy 0.525800, and val accuracy 0.525800\n",
      "epochs 588 / 2500: loss 1.376893 : training accuracy 0.513500, and val accuracy 0.513500\n",
      "epochs 589 / 2500: loss 1.377891 : training accuracy 0.508000, and val accuracy 0.508000\n",
      "epochs 590 / 2500: loss 1.351817 : training accuracy 0.519300, and val accuracy 0.519300\n",
      "epochs 591 / 2500: loss 1.340231 : training accuracy 0.529300, and val accuracy 0.529300\n",
      "epochs 592 / 2500: loss 1.411234 : training accuracy 0.505700, and val accuracy 0.505700\n",
      "epochs 593 / 2500: loss 1.361185 : training accuracy 0.524400, and val accuracy 0.524400\n",
      "epochs 594 / 2500: loss 1.358991 : training accuracy 0.523300, and val accuracy 0.523300\n",
      "epochs 595 / 2500: loss 1.350655 : training accuracy 0.526000, and val accuracy 0.526000\n",
      "epochs 596 / 2500: loss 1.342628 : training accuracy 0.530800, and val accuracy 0.530800\n",
      "epochs 597 / 2500: loss 1.347754 : training accuracy 0.529900, and val accuracy 0.529900\n",
      "epochs 598 / 2500: loss 1.357664 : training accuracy 0.527200, and val accuracy 0.527200\n",
      "epochs 599 / 2500: loss 1.349262 : training accuracy 0.529700, and val accuracy 0.529700\n",
      "epochs 600 / 2500: loss 1.343518 : training accuracy 0.530500, and val accuracy 0.530500\n",
      "epochs 601 / 2500: loss 1.350015 : training accuracy 0.523700, and val accuracy 0.523700\n",
      "epochs 602 / 2500: loss 1.341305 : training accuracy 0.528000, and val accuracy 0.528000\n",
      "epochs 603 / 2500: loss 1.333151 : training accuracy 0.530700, and val accuracy 0.530700\n",
      "epochs 604 / 2500: loss 1.353391 : training accuracy 0.520200, and val accuracy 0.520200\n",
      "epochs 605 / 2500: loss 1.346371 : training accuracy 0.522700, and val accuracy 0.522700\n",
      "epochs 606 / 2500: loss 1.357666 : training accuracy 0.522900, and val accuracy 0.522900\n",
      "epochs 607 / 2500: loss 1.376858 : training accuracy 0.520400, and val accuracy 0.520400\n",
      "epochs 608 / 2500: loss 1.365682 : training accuracy 0.524800, and val accuracy 0.524800\n",
      "epochs 609 / 2500: loss 1.356296 : training accuracy 0.525700, and val accuracy 0.525700\n",
      "epochs 610 / 2500: loss 1.352768 : training accuracy 0.521500, and val accuracy 0.521500\n",
      "epochs 611 / 2500: loss 1.361800 : training accuracy 0.511900, and val accuracy 0.511900\n",
      "epochs 612 / 2500: loss 1.340973 : training accuracy 0.520400, and val accuracy 0.520400\n",
      "epochs 613 / 2500: loss 1.335571 : training accuracy 0.522700, and val accuracy 0.522700\n",
      "epochs 614 / 2500: loss 1.341815 : training accuracy 0.523600, and val accuracy 0.523600\n",
      "epochs 615 / 2500: loss 1.353121 : training accuracy 0.525100, and val accuracy 0.525100\n",
      "epochs 616 / 2500: loss 1.349290 : training accuracy 0.530200, and val accuracy 0.530200\n",
      "epochs 617 / 2500: loss 1.344025 : training accuracy 0.532400, and val accuracy 0.532400\n",
      "epochs 618 / 2500: loss 1.348557 : training accuracy 0.532600, and val accuracy 0.532600\n",
      "epochs 619 / 2500: loss 1.346792 : training accuracy 0.528600, and val accuracy 0.528600\n",
      "epochs 620 / 2500: loss 1.340786 : training accuracy 0.530700, and val accuracy 0.530700\n",
      "epochs 621 / 2500: loss 1.341178 : training accuracy 0.531400, and val accuracy 0.531400\n",
      "epochs 622 / 2500: loss 1.350365 : training accuracy 0.528100, and val accuracy 0.528100\n",
      "epochs 623 / 2500: loss 1.353865 : training accuracy 0.528600, and val accuracy 0.528600\n",
      "epochs 624 / 2500: loss 1.348946 : training accuracy 0.530400, and val accuracy 0.530400\n",
      "epochs 625 / 2500: loss 1.343821 : training accuracy 0.531300, and val accuracy 0.531300\n",
      "epochs 626 / 2500: loss 1.337501 : training accuracy 0.527100, and val accuracy 0.527100\n",
      "epochs 627 / 2500: loss 1.354727 : training accuracy 0.516000, and val accuracy 0.516000\n",
      "epochs 628 / 2500: loss 1.337916 : training accuracy 0.525700, and val accuracy 0.525700\n",
      "epochs 629 / 2500: loss 1.333604 : training accuracy 0.526900, and val accuracy 0.526900\n",
      "epochs 630 / 2500: loss 1.320868 : training accuracy 0.535200, and val accuracy 0.535200\n",
      "epochs 631 / 2500: loss 1.317794 : training accuracy 0.538300, and val accuracy 0.538300\n",
      "epochs 632 / 2500: loss 1.348260 : training accuracy 0.523100, and val accuracy 0.523100\n",
      "epochs 633 / 2500: loss 1.334976 : training accuracy 0.527500, and val accuracy 0.527500\n",
      "epochs 634 / 2500: loss 1.357622 : training accuracy 0.526700, and val accuracy 0.526700\n",
      "epochs 635 / 2500: loss 1.357701 : training accuracy 0.536800, and val accuracy 0.536800\n",
      "epochs 636 / 2500: loss 1.343316 : training accuracy 0.541200, and val accuracy 0.541200\n",
      "epochs 637 / 2500: loss 1.341518 : training accuracy 0.534900, and val accuracy 0.534900\n",
      "epochs 638 / 2500: loss 1.355379 : training accuracy 0.529700, and val accuracy 0.529700\n",
      "epochs 639 / 2500: loss 1.338734 : training accuracy 0.542800, and val accuracy 0.542800\n",
      "epochs 640 / 2500: loss 1.339016 : training accuracy 0.538100, and val accuracy 0.538100\n",
      "epochs 641 / 2500: loss 1.323375 : training accuracy 0.532200, and val accuracy 0.532200\n",
      "epochs 642 / 2500: loss 1.338090 : training accuracy 0.526300, and val accuracy 0.526300\n",
      "epochs 643 / 2500: loss 1.336857 : training accuracy 0.525700, and val accuracy 0.525700\n",
      "epochs 644 / 2500: loss 1.347712 : training accuracy 0.522900, and val accuracy 0.522900\n",
      "epochs 645 / 2500: loss 1.341794 : training accuracy 0.533500, and val accuracy 0.533500\n",
      "epochs 646 / 2500: loss 1.343020 : training accuracy 0.539300, and val accuracy 0.539300\n",
      "epochs 647 / 2500: loss 1.346891 : training accuracy 0.537200, and val accuracy 0.537200\n",
      "epochs 648 / 2500: loss 1.334208 : training accuracy 0.535400, and val accuracy 0.535400\n",
      "epochs 649 / 2500: loss 1.342292 : training accuracy 0.532600, and val accuracy 0.532600\n",
      "epochs 650 / 2500: loss 1.324139 : training accuracy 0.536800, and val accuracy 0.536800\n",
      "epochs 651 / 2500: loss 1.327604 : training accuracy 0.536600, and val accuracy 0.536600\n",
      "epochs 652 / 2500: loss 1.318675 : training accuracy 0.541900, and val accuracy 0.541900\n",
      "epochs 653 / 2500: loss 1.314009 : training accuracy 0.542300, and val accuracy 0.542300\n",
      "epochs 654 / 2500: loss 1.311830 : training accuracy 0.540700, and val accuracy 0.540700\n",
      "epochs 655 / 2500: loss 1.339006 : training accuracy 0.518700, and val accuracy 0.518700\n",
      "epochs 656 / 2500: loss 1.313526 : training accuracy 0.532800, and val accuracy 0.532800\n",
      "epochs 657 / 2500: loss 1.313447 : training accuracy 0.533800, and val accuracy 0.533800\n",
      "epochs 658 / 2500: loss 1.312656 : training accuracy 0.534000, and val accuracy 0.534000\n",
      "epochs 659 / 2500: loss 1.325411 : training accuracy 0.524600, and val accuracy 0.524600\n",
      "epochs 660 / 2500: loss 1.329172 : training accuracy 0.532900, and val accuracy 0.532900\n",
      "epochs 661 / 2500: loss 1.360185 : training accuracy 0.538700, and val accuracy 0.538700\n",
      "epochs 662 / 2500: loss 1.329352 : training accuracy 0.546000, and val accuracy 0.546000\n",
      "epochs 663 / 2500: loss 1.326544 : training accuracy 0.539200, and val accuracy 0.539200\n",
      "epochs 664 / 2500: loss 1.321144 : training accuracy 0.534300, and val accuracy 0.534300\n",
      "epochs 665 / 2500: loss 1.314566 : training accuracy 0.539200, and val accuracy 0.539200\n",
      "epochs 666 / 2500: loss 1.323760 : training accuracy 0.523500, and val accuracy 0.523500\n",
      "epochs 667 / 2500: loss 1.305441 : training accuracy 0.535700, and val accuracy 0.535700\n",
      "epochs 668 / 2500: loss 1.322616 : training accuracy 0.530500, and val accuracy 0.530500\n",
      "epochs 669 / 2500: loss 1.304230 : training accuracy 0.538200, and val accuracy 0.538200\n",
      "epochs 670 / 2500: loss 1.321605 : training accuracy 0.537400, and val accuracy 0.537400\n",
      "epochs 671 / 2500: loss 1.335964 : training accuracy 0.540200, and val accuracy 0.540200\n",
      "epochs 672 / 2500: loss 1.345243 : training accuracy 0.542100, and val accuracy 0.542100\n",
      "epochs 673 / 2500: loss 1.314837 : training accuracy 0.535700, and val accuracy 0.535700\n",
      "epochs 674 / 2500: loss 1.308912 : training accuracy 0.536400, and val accuracy 0.536400\n",
      "epochs 675 / 2500: loss 1.301675 : training accuracy 0.542900, and val accuracy 0.542900\n",
      "epochs 676 / 2500: loss 1.297447 : training accuracy 0.545100, and val accuracy 0.545100\n",
      "epochs 677 / 2500: loss 1.294467 : training accuracy 0.546400, and val accuracy 0.546400\n",
      "epochs 678 / 2500: loss 1.327888 : training accuracy 0.535400, and val accuracy 0.535400\n",
      "epochs 679 / 2500: loss 1.329014 : training accuracy 0.540500, and val accuracy 0.540500\n",
      "epochs 680 / 2500: loss 1.304466 : training accuracy 0.544900, and val accuracy 0.544900\n",
      "epochs 681 / 2500: loss 1.309822 : training accuracy 0.534500, and val accuracy 0.534500\n",
      "epochs 682 / 2500: loss 1.318271 : training accuracy 0.533500, and val accuracy 0.533500\n",
      "epochs 683 / 2500: loss 1.319161 : training accuracy 0.540400, and val accuracy 0.540400\n",
      "epochs 684 / 2500: loss 1.341344 : training accuracy 0.529300, and val accuracy 0.529300\n",
      "epochs 685 / 2500: loss 1.301910 : training accuracy 0.546700, and val accuracy 0.546700\n",
      "epochs 686 / 2500: loss 1.304126 : training accuracy 0.546800, and val accuracy 0.546800\n",
      "epochs 687 / 2500: loss 1.331617 : training accuracy 0.528700, and val accuracy 0.528700\n",
      "epochs 688 / 2500: loss 1.314422 : training accuracy 0.540600, and val accuracy 0.540600\n",
      "epochs 689 / 2500: loss 1.305430 : training accuracy 0.543400, and val accuracy 0.543400\n",
      "epochs 690 / 2500: loss 1.294549 : training accuracy 0.548600, and val accuracy 0.548600\n",
      "epochs 691 / 2500: loss 1.288193 : training accuracy 0.554200, and val accuracy 0.554200\n",
      "epochs 692 / 2500: loss 1.308733 : training accuracy 0.536800, and val accuracy 0.536800\n",
      "epochs 693 / 2500: loss 1.319221 : training accuracy 0.535500, and val accuracy 0.535500\n",
      "epochs 694 / 2500: loss 1.309498 : training accuracy 0.536900, and val accuracy 0.536900\n",
      "epochs 695 / 2500: loss 1.296768 : training accuracy 0.541600, and val accuracy 0.541600\n",
      "epochs 696 / 2500: loss 1.319254 : training accuracy 0.542700, and val accuracy 0.542700\n",
      "epochs 697 / 2500: loss 1.315822 : training accuracy 0.540300, and val accuracy 0.540300\n",
      "epochs 698 / 2500: loss 1.291105 : training accuracy 0.547700, and val accuracy 0.547700\n",
      "epochs 699 / 2500: loss 1.297171 : training accuracy 0.542600, and val accuracy 0.542600\n",
      "epochs 700 / 2500: loss 1.315127 : training accuracy 0.542200, and val accuracy 0.542200\n",
      "epochs 701 / 2500: loss 1.328677 : training accuracy 0.541000, and val accuracy 0.541000\n",
      "epochs 702 / 2500: loss 1.347976 : training accuracy 0.526600, and val accuracy 0.526600\n",
      "epochs 703 / 2500: loss 1.296273 : training accuracy 0.549800, and val accuracy 0.549800\n",
      "epochs 704 / 2500: loss 1.294243 : training accuracy 0.547600, and val accuracy 0.547600\n",
      "epochs 705 / 2500: loss 1.319623 : training accuracy 0.531700, and val accuracy 0.531700\n",
      "epochs 706 / 2500: loss 1.297596 : training accuracy 0.546500, and val accuracy 0.546500\n",
      "epochs 707 / 2500: loss 1.311248 : training accuracy 0.543700, and val accuracy 0.543700\n",
      "epochs 708 / 2500: loss 1.288529 : training accuracy 0.550400, and val accuracy 0.550400\n",
      "epochs 709 / 2500: loss 1.283288 : training accuracy 0.551200, and val accuracy 0.551200\n",
      "epochs 710 / 2500: loss 1.288262 : training accuracy 0.544900, and val accuracy 0.544900\n",
      "epochs 711 / 2500: loss 1.303299 : training accuracy 0.545000, and val accuracy 0.545000\n",
      "epochs 712 / 2500: loss 1.293527 : training accuracy 0.548800, and val accuracy 0.548800\n",
      "epochs 713 / 2500: loss 1.329088 : training accuracy 0.541000, and val accuracy 0.541000\n",
      "epochs 714 / 2500: loss 1.292886 : training accuracy 0.556300, and val accuracy 0.556300\n",
      "epochs 715 / 2500: loss 1.297033 : training accuracy 0.554100, and val accuracy 0.554100\n",
      "epochs 716 / 2500: loss 1.286470 : training accuracy 0.553000, and val accuracy 0.553000\n",
      "epochs 717 / 2500: loss 1.304117 : training accuracy 0.545400, and val accuracy 0.545400\n",
      "epochs 718 / 2500: loss 1.289855 : training accuracy 0.550200, and val accuracy 0.550200\n",
      "epochs 719 / 2500: loss 1.323481 : training accuracy 0.533700, and val accuracy 0.533700\n",
      "epochs 720 / 2500: loss 1.299090 : training accuracy 0.540800, and val accuracy 0.540800\n",
      "epochs 721 / 2500: loss 1.295905 : training accuracy 0.544100, and val accuracy 0.544100\n",
      "epochs 722 / 2500: loss 1.287959 : training accuracy 0.554300, and val accuracy 0.554300\n",
      "epochs 723 / 2500: loss 1.278083 : training accuracy 0.557500, and val accuracy 0.557500\n",
      "epochs 724 / 2500: loss 1.270463 : training accuracy 0.554700, and val accuracy 0.554700\n",
      "epochs 725 / 2500: loss 1.296026 : training accuracy 0.543200, and val accuracy 0.543200\n",
      "epochs 726 / 2500: loss 1.299258 : training accuracy 0.550600, and val accuracy 0.550600\n",
      "epochs 727 / 2500: loss 1.313607 : training accuracy 0.556600, and val accuracy 0.556600\n",
      "epochs 728 / 2500: loss 1.300873 : training accuracy 0.555700, and val accuracy 0.555700\n",
      "epochs 729 / 2500: loss 1.300013 : training accuracy 0.550000, and val accuracy 0.550000\n",
      "epochs 730 / 2500: loss 1.305338 : training accuracy 0.554200, and val accuracy 0.554200\n",
      "epochs 731 / 2500: loss 1.321351 : training accuracy 0.546200, and val accuracy 0.546200\n",
      "epochs 732 / 2500: loss 1.291531 : training accuracy 0.547800, and val accuracy 0.547800\n",
      "epochs 733 / 2500: loss 1.294311 : training accuracy 0.545200, and val accuracy 0.545200\n",
      "epochs 734 / 2500: loss 1.289213 : training accuracy 0.550100, and val accuracy 0.550100\n",
      "epochs 735 / 2500: loss 1.282702 : training accuracy 0.554600, and val accuracy 0.554600\n",
      "epochs 736 / 2500: loss 1.271824 : training accuracy 0.557700, and val accuracy 0.557700\n",
      "epochs 737 / 2500: loss 1.268864 : training accuracy 0.560800, and val accuracy 0.560800\n",
      "epochs 738 / 2500: loss 1.288567 : training accuracy 0.546900, and val accuracy 0.546900\n",
      "epochs 739 / 2500: loss 1.286563 : training accuracy 0.540500, and val accuracy 0.540500\n",
      "epochs 740 / 2500: loss 1.290194 : training accuracy 0.546200, and val accuracy 0.546200\n",
      "epochs 741 / 2500: loss 1.273402 : training accuracy 0.556200, and val accuracy 0.556200\n",
      "epochs 742 / 2500: loss 1.278140 : training accuracy 0.559000, and val accuracy 0.559000\n",
      "epochs 743 / 2500: loss 1.277881 : training accuracy 0.554900, and val accuracy 0.554900\n",
      "epochs 744 / 2500: loss 1.286064 : training accuracy 0.548800, and val accuracy 0.548800\n",
      "epochs 745 / 2500: loss 1.287358 : training accuracy 0.548000, and val accuracy 0.548000\n",
      "epochs 746 / 2500: loss 1.285622 : training accuracy 0.550800, and val accuracy 0.550800\n",
      "epochs 747 / 2500: loss 1.308198 : training accuracy 0.550700, and val accuracy 0.550700\n",
      "epochs 748 / 2500: loss 1.290916 : training accuracy 0.555900, and val accuracy 0.555900\n",
      "epochs 749 / 2500: loss 1.315069 : training accuracy 0.542600, and val accuracy 0.542600\n",
      "epochs 750 / 2500: loss 1.286478 : training accuracy 0.559200, and val accuracy 0.559200\n",
      "epochs 751 / 2500: loss 1.319052 : training accuracy 0.549000, and val accuracy 0.549000\n",
      "epochs 752 / 2500: loss 1.294659 : training accuracy 0.550500, and val accuracy 0.550500\n",
      "epochs 753 / 2500: loss 1.280669 : training accuracy 0.543500, and val accuracy 0.543500\n",
      "epochs 754 / 2500: loss 1.278080 : training accuracy 0.543800, and val accuracy 0.543800\n",
      "epochs 755 / 2500: loss 1.273186 : training accuracy 0.544500, and val accuracy 0.544500\n",
      "epochs 756 / 2500: loss 1.320166 : training accuracy 0.551300, and val accuracy 0.551300\n",
      "epochs 757 / 2500: loss 1.277242 : training accuracy 0.552200, and val accuracy 0.552200\n",
      "epochs 758 / 2500: loss 1.269591 : training accuracy 0.559800, and val accuracy 0.559800\n",
      "epochs 759 / 2500: loss 1.279613 : training accuracy 0.541100, and val accuracy 0.541100\n",
      "epochs 760 / 2500: loss 1.275661 : training accuracy 0.555800, and val accuracy 0.555800\n",
      "epochs 761 / 2500: loss 1.280059 : training accuracy 0.555000, and val accuracy 0.555000\n",
      "epochs 762 / 2500: loss 1.264518 : training accuracy 0.561500, and val accuracy 0.561500\n",
      "epochs 763 / 2500: loss 1.280071 : training accuracy 0.560200, and val accuracy 0.560200\n",
      "epochs 764 / 2500: loss 1.272690 : training accuracy 0.564800, and val accuracy 0.564800\n",
      "epochs 765 / 2500: loss 1.284889 : training accuracy 0.560000, and val accuracy 0.560000\n",
      "epochs 766 / 2500: loss 1.266548 : training accuracy 0.562500, and val accuracy 0.562500\n",
      "epochs 767 / 2500: loss 1.275097 : training accuracy 0.562700, and val accuracy 0.562700\n",
      "epochs 768 / 2500: loss 1.316433 : training accuracy 0.544400, and val accuracy 0.544400\n",
      "epochs 769 / 2500: loss 1.289244 : training accuracy 0.552000, and val accuracy 0.552000\n",
      "epochs 770 / 2500: loss 1.276816 : training accuracy 0.554500, and val accuracy 0.554500\n",
      "epochs 771 / 2500: loss 1.300509 : training accuracy 0.536000, and val accuracy 0.536000\n",
      "epochs 772 / 2500: loss 1.262087 : training accuracy 0.563300, and val accuracy 0.563300\n",
      "epochs 773 / 2500: loss 1.257653 : training accuracy 0.567400, and val accuracy 0.567400\n",
      "epochs 774 / 2500: loss 1.250029 : training accuracy 0.573200, and val accuracy 0.573200\n",
      "epochs 775 / 2500: loss 1.251537 : training accuracy 0.566000, and val accuracy 0.566000\n",
      "epochs 776 / 2500: loss 1.291062 : training accuracy 0.548600, and val accuracy 0.548600\n",
      "epochs 777 / 2500: loss 1.262163 : training accuracy 0.564800, and val accuracy 0.564800\n",
      "epochs 778 / 2500: loss 1.283528 : training accuracy 0.561800, and val accuracy 0.561800\n",
      "epochs 779 / 2500: loss 1.302340 : training accuracy 0.547500, and val accuracy 0.547500\n",
      "epochs 780 / 2500: loss 1.256420 : training accuracy 0.556100, and val accuracy 0.556100\n",
      "epochs 781 / 2500: loss 1.267238 : training accuracy 0.554600, and val accuracy 0.554600\n",
      "epochs 782 / 2500: loss 1.267043 : training accuracy 0.560100, and val accuracy 0.560100\n",
      "epochs 783 / 2500: loss 1.260512 : training accuracy 0.559100, and val accuracy 0.559100\n",
      "epochs 784 / 2500: loss 1.273458 : training accuracy 0.557800, and val accuracy 0.557800\n",
      "epochs 785 / 2500: loss 1.283634 : training accuracy 0.551500, and val accuracy 0.551500\n",
      "epochs 786 / 2500: loss 1.282171 : training accuracy 0.557100, and val accuracy 0.557100\n",
      "epochs 787 / 2500: loss 1.265997 : training accuracy 0.561500, and val accuracy 0.561500\n",
      "epochs 788 / 2500: loss 1.259918 : training accuracy 0.566100, and val accuracy 0.566100\n",
      "epochs 789 / 2500: loss 1.269026 : training accuracy 0.567000, and val accuracy 0.567000\n",
      "epochs 790 / 2500: loss 1.283758 : training accuracy 0.549400, and val accuracy 0.549400\n",
      "epochs 791 / 2500: loss 1.277248 : training accuracy 0.555800, and val accuracy 0.555800\n",
      "epochs 792 / 2500: loss 1.261823 : training accuracy 0.567600, and val accuracy 0.567600\n",
      "epochs 793 / 2500: loss 1.262746 : training accuracy 0.560900, and val accuracy 0.560900\n",
      "epochs 794 / 2500: loss 1.268266 : training accuracy 0.547600, and val accuracy 0.547600\n",
      "epochs 795 / 2500: loss 1.264217 : training accuracy 0.549700, and val accuracy 0.549700\n",
      "epochs 796 / 2500: loss 1.252498 : training accuracy 0.557300, and val accuracy 0.557300\n",
      "epochs 797 / 2500: loss 1.290491 : training accuracy 0.541400, and val accuracy 0.541400\n",
      "epochs 798 / 2500: loss 1.257030 : training accuracy 0.566600, and val accuracy 0.566600\n",
      "epochs 799 / 2500: loss 1.246576 : training accuracy 0.567600, and val accuracy 0.567600\n",
      "epochs 800 / 2500: loss 1.263261 : training accuracy 0.556200, and val accuracy 0.556200\n",
      "epochs 801 / 2500: loss 1.244893 : training accuracy 0.565100, and val accuracy 0.565100\n",
      "epochs 802 / 2500: loss 1.252767 : training accuracy 0.560800, and val accuracy 0.560800\n",
      "epochs 803 / 2500: loss 1.280286 : training accuracy 0.565500, and val accuracy 0.565500\n",
      "epochs 804 / 2500: loss 1.264961 : training accuracy 0.562800, and val accuracy 0.562800\n",
      "epochs 805 / 2500: loss 1.266338 : training accuracy 0.553400, and val accuracy 0.553400\n",
      "epochs 806 / 2500: loss 1.254231 : training accuracy 0.563200, and val accuracy 0.563200\n",
      "epochs 807 / 2500: loss 1.240373 : training accuracy 0.570100, and val accuracy 0.570100\n",
      "epochs 808 / 2500: loss 1.275651 : training accuracy 0.551200, and val accuracy 0.551200\n",
      "epochs 809 / 2500: loss 1.252517 : training accuracy 0.547200, and val accuracy 0.547200\n",
      "epochs 810 / 2500: loss 1.237631 : training accuracy 0.552200, and val accuracy 0.552200\n",
      "epochs 811 / 2500: loss 1.252808 : training accuracy 0.555300, and val accuracy 0.555300\n",
      "epochs 812 / 2500: loss 1.270656 : training accuracy 0.564700, and val accuracy 0.564700\n",
      "epochs 813 / 2500: loss 1.289253 : training accuracy 0.554000, and val accuracy 0.554000\n",
      "epochs 814 / 2500: loss 1.235935 : training accuracy 0.565800, and val accuracy 0.565800\n",
      "epochs 815 / 2500: loss 1.238555 : training accuracy 0.565400, and val accuracy 0.565400\n",
      "epochs 816 / 2500: loss 1.255341 : training accuracy 0.564000, and val accuracy 0.564000\n",
      "epochs 817 / 2500: loss 1.247967 : training accuracy 0.560400, and val accuracy 0.560400\n",
      "epochs 818 / 2500: loss 1.254192 : training accuracy 0.553600, and val accuracy 0.553600\n",
      "epochs 819 / 2500: loss 1.261732 : training accuracy 0.556600, and val accuracy 0.556600\n",
      "epochs 820 / 2500: loss 1.242611 : training accuracy 0.566700, and val accuracy 0.566700\n",
      "epochs 821 / 2500: loss 1.236637 : training accuracy 0.573400, and val accuracy 0.573400\n",
      "epochs 822 / 2500: loss 1.278704 : training accuracy 0.558600, and val accuracy 0.558600\n",
      "epochs 823 / 2500: loss 1.231543 : training accuracy 0.577100, and val accuracy 0.577100\n",
      "epochs 824 / 2500: loss 1.260908 : training accuracy 0.558500, and val accuracy 0.558500\n",
      "epochs 825 / 2500: loss 1.252083 : training accuracy 0.565700, and val accuracy 0.565700\n",
      "epochs 826 / 2500: loss 1.237437 : training accuracy 0.569800, and val accuracy 0.569800\n",
      "epochs 827 / 2500: loss 1.243412 : training accuracy 0.568500, and val accuracy 0.568500\n",
      "epochs 828 / 2500: loss 1.242000 : training accuracy 0.566900, and val accuracy 0.566900\n",
      "epochs 829 / 2500: loss 1.229600 : training accuracy 0.569400, and val accuracy 0.569400\n",
      "epochs 830 / 2500: loss 1.252776 : training accuracy 0.568900, and val accuracy 0.568900\n",
      "epochs 831 / 2500: loss 1.266012 : training accuracy 0.565800, and val accuracy 0.565800\n",
      "epochs 832 / 2500: loss 1.259786 : training accuracy 0.553800, and val accuracy 0.553800\n",
      "epochs 833 / 2500: loss 1.252259 : training accuracy 0.551800, and val accuracy 0.551800\n",
      "epochs 834 / 2500: loss 1.252220 : training accuracy 0.555400, and val accuracy 0.555400\n",
      "epochs 835 / 2500: loss 1.242201 : training accuracy 0.560100, and val accuracy 0.560100\n",
      "epochs 836 / 2500: loss 1.243510 : training accuracy 0.563500, and val accuracy 0.563500\n",
      "epochs 837 / 2500: loss 1.232330 : training accuracy 0.568300, and val accuracy 0.568300\n",
      "epochs 838 / 2500: loss 1.226045 : training accuracy 0.571700, and val accuracy 0.571700\n",
      "epochs 839 / 2500: loss 1.216270 : training accuracy 0.568900, and val accuracy 0.568900\n",
      "epochs 840 / 2500: loss 1.228414 : training accuracy 0.565200, and val accuracy 0.565200\n",
      "epochs 841 / 2500: loss 1.238259 : training accuracy 0.555700, and val accuracy 0.555700\n",
      "epochs 842 / 2500: loss 1.220271 : training accuracy 0.570600, and val accuracy 0.570600\n",
      "epochs 843 / 2500: loss 1.230364 : training accuracy 0.564700, and val accuracy 0.564700\n",
      "epochs 844 / 2500: loss 1.261913 : training accuracy 0.559300, and val accuracy 0.559300\n",
      "epochs 845 / 2500: loss 1.247116 : training accuracy 0.560800, and val accuracy 0.560800\n",
      "epochs 846 / 2500: loss 1.258551 : training accuracy 0.561800, and val accuracy 0.561800\n",
      "epochs 847 / 2500: loss 1.250909 : training accuracy 0.569500, and val accuracy 0.569500\n",
      "epochs 848 / 2500: loss 1.234968 : training accuracy 0.568500, and val accuracy 0.568500\n",
      "epochs 849 / 2500: loss 1.225517 : training accuracy 0.573300, and val accuracy 0.573300\n",
      "epochs 850 / 2500: loss 1.234920 : training accuracy 0.561000, and val accuracy 0.561000\n",
      "epochs 851 / 2500: loss 1.220997 : training accuracy 0.575500, and val accuracy 0.575500\n",
      "epochs 852 / 2500: loss 1.228376 : training accuracy 0.569200, and val accuracy 0.569200\n",
      "epochs 853 / 2500: loss 1.230205 : training accuracy 0.571500, and val accuracy 0.571500\n",
      "epochs 854 / 2500: loss 1.257042 : training accuracy 0.567900, and val accuracy 0.567900\n",
      "epochs 855 / 2500: loss 1.284429 : training accuracy 0.551400, and val accuracy 0.551400\n",
      "epochs 856 / 2500: loss 1.241993 : training accuracy 0.566900, and val accuracy 0.566900\n",
      "epochs 857 / 2500: loss 1.229507 : training accuracy 0.580100, and val accuracy 0.580100\n",
      "epochs 858 / 2500: loss 1.256839 : training accuracy 0.577200, and val accuracy 0.577200\n",
      "epochs 859 / 2500: loss 1.230180 : training accuracy 0.578500, and val accuracy 0.578500\n",
      "epochs 860 / 2500: loss 1.258077 : training accuracy 0.545000, and val accuracy 0.545000\n",
      "epochs 861 / 2500: loss 1.236301 : training accuracy 0.569000, and val accuracy 0.569000\n",
      "epochs 862 / 2500: loss 1.225927 : training accuracy 0.572500, and val accuracy 0.572500\n",
      "epochs 863 / 2500: loss 1.219061 : training accuracy 0.572400, and val accuracy 0.572400\n",
      "epochs 864 / 2500: loss 1.212161 : training accuracy 0.575500, and val accuracy 0.575500\n",
      "epochs 865 / 2500: loss 1.234570 : training accuracy 0.564100, and val accuracy 0.564100\n",
      "epochs 866 / 2500: loss 1.242932 : training accuracy 0.564400, and val accuracy 0.564400\n",
      "epochs 867 / 2500: loss 1.230978 : training accuracy 0.576600, and val accuracy 0.576600\n",
      "epochs 868 / 2500: loss 1.293643 : training accuracy 0.564900, and val accuracy 0.564900\n",
      "epochs 869 / 2500: loss 1.242411 : training accuracy 0.576100, and val accuracy 0.576100\n",
      "epochs 870 / 2500: loss 1.217510 : training accuracy 0.580600, and val accuracy 0.580600\n",
      "epochs 871 / 2500: loss 1.246576 : training accuracy 0.569800, and val accuracy 0.569800\n",
      "epochs 872 / 2500: loss 1.233740 : training accuracy 0.567800, and val accuracy 0.567800\n",
      "epochs 873 / 2500: loss 1.224472 : training accuracy 0.569200, and val accuracy 0.569200\n",
      "epochs 874 / 2500: loss 1.231767 : training accuracy 0.564100, and val accuracy 0.564100\n",
      "epochs 875 / 2500: loss 1.210901 : training accuracy 0.584300, and val accuracy 0.584300\n",
      "epochs 876 / 2500: loss 1.223654 : training accuracy 0.575200, and val accuracy 0.575200\n",
      "epochs 877 / 2500: loss 1.214627 : training accuracy 0.577700, and val accuracy 0.577700\n",
      "epochs 878 / 2500: loss 1.205804 : training accuracy 0.582300, and val accuracy 0.582300\n",
      "epochs 879 / 2500: loss 1.202292 : training accuracy 0.585800, and val accuracy 0.585800\n",
      "epochs 880 / 2500: loss 1.209207 : training accuracy 0.579200, and val accuracy 0.579200\n",
      "epochs 881 / 2500: loss 1.215049 : training accuracy 0.577400, and val accuracy 0.577400\n",
      "epochs 882 / 2500: loss 1.258547 : training accuracy 0.580200, and val accuracy 0.580200\n",
      "epochs 883 / 2500: loss 1.238627 : training accuracy 0.572000, and val accuracy 0.572000\n",
      "epochs 884 / 2500: loss 1.240934 : training accuracy 0.569500, and val accuracy 0.569500\n",
      "epochs 885 / 2500: loss 1.226964 : training accuracy 0.571000, and val accuracy 0.571000\n",
      "epochs 886 / 2500: loss 1.203534 : training accuracy 0.578100, and val accuracy 0.578100\n",
      "epochs 887 / 2500: loss 1.200259 : training accuracy 0.581500, and val accuracy 0.581500\n",
      "epochs 888 / 2500: loss 1.213458 : training accuracy 0.582400, and val accuracy 0.582400\n",
      "epochs 889 / 2500: loss 1.235964 : training accuracy 0.569500, and val accuracy 0.569500\n",
      "epochs 890 / 2500: loss 1.197198 : training accuracy 0.572900, and val accuracy 0.572900\n",
      "epochs 891 / 2500: loss 1.202661 : training accuracy 0.567100, and val accuracy 0.567100\n",
      "epochs 892 / 2500: loss 1.225549 : training accuracy 0.565700, and val accuracy 0.565700\n",
      "epochs 893 / 2500: loss 1.216678 : training accuracy 0.567000, and val accuracy 0.567000\n",
      "epochs 894 / 2500: loss 1.222779 : training accuracy 0.564600, and val accuracy 0.564600\n",
      "epochs 895 / 2500: loss 1.206681 : training accuracy 0.580100, and val accuracy 0.580100\n",
      "epochs 896 / 2500: loss 1.199700 : training accuracy 0.579700, and val accuracy 0.579700\n",
      "epochs 897 / 2500: loss 1.230479 : training accuracy 0.569600, and val accuracy 0.569600\n",
      "epochs 898 / 2500: loss 1.220924 : training accuracy 0.575600, and val accuracy 0.575600\n",
      "epochs 899 / 2500: loss 1.214484 : training accuracy 0.581200, and val accuracy 0.581200\n",
      "epochs 900 / 2500: loss 1.205031 : training accuracy 0.586300, and val accuracy 0.586300\n",
      "epochs 901 / 2500: loss 1.202334 : training accuracy 0.585200, and val accuracy 0.585200\n",
      "epochs 902 / 2500: loss 1.253558 : training accuracy 0.558400, and val accuracy 0.558400\n",
      "epochs 903 / 2500: loss 1.271130 : training accuracy 0.561400, and val accuracy 0.561400\n",
      "epochs 904 / 2500: loss 1.249976 : training accuracy 0.574100, and val accuracy 0.574100\n",
      "epochs 905 / 2500: loss 1.209488 : training accuracy 0.585300, and val accuracy 0.585300\n",
      "epochs 906 / 2500: loss 1.259195 : training accuracy 0.560700, and val accuracy 0.560700\n",
      "epochs 907 / 2500: loss 1.213997 : training accuracy 0.572100, and val accuracy 0.572100\n",
      "epochs 908 / 2500: loss 1.224529 : training accuracy 0.560300, and val accuracy 0.560300\n",
      "epochs 909 / 2500: loss 1.202027 : training accuracy 0.574800, and val accuracy 0.574800\n",
      "epochs 910 / 2500: loss 1.192825 : training accuracy 0.581000, and val accuracy 0.581000\n",
      "epochs 911 / 2500: loss 1.191936 : training accuracy 0.582500, and val accuracy 0.582500\n",
      "epochs 912 / 2500: loss 1.206200 : training accuracy 0.590100, and val accuracy 0.590100\n",
      "epochs 913 / 2500: loss 1.195055 : training accuracy 0.584100, and val accuracy 0.584100\n",
      "epochs 914 / 2500: loss 1.204498 : training accuracy 0.590400, and val accuracy 0.590400\n",
      "epochs 915 / 2500: loss 1.196298 : training accuracy 0.581500, and val accuracy 0.581500\n",
      "epochs 916 / 2500: loss 1.204036 : training accuracy 0.574400, and val accuracy 0.574400\n",
      "epochs 917 / 2500: loss 1.198302 : training accuracy 0.583100, and val accuracy 0.583100\n",
      "epochs 918 / 2500: loss 1.220907 : training accuracy 0.573500, and val accuracy 0.573500\n",
      "epochs 919 / 2500: loss 1.202492 : training accuracy 0.579900, and val accuracy 0.579900\n",
      "epochs 920 / 2500: loss 1.231665 : training accuracy 0.581000, and val accuracy 0.581000\n",
      "epochs 921 / 2500: loss 1.204799 : training accuracy 0.587100, and val accuracy 0.587100\n",
      "epochs 922 / 2500: loss 1.197052 : training accuracy 0.592700, and val accuracy 0.592700\n",
      "epochs 923 / 2500: loss 1.202529 : training accuracy 0.592400, and val accuracy 0.592400\n",
      "epochs 924 / 2500: loss 1.245825 : training accuracy 0.569400, and val accuracy 0.569400\n",
      "epochs 925 / 2500: loss 1.241613 : training accuracy 0.579000, and val accuracy 0.579000\n",
      "epochs 926 / 2500: loss 1.237958 : training accuracy 0.573500, and val accuracy 0.573500\n",
      "epochs 927 / 2500: loss 1.195554 : training accuracy 0.579600, and val accuracy 0.579600\n",
      "epochs 928 / 2500: loss 1.205723 : training accuracy 0.579800, and val accuracy 0.579800\n",
      "epochs 929 / 2500: loss 1.196913 : training accuracy 0.585700, and val accuracy 0.585700\n",
      "epochs 930 / 2500: loss 1.182798 : training accuracy 0.590200, and val accuracy 0.590200\n",
      "epochs 931 / 2500: loss 1.179044 : training accuracy 0.590800, and val accuracy 0.590800\n",
      "epochs 932 / 2500: loss 1.201486 : training accuracy 0.588600, and val accuracy 0.588600\n",
      "epochs 933 / 2500: loss 1.246866 : training accuracy 0.576600, and val accuracy 0.576600\n",
      "epochs 934 / 2500: loss 1.208199 : training accuracy 0.578400, and val accuracy 0.578400\n",
      "epochs 935 / 2500: loss 1.196163 : training accuracy 0.587300, and val accuracy 0.587300\n",
      "epochs 936 / 2500: loss 1.188415 : training accuracy 0.590600, and val accuracy 0.590600\n",
      "epochs 937 / 2500: loss 1.189427 : training accuracy 0.579500, and val accuracy 0.579500\n",
      "epochs 938 / 2500: loss 1.180093 : training accuracy 0.573600, and val accuracy 0.573600\n",
      "epochs 939 / 2500: loss 1.201489 : training accuracy 0.575600, and val accuracy 0.575600\n",
      "epochs 940 / 2500: loss 1.212959 : training accuracy 0.590400, and val accuracy 0.590400\n",
      "epochs 941 / 2500: loss 1.197479 : training accuracy 0.593600, and val accuracy 0.593600\n",
      "epochs 942 / 2500: loss 1.204041 : training accuracy 0.575900, and val accuracy 0.575900\n",
      "epochs 943 / 2500: loss 1.218823 : training accuracy 0.566800, and val accuracy 0.566800\n",
      "epochs 944 / 2500: loss 1.205979 : training accuracy 0.578000, and val accuracy 0.578000\n",
      "epochs 945 / 2500: loss 1.189998 : training accuracy 0.589500, and val accuracy 0.589500\n",
      "epochs 946 / 2500: loss 1.177853 : training accuracy 0.591400, and val accuracy 0.591400\n",
      "epochs 947 / 2500: loss 1.210730 : training accuracy 0.562900, and val accuracy 0.562900\n",
      "epochs 948 / 2500: loss 1.231109 : training accuracy 0.565100, and val accuracy 0.565100\n",
      "epochs 949 / 2500: loss 1.195828 : training accuracy 0.570800, and val accuracy 0.570800\n",
      "epochs 950 / 2500: loss 1.196100 : training accuracy 0.576300, and val accuracy 0.576300\n",
      "epochs 951 / 2500: loss 1.179769 : training accuracy 0.591500, and val accuracy 0.591500\n",
      "epochs 952 / 2500: loss 1.197726 : training accuracy 0.570700, and val accuracy 0.570700\n",
      "epochs 953 / 2500: loss 1.202820 : training accuracy 0.570700, and val accuracy 0.570700\n",
      "epochs 954 / 2500: loss 1.184415 : training accuracy 0.581300, and val accuracy 0.581300\n",
      "epochs 955 / 2500: loss 1.184813 : training accuracy 0.576600, and val accuracy 0.576600\n",
      "epochs 956 / 2500: loss 1.201860 : training accuracy 0.569200, and val accuracy 0.569200\n",
      "epochs 957 / 2500: loss 1.188546 : training accuracy 0.583100, and val accuracy 0.583100\n",
      "epochs 958 / 2500: loss 1.183169 : training accuracy 0.583100, and val accuracy 0.583100\n",
      "epochs 959 / 2500: loss 1.203142 : training accuracy 0.579100, and val accuracy 0.579100\n",
      "epochs 960 / 2500: loss 1.243896 : training accuracy 0.575700, and val accuracy 0.575700\n",
      "epochs 961 / 2500: loss 1.227876 : training accuracy 0.576300, and val accuracy 0.576300\n",
      "epochs 962 / 2500: loss 1.206176 : training accuracy 0.583900, and val accuracy 0.583900\n",
      "epochs 963 / 2500: loss 1.177936 : training accuracy 0.588000, and val accuracy 0.588000\n",
      "epochs 964 / 2500: loss 1.184857 : training accuracy 0.581700, and val accuracy 0.581700\n",
      "epochs 965 / 2500: loss 1.179355 : training accuracy 0.588400, and val accuracy 0.588400\n",
      "epochs 966 / 2500: loss 1.177437 : training accuracy 0.588500, and val accuracy 0.588500\n",
      "epochs 967 / 2500: loss 1.194601 : training accuracy 0.594100, and val accuracy 0.594100\n",
      "epochs 968 / 2500: loss 1.201014 : training accuracy 0.580100, and val accuracy 0.580100\n",
      "epochs 969 / 2500: loss 1.191869 : training accuracy 0.592700, and val accuracy 0.592700\n",
      "epochs 970 / 2500: loss 1.177245 : training accuracy 0.594300, and val accuracy 0.594300\n",
      "epochs 971 / 2500: loss 1.171241 : training accuracy 0.592900, and val accuracy 0.592900\n",
      "epochs 972 / 2500: loss 1.197268 : training accuracy 0.587900, and val accuracy 0.587900\n",
      "epochs 973 / 2500: loss 1.203118 : training accuracy 0.569500, and val accuracy 0.569500\n",
      "epochs 974 / 2500: loss 1.170768 : training accuracy 0.583300, and val accuracy 0.583300\n",
      "epochs 975 / 2500: loss 1.173661 : training accuracy 0.583800, and val accuracy 0.583800\n",
      "epochs 976 / 2500: loss 1.203601 : training accuracy 0.576000, and val accuracy 0.576000\n",
      "epochs 977 / 2500: loss 1.221668 : training accuracy 0.593500, and val accuracy 0.593500\n",
      "epochs 978 / 2500: loss 1.208249 : training accuracy 0.593800, and val accuracy 0.593800\n",
      "epochs 979 / 2500: loss 1.197642 : training accuracy 0.588800, and val accuracy 0.588800\n",
      "epochs 980 / 2500: loss 1.171879 : training accuracy 0.590600, and val accuracy 0.590600\n",
      "epochs 981 / 2500: loss 1.191693 : training accuracy 0.590300, and val accuracy 0.590300\n",
      "epochs 982 / 2500: loss 1.172468 : training accuracy 0.595400, and val accuracy 0.595400\n",
      "epochs 983 / 2500: loss 1.165844 : training accuracy 0.600000, and val accuracy 0.600000\n",
      "epochs 984 / 2500: loss 1.150654 : training accuracy 0.607400, and val accuracy 0.607400\n",
      "epochs 985 / 2500: loss 1.234756 : training accuracy 0.578000, and val accuracy 0.578000\n",
      "epochs 986 / 2500: loss 1.217815 : training accuracy 0.573400, and val accuracy 0.573400\n",
      "epochs 987 / 2500: loss 1.198467 : training accuracy 0.589400, and val accuracy 0.589400\n",
      "epochs 988 / 2500: loss 1.190771 : training accuracy 0.587100, and val accuracy 0.587100\n",
      "epochs 989 / 2500: loss 1.179552 : training accuracy 0.578500, and val accuracy 0.578500\n",
      "epochs 990 / 2500: loss 1.155164 : training accuracy 0.586300, and val accuracy 0.586300\n",
      "epochs 991 / 2500: loss 1.250390 : training accuracy 0.560000, and val accuracy 0.560000\n",
      "epochs 992 / 2500: loss 1.210432 : training accuracy 0.571500, and val accuracy 0.571500\n",
      "epochs 993 / 2500: loss 1.188135 : training accuracy 0.584300, and val accuracy 0.584300\n",
      "epochs 994 / 2500: loss 1.171417 : training accuracy 0.591500, and val accuracy 0.591500\n",
      "epochs 995 / 2500: loss 1.212380 : training accuracy 0.581700, and val accuracy 0.581700\n",
      "epochs 996 / 2500: loss 1.207036 : training accuracy 0.586900, and val accuracy 0.586900\n",
      "epochs 997 / 2500: loss 1.181669 : training accuracy 0.600100, and val accuracy 0.600100\n",
      "epochs 998 / 2500: loss 1.194096 : training accuracy 0.582100, and val accuracy 0.582100\n",
      "epochs 999 / 2500: loss 1.166542 : training accuracy 0.588000, and val accuracy 0.588000\n",
      "epochs 1000 / 2500: loss 1.154224 : training accuracy 0.595400, and val accuracy 0.595400\n",
      "epochs 1001 / 2500: loss 1.176560 : training accuracy 0.591700, and val accuracy 0.591700\n",
      "epochs 1002 / 2500: loss 1.170044 : training accuracy 0.599200, and val accuracy 0.599200\n",
      "epochs 1003 / 2500: loss 1.177944 : training accuracy 0.584400, and val accuracy 0.584400\n",
      "epochs 1004 / 2500: loss 1.161138 : training accuracy 0.578500, and val accuracy 0.578500\n",
      "epochs 1005 / 2500: loss 1.147396 : training accuracy 0.589200, and val accuracy 0.589200\n",
      "epochs 1006 / 2500: loss 1.168303 : training accuracy 0.585800, and val accuracy 0.585800\n",
      "epochs 1007 / 2500: loss 1.178591 : training accuracy 0.581500, and val accuracy 0.581500\n",
      "epochs 1008 / 2500: loss 1.179324 : training accuracy 0.584400, and val accuracy 0.584400\n",
      "epochs 1009 / 2500: loss 1.175947 : training accuracy 0.591300, and val accuracy 0.591300\n",
      "epochs 1010 / 2500: loss 1.175456 : training accuracy 0.601500, and val accuracy 0.601500\n",
      "epochs 1011 / 2500: loss 1.165402 : training accuracy 0.596800, and val accuracy 0.596800\n",
      "epochs 1012 / 2500: loss 1.151740 : training accuracy 0.589700, and val accuracy 0.589700\n",
      "epochs 1013 / 2500: loss 1.156231 : training accuracy 0.583700, and val accuracy 0.583700\n",
      "epochs 1014 / 2500: loss 1.198343 : training accuracy 0.582500, and val accuracy 0.582500\n",
      "epochs 1015 / 2500: loss 1.152321 : training accuracy 0.589500, and val accuracy 0.589500\n",
      "epochs 1016 / 2500: loss 1.151041 : training accuracy 0.594500, and val accuracy 0.594500\n",
      "epochs 1017 / 2500: loss 1.141777 : training accuracy 0.593800, and val accuracy 0.593800\n",
      "epochs 1018 / 2500: loss 1.172994 : training accuracy 0.591800, and val accuracy 0.591800\n",
      "epochs 1019 / 2500: loss 1.149210 : training accuracy 0.614700, and val accuracy 0.614700\n",
      "epochs 1020 / 2500: loss 1.236053 : training accuracy 0.583800, and val accuracy 0.583800\n",
      "epochs 1021 / 2500: loss 1.183038 : training accuracy 0.583800, and val accuracy 0.583800\n",
      "epochs 1022 / 2500: loss 1.155222 : training accuracy 0.593500, and val accuracy 0.593500\n",
      "epochs 1023 / 2500: loss 1.140199 : training accuracy 0.602300, and val accuracy 0.602300\n",
      "epochs 1024 / 2500: loss 1.172328 : training accuracy 0.588400, and val accuracy 0.588400\n",
      "epochs 1025 / 2500: loss 1.190571 : training accuracy 0.586700, and val accuracy 0.586700\n",
      "epochs 1026 / 2500: loss 1.169410 : training accuracy 0.592600, and val accuracy 0.592600\n",
      "epochs 1027 / 2500: loss 1.151446 : training accuracy 0.594900, and val accuracy 0.594900\n",
      "epochs 1028 / 2500: loss 1.164501 : training accuracy 0.573900, and val accuracy 0.573900\n",
      "epochs 1029 / 2500: loss 1.178184 : training accuracy 0.584700, and val accuracy 0.584700\n",
      "epochs 1030 / 2500: loss 1.165531 : training accuracy 0.589500, and val accuracy 0.589500\n",
      "epochs 1031 / 2500: loss 1.157534 : training accuracy 0.593800, and val accuracy 0.593800\n",
      "epochs 1032 / 2500: loss 1.179298 : training accuracy 0.594300, and val accuracy 0.594300\n",
      "epochs 1033 / 2500: loss 1.190816 : training accuracy 0.581100, and val accuracy 0.581100\n",
      "epochs 1034 / 2500: loss 1.167277 : training accuracy 0.593800, and val accuracy 0.593800\n",
      "epochs 1035 / 2500: loss 1.168258 : training accuracy 0.591200, and val accuracy 0.591200\n",
      "epochs 1036 / 2500: loss 1.165324 : training accuracy 0.594200, and val accuracy 0.594200\n",
      "epochs 1037 / 2500: loss 1.174829 : training accuracy 0.582700, and val accuracy 0.582700\n",
      "epochs 1038 / 2500: loss 1.166259 : training accuracy 0.584100, and val accuracy 0.584100\n",
      "epochs 1039 / 2500: loss 1.160197 : training accuracy 0.593200, and val accuracy 0.593200\n",
      "epochs 1040 / 2500: loss 1.148025 : training accuracy 0.593500, and val accuracy 0.593500\n",
      "epochs 1041 / 2500: loss 1.127335 : training accuracy 0.594700, and val accuracy 0.594700\n",
      "epochs 1042 / 2500: loss 1.139675 : training accuracy 0.593000, and val accuracy 0.593000\n",
      "epochs 1043 / 2500: loss 1.167391 : training accuracy 0.591400, and val accuracy 0.591400\n",
      "epochs 1044 / 2500: loss 1.151167 : training accuracy 0.602800, and val accuracy 0.602800\n",
      "epochs 1045 / 2500: loss 1.151603 : training accuracy 0.611100, and val accuracy 0.611100\n",
      "epochs 1046 / 2500: loss 1.147277 : training accuracy 0.602400, and val accuracy 0.602400\n",
      "epochs 1047 / 2500: loss 1.132970 : training accuracy 0.603300, and val accuracy 0.603300\n",
      "epochs 1048 / 2500: loss 1.181244 : training accuracy 0.592300, and val accuracy 0.592300\n",
      "epochs 1049 / 2500: loss 1.153594 : training accuracy 0.605400, and val accuracy 0.605400\n",
      "epochs 1050 / 2500: loss 1.164349 : training accuracy 0.595700, and val accuracy 0.595700\n",
      "epochs 1051 / 2500: loss 1.147362 : training accuracy 0.605700, and val accuracy 0.605700\n",
      "epochs 1052 / 2500: loss 1.144224 : training accuracy 0.606900, and val accuracy 0.606900\n",
      "epochs 1053 / 2500: loss 1.144131 : training accuracy 0.601800, and val accuracy 0.601800\n",
      "epochs 1054 / 2500: loss 1.158921 : training accuracy 0.593300, and val accuracy 0.593300\n",
      "epochs 1055 / 2500: loss 1.149654 : training accuracy 0.589300, and val accuracy 0.589300\n",
      "epochs 1056 / 2500: loss 1.151898 : training accuracy 0.592100, and val accuracy 0.592100\n",
      "epochs 1057 / 2500: loss 1.148737 : training accuracy 0.589300, and val accuracy 0.589300\n",
      "epochs 1058 / 2500: loss 1.128542 : training accuracy 0.598000, and val accuracy 0.598000\n",
      "epochs 1059 / 2500: loss 1.191916 : training accuracy 0.576400, and val accuracy 0.576400\n",
      "epochs 1060 / 2500: loss 1.147195 : training accuracy 0.602100, and val accuracy 0.602100\n",
      "epochs 1061 / 2500: loss 1.138145 : training accuracy 0.606100, and val accuracy 0.606100\n",
      "epochs 1062 / 2500: loss 1.132374 : training accuracy 0.611600, and val accuracy 0.611600\n",
      "epochs 1063 / 2500: loss 1.166010 : training accuracy 0.584700, and val accuracy 0.584700\n",
      "epochs 1064 / 2500: loss 1.159728 : training accuracy 0.586600, and val accuracy 0.586600\n",
      "epochs 1065 / 2500: loss 1.152660 : training accuracy 0.599600, and val accuracy 0.599600\n",
      "epochs 1066 / 2500: loss 1.161493 : training accuracy 0.590700, and val accuracy 0.590700\n",
      "epochs 1067 / 2500: loss 1.201698 : training accuracy 0.586000, and val accuracy 0.586000\n",
      "epochs 1068 / 2500: loss 1.166828 : training accuracy 0.600400, and val accuracy 0.600400\n",
      "epochs 1069 / 2500: loss 1.136950 : training accuracy 0.603700, and val accuracy 0.603700\n",
      "epochs 1070 / 2500: loss 1.106977 : training accuracy 0.602800, and val accuracy 0.602800\n",
      "epochs 1071 / 2500: loss 1.138556 : training accuracy 0.600300, and val accuracy 0.600300\n",
      "epochs 1072 / 2500: loss 1.158263 : training accuracy 0.600100, and val accuracy 0.600100\n",
      "epochs 1073 / 2500: loss 1.135694 : training accuracy 0.615700, and val accuracy 0.615700\n",
      "epochs 1074 / 2500: loss 1.180098 : training accuracy 0.600200, and val accuracy 0.600200\n",
      "epochs 1075 / 2500: loss 1.159634 : training accuracy 0.602400, and val accuracy 0.602400\n",
      "epochs 1076 / 2500: loss 1.163221 : training accuracy 0.601700, and val accuracy 0.601700\n",
      "epochs 1077 / 2500: loss 1.146406 : training accuracy 0.607700, and val accuracy 0.607700\n",
      "epochs 1078 / 2500: loss 1.144505 : training accuracy 0.611300, and val accuracy 0.611300\n",
      "epochs 1079 / 2500: loss 1.166797 : training accuracy 0.597100, and val accuracy 0.597100\n",
      "epochs 1080 / 2500: loss 1.178773 : training accuracy 0.589700, and val accuracy 0.589700\n",
      "epochs 1081 / 2500: loss 1.136152 : training accuracy 0.604800, and val accuracy 0.604800\n",
      "epochs 1082 / 2500: loss 1.126825 : training accuracy 0.606100, and val accuracy 0.606100\n",
      "epochs 1083 / 2500: loss 1.162920 : training accuracy 0.594800, and val accuracy 0.594800\n",
      "epochs 1084 / 2500: loss 1.145428 : training accuracy 0.592300, and val accuracy 0.592300\n",
      "epochs 1085 / 2500: loss 1.195150 : training accuracy 0.600100, and val accuracy 0.600100\n",
      "epochs 1086 / 2500: loss 1.152708 : training accuracy 0.599800, and val accuracy 0.599800\n",
      "epochs 1087 / 2500: loss 1.133118 : training accuracy 0.614000, and val accuracy 0.614000\n",
      "epochs 1088 / 2500: loss 1.125885 : training accuracy 0.615000, and val accuracy 0.615000\n",
      "epochs 1089 / 2500: loss 1.118960 : training accuracy 0.615800, and val accuracy 0.615800\n",
      "epochs 1090 / 2500: loss 1.118155 : training accuracy 0.601500, and val accuracy 0.601500\n",
      "epochs 1091 / 2500: loss 1.110007 : training accuracy 0.605700, and val accuracy 0.605700\n",
      "epochs 1092 / 2500: loss 1.112433 : training accuracy 0.605000, and val accuracy 0.605000\n",
      "epochs 1093 / 2500: loss 1.120492 : training accuracy 0.600800, and val accuracy 0.600800\n",
      "epochs 1094 / 2500: loss 1.120620 : training accuracy 0.599600, and val accuracy 0.599600\n",
      "epochs 1095 / 2500: loss 1.104064 : training accuracy 0.602800, and val accuracy 0.602800\n",
      "epochs 1096 / 2500: loss 1.199839 : training accuracy 0.587800, and val accuracy 0.587800\n",
      "epochs 1097 / 2500: loss 1.138114 : training accuracy 0.597200, and val accuracy 0.597200\n",
      "epochs 1098 / 2500: loss 1.119985 : training accuracy 0.603400, and val accuracy 0.603400\n",
      "epochs 1099 / 2500: loss 1.116598 : training accuracy 0.617500, and val accuracy 0.617500\n",
      "epochs 1100 / 2500: loss 1.112203 : training accuracy 0.621900, and val accuracy 0.621900\n",
      "epochs 1101 / 2500: loss 1.156910 : training accuracy 0.599400, and val accuracy 0.599400\n",
      "epochs 1102 / 2500: loss 1.196956 : training accuracy 0.606200, and val accuracy 0.606200\n",
      "epochs 1103 / 2500: loss 1.172476 : training accuracy 0.601800, and val accuracy 0.601800\n",
      "epochs 1104 / 2500: loss 1.156548 : training accuracy 0.604200, and val accuracy 0.604200\n",
      "epochs 1105 / 2500: loss 1.141491 : training accuracy 0.605400, and val accuracy 0.605400\n",
      "epochs 1106 / 2500: loss 1.154688 : training accuracy 0.606900, and val accuracy 0.606900\n",
      "epochs 1107 / 2500: loss 1.145378 : training accuracy 0.607000, and val accuracy 0.607000\n",
      "epochs 1108 / 2500: loss 1.161081 : training accuracy 0.594100, and val accuracy 0.594100\n",
      "epochs 1109 / 2500: loss 1.125408 : training accuracy 0.606700, and val accuracy 0.606700\n",
      "epochs 1110 / 2500: loss 1.123125 : training accuracy 0.601000, and val accuracy 0.601000\n",
      "epochs 1111 / 2500: loss 1.118553 : training accuracy 0.599900, and val accuracy 0.599900\n",
      "epochs 1112 / 2500: loss 1.120903 : training accuracy 0.595100, and val accuracy 0.595100\n",
      "epochs 1113 / 2500: loss 1.109233 : training accuracy 0.595500, and val accuracy 0.595500\n",
      "epochs 1114 / 2500: loss 1.116303 : training accuracy 0.609400, and val accuracy 0.609400\n",
      "epochs 1115 / 2500: loss 1.135148 : training accuracy 0.608600, and val accuracy 0.608600\n",
      "epochs 1116 / 2500: loss 1.159603 : training accuracy 0.607400, and val accuracy 0.607400\n",
      "epochs 1117 / 2500: loss 1.127333 : training accuracy 0.619800, and val accuracy 0.619800\n",
      "epochs 1118 / 2500: loss 1.104959 : training accuracy 0.623200, and val accuracy 0.623200\n",
      "epochs 1119 / 2500: loss 1.149273 : training accuracy 0.601200, and val accuracy 0.601200\n",
      "epochs 1120 / 2500: loss 1.126251 : training accuracy 0.608200, and val accuracy 0.608200\n",
      "epochs 1121 / 2500: loss 1.116346 : training accuracy 0.610900, and val accuracy 0.610900\n",
      "epochs 1122 / 2500: loss 1.106037 : training accuracy 0.615200, and val accuracy 0.615200\n",
      "epochs 1123 / 2500: loss 1.140139 : training accuracy 0.603300, and val accuracy 0.603300\n",
      "epochs 1124 / 2500: loss 1.114684 : training accuracy 0.606100, and val accuracy 0.606100\n",
      "epochs 1125 / 2500: loss 1.121694 : training accuracy 0.619600, and val accuracy 0.619600\n",
      "epochs 1126 / 2500: loss 1.130570 : training accuracy 0.605500, and val accuracy 0.605500\n",
      "epochs 1127 / 2500: loss 1.115007 : training accuracy 0.613000, and val accuracy 0.613000\n",
      "epochs 1128 / 2500: loss 1.104290 : training accuracy 0.616000, and val accuracy 0.616000\n",
      "epochs 1129 / 2500: loss 1.096800 : training accuracy 0.618000, and val accuracy 0.618000\n",
      "epochs 1130 / 2500: loss 1.095067 : training accuracy 0.624200, and val accuracy 0.624200\n",
      "epochs 1131 / 2500: loss 1.132954 : training accuracy 0.608800, and val accuracy 0.608800\n",
      "epochs 1132 / 2500: loss 1.138109 : training accuracy 0.593700, and val accuracy 0.593700\n",
      "epochs 1133 / 2500: loss 1.133729 : training accuracy 0.593500, and val accuracy 0.593500\n",
      "epochs 1134 / 2500: loss 1.118924 : training accuracy 0.605100, and val accuracy 0.605100\n",
      "epochs 1135 / 2500: loss 1.110471 : training accuracy 0.608000, and val accuracy 0.608000\n",
      "epochs 1136 / 2500: loss 1.105295 : training accuracy 0.608300, and val accuracy 0.608300\n",
      "epochs 1137 / 2500: loss 1.082967 : training accuracy 0.618200, and val accuracy 0.618200\n",
      "epochs 1138 / 2500: loss 1.097105 : training accuracy 0.619800, and val accuracy 0.619800\n",
      "epochs 1139 / 2500: loss 1.139028 : training accuracy 0.609900, and val accuracy 0.609900\n",
      "epochs 1140 / 2500: loss 1.139174 : training accuracy 0.608700, and val accuracy 0.608700\n",
      "epochs 1141 / 2500: loss 1.162642 : training accuracy 0.608500, and val accuracy 0.608500\n",
      "epochs 1142 / 2500: loss 1.172761 : training accuracy 0.613400, and val accuracy 0.613400\n",
      "epochs 1143 / 2500: loss 1.106963 : training accuracy 0.614800, and val accuracy 0.614800\n",
      "epochs 1144 / 2500: loss 1.099890 : training accuracy 0.618800, and val accuracy 0.618800\n",
      "epochs 1145 / 2500: loss 1.151281 : training accuracy 0.597100, and val accuracy 0.597100\n",
      "epochs 1146 / 2500: loss 1.092302 : training accuracy 0.619600, and val accuracy 0.619600\n",
      "epochs 1147 / 2500: loss 1.085550 : training accuracy 0.628500, and val accuracy 0.628500\n",
      "epochs 1148 / 2500: loss 1.083098 : training accuracy 0.627400, and val accuracy 0.627400\n",
      "epochs 1149 / 2500: loss 1.081003 : training accuracy 0.608200, and val accuracy 0.608200\n",
      "epochs 1150 / 2500: loss 1.098024 : training accuracy 0.603200, and val accuracy 0.603200\n",
      "epochs 1151 / 2500: loss 1.101652 : training accuracy 0.607000, and val accuracy 0.607000\n",
      "epochs 1152 / 2500: loss 1.106002 : training accuracy 0.604200, and val accuracy 0.604200\n",
      "epochs 1153 / 2500: loss 1.123264 : training accuracy 0.602700, and val accuracy 0.602700\n",
      "epochs 1154 / 2500: loss 1.154769 : training accuracy 0.595500, and val accuracy 0.595500\n",
      "epochs 1155 / 2500: loss 1.118945 : training accuracy 0.612400, and val accuracy 0.612400\n",
      "epochs 1156 / 2500: loss 1.103747 : training accuracy 0.626300, and val accuracy 0.626300\n",
      "epochs 1157 / 2500: loss 1.098443 : training accuracy 0.617100, and val accuracy 0.617100\n",
      "epochs 1158 / 2500: loss 1.087509 : training accuracy 0.617400, and val accuracy 0.617400\n",
      "epochs 1159 / 2500: loss 1.114174 : training accuracy 0.613500, and val accuracy 0.613500\n",
      "epochs 1160 / 2500: loss 1.107581 : training accuracy 0.609400, and val accuracy 0.609400\n",
      "epochs 1161 / 2500: loss 1.090130 : training accuracy 0.619300, and val accuracy 0.619300\n",
      "epochs 1162 / 2500: loss 1.087256 : training accuracy 0.622100, and val accuracy 0.622100\n",
      "epochs 1163 / 2500: loss 1.114210 : training accuracy 0.599000, and val accuracy 0.599000\n",
      "epochs 1164 / 2500: loss 1.084726 : training accuracy 0.608700, and val accuracy 0.608700\n",
      "epochs 1165 / 2500: loss 1.073443 : training accuracy 0.621200, and val accuracy 0.621200\n",
      "epochs 1166 / 2500: loss 1.066652 : training accuracy 0.624400, and val accuracy 0.624400\n",
      "epochs 1167 / 2500: loss 1.070735 : training accuracy 0.626000, and val accuracy 0.626000\n",
      "epochs 1168 / 2500: loss 1.147689 : training accuracy 0.591600, and val accuracy 0.591600\n",
      "epochs 1169 / 2500: loss 1.137034 : training accuracy 0.603300, and val accuracy 0.603300\n",
      "epochs 1170 / 2500: loss 1.152784 : training accuracy 0.608300, and val accuracy 0.608300\n",
      "epochs 1171 / 2500: loss 1.118408 : training accuracy 0.608300, and val accuracy 0.608300\n",
      "epochs 1172 / 2500: loss 1.113637 : training accuracy 0.611300, and val accuracy 0.611300\n",
      "epochs 1173 / 2500: loss 1.132955 : training accuracy 0.614600, and val accuracy 0.614600\n",
      "epochs 1174 / 2500: loss 1.138591 : training accuracy 0.603700, and val accuracy 0.603700\n",
      "epochs 1175 / 2500: loss 1.165213 : training accuracy 0.603600, and val accuracy 0.603600\n",
      "epochs 1176 / 2500: loss 1.139684 : training accuracy 0.608600, and val accuracy 0.608600\n",
      "epochs 1177 / 2500: loss 1.137139 : training accuracy 0.589500, and val accuracy 0.589500\n",
      "epochs 1178 / 2500: loss 1.102699 : training accuracy 0.610100, and val accuracy 0.610100\n",
      "epochs 1179 / 2500: loss 1.095310 : training accuracy 0.616200, and val accuracy 0.616200\n",
      "epochs 1180 / 2500: loss 1.091925 : training accuracy 0.634100, and val accuracy 0.634100\n",
      "epochs 1181 / 2500: loss 1.093675 : training accuracy 0.630500, and val accuracy 0.630500\n",
      "epochs 1182 / 2500: loss 1.119864 : training accuracy 0.617200, and val accuracy 0.617200\n",
      "epochs 1183 / 2500: loss 1.121710 : training accuracy 0.624900, and val accuracy 0.624900\n",
      "epochs 1184 / 2500: loss 1.122686 : training accuracy 0.628500, and val accuracy 0.628500\n",
      "epochs 1185 / 2500: loss 1.090107 : training accuracy 0.635900, and val accuracy 0.635900\n",
      "epochs 1186 / 2500: loss 1.081161 : training accuracy 0.619600, and val accuracy 0.619600\n",
      "epochs 1187 / 2500: loss 1.117727 : training accuracy 0.607600, and val accuracy 0.607600\n",
      "epochs 1188 / 2500: loss 1.093356 : training accuracy 0.620000, and val accuracy 0.620000\n",
      "epochs 1189 / 2500: loss 1.107434 : training accuracy 0.610100, and val accuracy 0.610100\n",
      "epochs 1190 / 2500: loss 1.103218 : training accuracy 0.624300, and val accuracy 0.624300\n",
      "epochs 1191 / 2500: loss 1.112485 : training accuracy 0.623400, and val accuracy 0.623400\n",
      "epochs 1192 / 2500: loss 1.142683 : training accuracy 0.622900, and val accuracy 0.622900\n",
      "epochs 1193 / 2500: loss 1.106526 : training accuracy 0.623100, and val accuracy 0.623100\n",
      "epochs 1194 / 2500: loss 1.085398 : training accuracy 0.629800, and val accuracy 0.629800\n",
      "epochs 1195 / 2500: loss 1.069861 : training accuracy 0.628900, and val accuracy 0.628900\n",
      "epochs 1196 / 2500: loss 1.111095 : training accuracy 0.607800, and val accuracy 0.607800\n",
      "epochs 1197 / 2500: loss 1.088865 : training accuracy 0.616300, and val accuracy 0.616300\n",
      "epochs 1198 / 2500: loss 1.100322 : training accuracy 0.615200, and val accuracy 0.615200\n",
      "epochs 1199 / 2500: loss 1.101229 : training accuracy 0.614000, and val accuracy 0.614000\n",
      "epochs 1200 / 2500: loss 1.089529 : training accuracy 0.622100, and val accuracy 0.622100\n",
      "epochs 1201 / 2500: loss 1.122874 : training accuracy 0.609600, and val accuracy 0.609600\n",
      "epochs 1202 / 2500: loss 1.072179 : training accuracy 0.618500, and val accuracy 0.618500\n",
      "epochs 1203 / 2500: loss 1.102569 : training accuracy 0.621500, and val accuracy 0.621500\n",
      "epochs 1204 / 2500: loss 1.132658 : training accuracy 0.585900, and val accuracy 0.585900\n",
      "epochs 1205 / 2500: loss 1.086407 : training accuracy 0.616000, and val accuracy 0.616000\n",
      "epochs 1206 / 2500: loss 1.081681 : training accuracy 0.615000, and val accuracy 0.615000\n",
      "epochs 1207 / 2500: loss 1.076641 : training accuracy 0.620800, and val accuracy 0.620800\n",
      "epochs 1208 / 2500: loss 1.121703 : training accuracy 0.595000, and val accuracy 0.595000\n",
      "epochs 1209 / 2500: loss 1.077795 : training accuracy 0.609600, and val accuracy 0.609600\n",
      "epochs 1210 / 2500: loss 1.080098 : training accuracy 0.617700, and val accuracy 0.617700\n",
      "epochs 1211 / 2500: loss 1.078796 : training accuracy 0.613900, and val accuracy 0.613900\n",
      "epochs 1212 / 2500: loss 1.104458 : training accuracy 0.607500, and val accuracy 0.607500\n",
      "epochs 1213 / 2500: loss 1.085831 : training accuracy 0.619200, and val accuracy 0.619200\n",
      "epochs 1214 / 2500: loss 1.087319 : training accuracy 0.625100, and val accuracy 0.625100\n",
      "epochs 1215 / 2500: loss 1.114365 : training accuracy 0.616500, and val accuracy 0.616500\n",
      "epochs 1216 / 2500: loss 1.104598 : training accuracy 0.614400, and val accuracy 0.614400\n",
      "epochs 1217 / 2500: loss 1.097034 : training accuracy 0.612600, and val accuracy 0.612600\n",
      "epochs 1218 / 2500: loss 1.076863 : training accuracy 0.626300, and val accuracy 0.626300\n",
      "epochs 1219 / 2500: loss 1.060820 : training accuracy 0.628400, and val accuracy 0.628400\n",
      "epochs 1220 / 2500: loss 1.088611 : training accuracy 0.612600, and val accuracy 0.612600\n",
      "epochs 1221 / 2500: loss 1.058860 : training accuracy 0.626000, and val accuracy 0.626000\n",
      "epochs 1222 / 2500: loss 1.074393 : training accuracy 0.620900, and val accuracy 0.620900\n",
      "epochs 1223 / 2500: loss 1.065412 : training accuracy 0.625900, and val accuracy 0.625900\n",
      "epochs 1224 / 2500: loss 1.067279 : training accuracy 0.624800, and val accuracy 0.624800\n",
      "epochs 1225 / 2500: loss 1.114569 : training accuracy 0.612800, and val accuracy 0.612800\n",
      "epochs 1226 / 2500: loss 1.103408 : training accuracy 0.621000, and val accuracy 0.621000\n",
      "epochs 1227 / 2500: loss 1.088160 : training accuracy 0.623000, and val accuracy 0.623000\n",
      "epochs 1228 / 2500: loss 1.089267 : training accuracy 0.639800, and val accuracy 0.639800\n",
      "epochs 1229 / 2500: loss 1.114636 : training accuracy 0.606000, and val accuracy 0.606000\n",
      "epochs 1230 / 2500: loss 1.137761 : training accuracy 0.603700, and val accuracy 0.603700\n",
      "epochs 1231 / 2500: loss 1.094688 : training accuracy 0.618500, and val accuracy 0.618500\n",
      "epochs 1232 / 2500: loss 1.085886 : training accuracy 0.626000, and val accuracy 0.626000\n",
      "epochs 1233 / 2500: loss 1.086688 : training accuracy 0.627500, and val accuracy 0.627500\n",
      "epochs 1234 / 2500: loss 1.079602 : training accuracy 0.630400, and val accuracy 0.630400\n",
      "epochs 1235 / 2500: loss 1.064447 : training accuracy 0.632300, and val accuracy 0.632300\n",
      "epochs 1236 / 2500: loss 1.076479 : training accuracy 0.629300, and val accuracy 0.629300\n",
      "epochs 1237 / 2500: loss 1.071538 : training accuracy 0.636300, and val accuracy 0.636300\n",
      "epochs 1238 / 2500: loss 1.119878 : training accuracy 0.601700, and val accuracy 0.601700\n",
      "epochs 1239 / 2500: loss 1.081873 : training accuracy 0.616900, and val accuracy 0.616900\n",
      "epochs 1240 / 2500: loss 1.066409 : training accuracy 0.628600, and val accuracy 0.628600\n",
      "epochs 1241 / 2500: loss 1.058557 : training accuracy 0.631300, and val accuracy 0.631300\n",
      "epochs 1242 / 2500: loss 1.049352 : training accuracy 0.636100, and val accuracy 0.636100\n",
      "epochs 1243 / 2500: loss 1.099030 : training accuracy 0.628200, and val accuracy 0.628200\n",
      "epochs 1244 / 2500: loss 1.102094 : training accuracy 0.609400, and val accuracy 0.609400\n",
      "epochs 1245 / 2500: loss 1.106008 : training accuracy 0.606400, and val accuracy 0.606400\n",
      "epochs 1246 / 2500: loss 1.082818 : training accuracy 0.615900, and val accuracy 0.615900\n",
      "epochs 1247 / 2500: loss 1.074732 : training accuracy 0.618000, and val accuracy 0.618000\n",
      "epochs 1248 / 2500: loss 1.071464 : training accuracy 0.614400, and val accuracy 0.614400\n",
      "epochs 1249 / 2500: loss 1.077359 : training accuracy 0.621200, and val accuracy 0.621200\n",
      "epochs 1250 / 2500: loss 1.115932 : training accuracy 0.621800, and val accuracy 0.621800\n",
      "epochs 1251 / 2500: loss 1.113076 : training accuracy 0.629100, and val accuracy 0.629100\n",
      "epochs 1252 / 2500: loss 1.110024 : training accuracy 0.617900, and val accuracy 0.617900\n",
      "epochs 1253 / 2500: loss 1.115351 : training accuracy 0.609100, and val accuracy 0.609100\n",
      "epochs 1254 / 2500: loss 1.055962 : training accuracy 0.621800, and val accuracy 0.621800\n",
      "epochs 1255 / 2500: loss 1.061054 : training accuracy 0.626800, and val accuracy 0.626800\n",
      "epochs 1256 / 2500: loss 1.056367 : training accuracy 0.625300, and val accuracy 0.625300\n",
      "epochs 1257 / 2500: loss 1.101003 : training accuracy 0.597200, and val accuracy 0.597200\n",
      "epochs 1258 / 2500: loss 1.101410 : training accuracy 0.612500, and val accuracy 0.612500\n",
      "epochs 1259 / 2500: loss 1.079317 : training accuracy 0.613600, and val accuracy 0.613600\n",
      "epochs 1260 / 2500: loss 1.042681 : training accuracy 0.629400, and val accuracy 0.629400\n",
      "epochs 1261 / 2500: loss 1.027598 : training accuracy 0.642400, and val accuracy 0.642400\n",
      "epochs 1262 / 2500: loss 1.055122 : training accuracy 0.624300, and val accuracy 0.624300\n",
      "epochs 1263 / 2500: loss 1.077892 : training accuracy 0.625800, and val accuracy 0.625800\n",
      "epochs 1264 / 2500: loss 1.052192 : training accuracy 0.631100, and val accuracy 0.631100\n",
      "epochs 1265 / 2500: loss 1.060106 : training accuracy 0.617600, and val accuracy 0.617600\n",
      "epochs 1266 / 2500: loss 1.050752 : training accuracy 0.627000, and val accuracy 0.627000\n",
      "epochs 1267 / 2500: loss 1.069367 : training accuracy 0.626300, and val accuracy 0.626300\n",
      "epochs 1268 / 2500: loss 1.093552 : training accuracy 0.622000, and val accuracy 0.622000\n",
      "epochs 1269 / 2500: loss 1.081000 : training accuracy 0.630500, and val accuracy 0.630500\n",
      "epochs 1270 / 2500: loss 1.079373 : training accuracy 0.627600, and val accuracy 0.627600\n",
      "epochs 1271 / 2500: loss 1.060470 : training accuracy 0.614200, and val accuracy 0.614200\n",
      "epochs 1272 / 2500: loss 1.033079 : training accuracy 0.632000, and val accuracy 0.632000\n",
      "epochs 1273 / 2500: loss 1.070457 : training accuracy 0.616000, and val accuracy 0.616000\n",
      "epochs 1274 / 2500: loss 1.042978 : training accuracy 0.620900, and val accuracy 0.620900\n",
      "epochs 1275 / 2500: loss 1.084511 : training accuracy 0.618300, and val accuracy 0.618300\n",
      "epochs 1276 / 2500: loss 1.128656 : training accuracy 0.629100, and val accuracy 0.629100\n",
      "epochs 1277 / 2500: loss 1.108201 : training accuracy 0.619500, and val accuracy 0.619500\n",
      "epochs 1278 / 2500: loss 1.073945 : training accuracy 0.620900, and val accuracy 0.620900\n",
      "epochs 1279 / 2500: loss 1.038793 : training accuracy 0.635400, and val accuracy 0.635400\n",
      "epochs 1280 / 2500: loss 1.036216 : training accuracy 0.639900, and val accuracy 0.639900\n",
      "epochs 1281 / 2500: loss 1.053535 : training accuracy 0.632600, and val accuracy 0.632600\n",
      "epochs 1282 / 2500: loss 1.043105 : training accuracy 0.636900, and val accuracy 0.636900\n",
      "epochs 1283 / 2500: loss 1.033667 : training accuracy 0.645400, and val accuracy 0.645400\n",
      "epochs 1284 / 2500: loss 1.062988 : training accuracy 0.631700, and val accuracy 0.631700\n",
      "epochs 1285 / 2500: loss 1.103272 : training accuracy 0.621200, and val accuracy 0.621200\n",
      "epochs 1286 / 2500: loss 1.077908 : training accuracy 0.635300, and val accuracy 0.635300\n",
      "epochs 1287 / 2500: loss 1.097154 : training accuracy 0.631500, and val accuracy 0.631500\n",
      "epochs 1288 / 2500: loss 1.064606 : training accuracy 0.637400, and val accuracy 0.637400\n",
      "epochs 1289 / 2500: loss 1.062599 : training accuracy 0.620500, and val accuracy 0.620500\n",
      "epochs 1290 / 2500: loss 1.076747 : training accuracy 0.620700, and val accuracy 0.620700\n",
      "epochs 1291 / 2500: loss 1.066228 : training accuracy 0.620000, and val accuracy 0.620000\n",
      "epochs 1292 / 2500: loss 1.098874 : training accuracy 0.614200, and val accuracy 0.614200\n",
      "epochs 1293 / 2500: loss 1.114365 : training accuracy 0.634500, and val accuracy 0.634500\n",
      "epochs 1294 / 2500: loss 1.114444 : training accuracy 0.623300, and val accuracy 0.623300\n",
      "epochs 1295 / 2500: loss 1.086608 : training accuracy 0.636200, and val accuracy 0.636200\n",
      "epochs 1296 / 2500: loss 1.058366 : training accuracy 0.625900, and val accuracy 0.625900\n",
      "epochs 1297 / 2500: loss 1.030550 : training accuracy 0.638200, and val accuracy 0.638200\n",
      "epochs 1298 / 2500: loss 1.074071 : training accuracy 0.631800, and val accuracy 0.631800\n",
      "epochs 1299 / 2500: loss 1.072348 : training accuracy 0.632300, and val accuracy 0.632300\n",
      "epochs 1300 / 2500: loss 1.067176 : training accuracy 0.636100, and val accuracy 0.636100\n",
      "epochs 1301 / 2500: loss 1.041013 : training accuracy 0.639200, and val accuracy 0.639200\n",
      "epochs 1302 / 2500: loss 1.068102 : training accuracy 0.632800, and val accuracy 0.632800\n",
      "epochs 1303 / 2500: loss 1.045160 : training accuracy 0.639400, and val accuracy 0.639400\n",
      "epochs 1304 / 2500: loss 1.042228 : training accuracy 0.640500, and val accuracy 0.640500\n",
      "epochs 1305 / 2500: loss 1.029806 : training accuracy 0.647200, and val accuracy 0.647200\n",
      "epochs 1306 / 2500: loss 1.028037 : training accuracy 0.646900, and val accuracy 0.646900\n",
      "epochs 1307 / 2500: loss 1.017111 : training accuracy 0.637800, and val accuracy 0.637800\n",
      "epochs 1308 / 2500: loss 1.052092 : training accuracy 0.636100, and val accuracy 0.636100\n",
      "epochs 1309 / 2500: loss 1.087171 : training accuracy 0.639600, and val accuracy 0.639600\n",
      "epochs 1310 / 2500: loss 1.086943 : training accuracy 0.650800, and val accuracy 0.650800\n",
      "epochs 1311 / 2500: loss 1.100473 : training accuracy 0.630000, and val accuracy 0.630000\n",
      "epochs 1312 / 2500: loss 1.063579 : training accuracy 0.634400, and val accuracy 0.634400\n",
      "epochs 1313 / 2500: loss 1.051165 : training accuracy 0.636700, and val accuracy 0.636700\n",
      "epochs 1314 / 2500: loss 1.038130 : training accuracy 0.628500, and val accuracy 0.628500\n",
      "epochs 1315 / 2500: loss 1.101351 : training accuracy 0.611100, and val accuracy 0.611100\n",
      "epochs 1316 / 2500: loss 1.075370 : training accuracy 0.631500, and val accuracy 0.631500\n",
      "epochs 1317 / 2500: loss 1.055066 : training accuracy 0.639800, and val accuracy 0.639800\n",
      "epochs 1318 / 2500: loss 1.052806 : training accuracy 0.639900, and val accuracy 0.639900\n",
      "epochs 1319 / 2500: loss 1.065039 : training accuracy 0.626200, and val accuracy 0.626200\n",
      "epochs 1320 / 2500: loss 1.054865 : training accuracy 0.633000, and val accuracy 0.633000\n",
      "epochs 1321 / 2500: loss 1.082082 : training accuracy 0.613700, and val accuracy 0.613700\n",
      "epochs 1322 / 2500: loss 1.049044 : training accuracy 0.621900, and val accuracy 0.621900\n",
      "epochs 1323 / 2500: loss 1.045019 : training accuracy 0.630600, and val accuracy 0.630600\n",
      "epochs 1324 / 2500: loss 1.029651 : training accuracy 0.636900, and val accuracy 0.636900\n",
      "epochs 1325 / 2500: loss 1.013541 : training accuracy 0.648300, and val accuracy 0.648300\n",
      "epochs 1326 / 2500: loss 1.065138 : training accuracy 0.611800, and val accuracy 0.611800\n",
      "epochs 1327 / 2500: loss 1.070831 : training accuracy 0.609300, and val accuracy 0.609300\n",
      "epochs 1328 / 2500: loss 1.106221 : training accuracy 0.609000, and val accuracy 0.609000\n",
      "epochs 1329 / 2500: loss 1.060809 : training accuracy 0.625300, and val accuracy 0.625300\n",
      "epochs 1330 / 2500: loss 1.047592 : training accuracy 0.625300, and val accuracy 0.625300\n",
      "epochs 1331 / 2500: loss 1.013876 : training accuracy 0.643300, and val accuracy 0.643300\n",
      "epochs 1332 / 2500: loss 1.073042 : training accuracy 0.615200, and val accuracy 0.615200\n",
      "epochs 1333 / 2500: loss 1.052665 : training accuracy 0.633400, and val accuracy 0.633400\n",
      "epochs 1334 / 2500: loss 1.049486 : training accuracy 0.624200, and val accuracy 0.624200\n",
      "epochs 1335 / 2500: loss 1.052232 : training accuracy 0.635100, and val accuracy 0.635100\n",
      "epochs 1336 / 2500: loss 1.051050 : training accuracy 0.642800, and val accuracy 0.642800\n",
      "epochs 1337 / 2500: loss 1.067039 : training accuracy 0.627200, and val accuracy 0.627200\n",
      "epochs 1338 / 2500: loss 1.077974 : training accuracy 0.642000, and val accuracy 0.642000\n",
      "epochs 1339 / 2500: loss 1.031211 : training accuracy 0.652600, and val accuracy 0.652600\n",
      "epochs 1340 / 2500: loss 1.079573 : training accuracy 0.625600, and val accuracy 0.625600\n",
      "epochs 1341 / 2500: loss 1.054938 : training accuracy 0.631500, and val accuracy 0.631500\n",
      "epochs 1342 / 2500: loss 1.107336 : training accuracy 0.622700, and val accuracy 0.622700\n",
      "epochs 1343 / 2500: loss 1.063941 : training accuracy 0.614500, and val accuracy 0.614500\n",
      "epochs 1344 / 2500: loss 1.062934 : training accuracy 0.613800, and val accuracy 0.613800\n",
      "epochs 1345 / 2500: loss 1.055574 : training accuracy 0.625200, and val accuracy 0.625200\n",
      "epochs 1346 / 2500: loss 1.062614 : training accuracy 0.618100, and val accuracy 0.618100\n",
      "epochs 1347 / 2500: loss 1.029367 : training accuracy 0.630900, and val accuracy 0.630900\n",
      "epochs 1348 / 2500: loss 1.003557 : training accuracy 0.646600, and val accuracy 0.646600\n",
      "epochs 1349 / 2500: loss 1.038957 : training accuracy 0.640400, and val accuracy 0.640400\n",
      "epochs 1350 / 2500: loss 1.039814 : training accuracy 0.631000, and val accuracy 0.631000\n",
      "epochs 1351 / 2500: loss 1.017407 : training accuracy 0.636600, and val accuracy 0.636600\n",
      "epochs 1352 / 2500: loss 1.023155 : training accuracy 0.635500, and val accuracy 0.635500\n",
      "epochs 1353 / 2500: loss 1.085083 : training accuracy 0.629100, and val accuracy 0.629100\n",
      "epochs 1354 / 2500: loss 1.055343 : training accuracy 0.637100, and val accuracy 0.637100\n",
      "epochs 1355 / 2500: loss 1.023123 : training accuracy 0.641100, and val accuracy 0.641100\n",
      "epochs 1356 / 2500: loss 1.022190 : training accuracy 0.641400, and val accuracy 0.641400\n",
      "epochs 1357 / 2500: loss 1.013462 : training accuracy 0.639100, and val accuracy 0.639100\n",
      "epochs 1358 / 2500: loss 1.039422 : training accuracy 0.630600, and val accuracy 0.630600\n",
      "epochs 1359 / 2500: loss 1.032131 : training accuracy 0.632300, and val accuracy 0.632300\n",
      "epochs 1360 / 2500: loss 1.059093 : training accuracy 0.633800, and val accuracy 0.633800\n",
      "epochs 1361 / 2500: loss 1.028267 : training accuracy 0.635300, and val accuracy 0.635300\n",
      "epochs 1362 / 2500: loss 1.018773 : training accuracy 0.645900, and val accuracy 0.645900\n",
      "epochs 1363 / 2500: loss 1.001361 : training accuracy 0.654800, and val accuracy 0.654800\n",
      "epochs 1364 / 2500: loss 1.098344 : training accuracy 0.629700, and val accuracy 0.629700\n",
      "epochs 1365 / 2500: loss 1.025705 : training accuracy 0.647900, and val accuracy 0.647900\n",
      "epochs 1366 / 2500: loss 1.040368 : training accuracy 0.637200, and val accuracy 0.637200\n",
      "epochs 1367 / 2500: loss 1.042256 : training accuracy 0.630700, and val accuracy 0.630700\n",
      "epochs 1368 / 2500: loss 1.042286 : training accuracy 0.632600, and val accuracy 0.632600\n",
      "epochs 1369 / 2500: loss 1.048069 : training accuracy 0.643900, and val accuracy 0.643900\n",
      "epochs 1370 / 2500: loss 1.088390 : training accuracy 0.633600, and val accuracy 0.633600\n",
      "epochs 1371 / 2500: loss 1.105641 : training accuracy 0.638500, and val accuracy 0.638500\n",
      "epochs 1372 / 2500: loss 1.051733 : training accuracy 0.651600, and val accuracy 0.651600\n",
      "epochs 1373 / 2500: loss 1.037234 : training accuracy 0.652300, and val accuracy 0.652300\n",
      "epochs 1374 / 2500: loss 1.033652 : training accuracy 0.656700, and val accuracy 0.656700\n",
      "epochs 1375 / 2500: loss 1.060477 : training accuracy 0.635200, and val accuracy 0.635200\n",
      "epochs 1376 / 2500: loss 1.052665 : training accuracy 0.643600, and val accuracy 0.643600\n",
      "epochs 1377 / 2500: loss 1.021573 : training accuracy 0.654300, and val accuracy 0.654300\n",
      "epochs 1378 / 2500: loss 1.009059 : training accuracy 0.660400, and val accuracy 0.660400\n",
      "epochs 1379 / 2500: loss 1.009370 : training accuracy 0.665800, and val accuracy 0.665800\n",
      "epochs 1380 / 2500: loss 1.066020 : training accuracy 0.639400, and val accuracy 0.639400\n",
      "epochs 1381 / 2500: loss 1.045480 : training accuracy 0.635900, and val accuracy 0.635900\n",
      "epochs 1382 / 2500: loss 1.036075 : training accuracy 0.639900, and val accuracy 0.639900\n",
      "epochs 1383 / 2500: loss 1.026935 : training accuracy 0.642600, and val accuracy 0.642600\n",
      "epochs 1384 / 2500: loss 1.042585 : training accuracy 0.639100, and val accuracy 0.639100\n",
      "epochs 1385 / 2500: loss 1.070725 : training accuracy 0.634400, and val accuracy 0.634400\n",
      "epochs 1386 / 2500: loss 1.021694 : training accuracy 0.637800, and val accuracy 0.637800\n",
      "epochs 1387 / 2500: loss 1.031970 : training accuracy 0.642000, and val accuracy 0.642000\n",
      "epochs 1388 / 2500: loss 1.015284 : training accuracy 0.655000, and val accuracy 0.655000\n",
      "epochs 1389 / 2500: loss 1.049542 : training accuracy 0.629100, and val accuracy 0.629100\n",
      "epochs 1390 / 2500: loss 1.030482 : training accuracy 0.636500, and val accuracy 0.636500\n",
      "epochs 1391 / 2500: loss 1.029089 : training accuracy 0.634800, and val accuracy 0.634800\n",
      "epochs 1392 / 2500: loss 1.062062 : training accuracy 0.646300, and val accuracy 0.646300\n",
      "epochs 1393 / 2500: loss 1.022774 : training accuracy 0.660900, and val accuracy 0.660900\n",
      "epochs 1394 / 2500: loss 1.048906 : training accuracy 0.642300, and val accuracy 0.642300\n",
      "epochs 1395 / 2500: loss 1.019926 : training accuracy 0.652400, and val accuracy 0.652400\n",
      "epochs 1396 / 2500: loss 1.037144 : training accuracy 0.642200, and val accuracy 0.642200\n",
      "epochs 1397 / 2500: loss 1.013215 : training accuracy 0.636000, and val accuracy 0.636000\n",
      "epochs 1398 / 2500: loss 1.068754 : training accuracy 0.642100, and val accuracy 0.642100\n",
      "epochs 1399 / 2500: loss 1.035933 : training accuracy 0.654700, and val accuracy 0.654700\n",
      "epochs 1400 / 2500: loss 1.014970 : training accuracy 0.649300, and val accuracy 0.649300\n",
      "epochs 1401 / 2500: loss 0.998585 : training accuracy 0.652100, and val accuracy 0.652100\n",
      "epochs 1402 / 2500: loss 0.997431 : training accuracy 0.644200, and val accuracy 0.644200\n",
      "epochs 1403 / 2500: loss 1.042181 : training accuracy 0.624200, and val accuracy 0.624200\n",
      "epochs 1404 / 2500: loss 0.999657 : training accuracy 0.645500, and val accuracy 0.645500\n",
      "epochs 1405 / 2500: loss 0.987555 : training accuracy 0.657700, and val accuracy 0.657700\n",
      "epochs 1406 / 2500: loss 1.052973 : training accuracy 0.635800, and val accuracy 0.635800\n",
      "epochs 1407 / 2500: loss 1.044010 : training accuracy 0.634600, and val accuracy 0.634600\n",
      "epochs 1408 / 2500: loss 1.049470 : training accuracy 0.646000, and val accuracy 0.646000\n",
      "epochs 1409 / 2500: loss 1.033200 : training accuracy 0.643100, and val accuracy 0.643100\n",
      "epochs 1410 / 2500: loss 1.034794 : training accuracy 0.635400, and val accuracy 0.635400\n",
      "epochs 1411 / 2500: loss 1.132303 : training accuracy 0.592000, and val accuracy 0.592000\n",
      "epochs 1412 / 2500: loss 1.066233 : training accuracy 0.631000, and val accuracy 0.631000\n",
      "epochs 1413 / 2500: loss 1.033065 : training accuracy 0.643900, and val accuracy 0.643900\n",
      "epochs 1414 / 2500: loss 1.028481 : training accuracy 0.648400, and val accuracy 0.648400\n",
      "epochs 1415 / 2500: loss 1.029784 : training accuracy 0.642500, and val accuracy 0.642500\n",
      "epochs 1416 / 2500: loss 1.013779 : training accuracy 0.652600, and val accuracy 0.652600\n",
      "epochs 1417 / 2500: loss 1.013548 : training accuracy 0.653500, and val accuracy 0.653500\n",
      "epochs 1418 / 2500: loss 1.048037 : training accuracy 0.635700, and val accuracy 0.635700\n",
      "epochs 1419 / 2500: loss 1.072325 : training accuracy 0.633100, and val accuracy 0.633100\n",
      "epochs 1420 / 2500: loss 1.063371 : training accuracy 0.649200, and val accuracy 0.649200\n",
      "epochs 1421 / 2500: loss 1.008742 : training accuracy 0.656800, and val accuracy 0.656800\n",
      "epochs 1422 / 2500: loss 1.026897 : training accuracy 0.638600, and val accuracy 0.638600\n",
      "epochs 1423 / 2500: loss 1.010092 : training accuracy 0.635400, and val accuracy 0.635400\n",
      "epochs 1424 / 2500: loss 1.023350 : training accuracy 0.646200, and val accuracy 0.646200\n",
      "epochs 1425 / 2500: loss 1.025815 : training accuracy 0.643300, and val accuracy 0.643300\n",
      "epochs 1426 / 2500: loss 1.024990 : training accuracy 0.632000, and val accuracy 0.632000\n",
      "epochs 1427 / 2500: loss 1.009307 : training accuracy 0.645000, and val accuracy 0.645000\n",
      "epochs 1428 / 2500: loss 1.008173 : training accuracy 0.650800, and val accuracy 0.650800\n",
      "epochs 1429 / 2500: loss 1.003893 : training accuracy 0.637100, and val accuracy 0.637100\n",
      "epochs 1430 / 2500: loss 0.986538 : training accuracy 0.646200, and val accuracy 0.646200\n",
      "epochs 1431 / 2500: loss 0.994752 : training accuracy 0.645000, and val accuracy 0.645000\n",
      "epochs 1432 / 2500: loss 1.009325 : training accuracy 0.650500, and val accuracy 0.650500\n",
      "epochs 1433 / 2500: loss 1.026608 : training accuracy 0.637600, and val accuracy 0.637600\n",
      "epochs 1434 / 2500: loss 1.013989 : training accuracy 0.642400, and val accuracy 0.642400\n",
      "epochs 1435 / 2500: loss 0.995965 : training accuracy 0.650600, and val accuracy 0.650600\n",
      "epochs 1436 / 2500: loss 0.987542 : training accuracy 0.657100, and val accuracy 0.657100\n",
      "epochs 1437 / 2500: loss 1.074125 : training accuracy 0.643000, and val accuracy 0.643000\n",
      "epochs 1438 / 2500: loss 1.037337 : training accuracy 0.647700, and val accuracy 0.647700\n",
      "epochs 1439 / 2500: loss 1.010989 : training accuracy 0.649400, and val accuracy 0.649400\n",
      "epochs 1440 / 2500: loss 1.000056 : training accuracy 0.635300, and val accuracy 0.635300\n",
      "epochs 1441 / 2500: loss 0.987958 : training accuracy 0.661800, and val accuracy 0.661800\n",
      "epochs 1442 / 2500: loss 1.012970 : training accuracy 0.655900, and val accuracy 0.655900\n",
      "epochs 1443 / 2500: loss 1.005615 : training accuracy 0.644000, and val accuracy 0.644000\n",
      "epochs 1444 / 2500: loss 1.016860 : training accuracy 0.641600, and val accuracy 0.641600\n",
      "epochs 1445 / 2500: loss 1.055851 : training accuracy 0.632200, and val accuracy 0.632200\n",
      "epochs 1446 / 2500: loss 1.018613 : training accuracy 0.636700, and val accuracy 0.636700\n",
      "epochs 1447 / 2500: loss 1.003339 : training accuracy 0.644600, and val accuracy 0.644600\n",
      "epochs 1448 / 2500: loss 1.014486 : training accuracy 0.641500, and val accuracy 0.641500\n",
      "epochs 1449 / 2500: loss 1.003623 : training accuracy 0.645800, and val accuracy 0.645800\n",
      "epochs 1450 / 2500: loss 0.991190 : training accuracy 0.652700, and val accuracy 0.652700\n",
      "epochs 1451 / 2500: loss 0.975448 : training accuracy 0.655800, and val accuracy 0.655800\n",
      "epochs 1452 / 2500: loss 0.998217 : training accuracy 0.660100, and val accuracy 0.660100\n",
      "epochs 1453 / 2500: loss 1.061991 : training accuracy 0.638700, and val accuracy 0.638700\n",
      "epochs 1454 / 2500: loss 1.033596 : training accuracy 0.641500, and val accuracy 0.641500\n",
      "epochs 1455 / 2500: loss 1.006088 : training accuracy 0.655100, and val accuracy 0.655100\n",
      "epochs 1456 / 2500: loss 0.979801 : training accuracy 0.650200, and val accuracy 0.650200\n",
      "epochs 1457 / 2500: loss 1.006023 : training accuracy 0.637200, and val accuracy 0.637200\n",
      "epochs 1458 / 2500: loss 1.005418 : training accuracy 0.638100, and val accuracy 0.638100\n",
      "epochs 1459 / 2500: loss 0.993580 : training accuracy 0.640600, and val accuracy 0.640600\n",
      "epochs 1460 / 2500: loss 0.994076 : training accuracy 0.641900, and val accuracy 0.641900\n",
      "epochs 1461 / 2500: loss 0.986632 : training accuracy 0.660900, and val accuracy 0.660900\n",
      "epochs 1462 / 2500: loss 0.980911 : training accuracy 0.645300, and val accuracy 0.645300\n",
      "epochs 1463 / 2500: loss 1.002828 : training accuracy 0.646500, and val accuracy 0.646500\n",
      "epochs 1464 / 2500: loss 1.110548 : training accuracy 0.621700, and val accuracy 0.621700\n",
      "epochs 1465 / 2500: loss 1.076668 : training accuracy 0.636700, and val accuracy 0.636700\n",
      "epochs 1466 / 2500: loss 1.007958 : training accuracy 0.652500, and val accuracy 0.652500\n",
      "epochs 1467 / 2500: loss 1.026773 : training accuracy 0.641000, and val accuracy 0.641000\n",
      "epochs 1468 / 2500: loss 0.984749 : training accuracy 0.655500, and val accuracy 0.655500\n",
      "epochs 1469 / 2500: loss 0.997213 : training accuracy 0.645300, and val accuracy 0.645300\n",
      "epochs 1470 / 2500: loss 0.994492 : training accuracy 0.659800, and val accuracy 0.659800\n",
      "epochs 1471 / 2500: loss 1.002749 : training accuracy 0.646800, and val accuracy 0.646800\n",
      "epochs 1472 / 2500: loss 1.011786 : training accuracy 0.651600, and val accuracy 0.651600\n",
      "epochs 1473 / 2500: loss 1.006020 : training accuracy 0.646600, and val accuracy 0.646600\n",
      "epochs 1474 / 2500: loss 1.023673 : training accuracy 0.648000, and val accuracy 0.648000\n",
      "epochs 1475 / 2500: loss 1.058291 : training accuracy 0.632800, and val accuracy 0.632800\n",
      "epochs 1476 / 2500: loss 0.991619 : training accuracy 0.659700, and val accuracy 0.659700\n",
      "epochs 1477 / 2500: loss 1.024572 : training accuracy 0.658900, and val accuracy 0.658900\n",
      "epochs 1478 / 2500: loss 1.017255 : training accuracy 0.666900, and val accuracy 0.666900\n",
      "epochs 1479 / 2500: loss 0.990587 : training accuracy 0.652900, and val accuracy 0.652900\n",
      "epochs 1480 / 2500: loss 0.991966 : training accuracy 0.642000, and val accuracy 0.642000\n",
      "epochs 1481 / 2500: loss 1.022345 : training accuracy 0.631100, and val accuracy 0.631100\n",
      "epochs 1482 / 2500: loss 1.036298 : training accuracy 0.640000, and val accuracy 0.640000\n",
      "epochs 1483 / 2500: loss 1.046506 : training accuracy 0.627900, and val accuracy 0.627900\n",
      "epochs 1484 / 2500: loss 0.996720 : training accuracy 0.664700, and val accuracy 0.664700\n",
      "epochs 1485 / 2500: loss 1.000330 : training accuracy 0.658000, and val accuracy 0.658000\n",
      "epochs 1486 / 2500: loss 1.030140 : training accuracy 0.633700, and val accuracy 0.633700\n",
      "epochs 1487 / 2500: loss 1.011472 : training accuracy 0.639300, and val accuracy 0.639300\n",
      "epochs 1488 / 2500: loss 0.985669 : training accuracy 0.654100, and val accuracy 0.654100\n",
      "epochs 1489 / 2500: loss 0.993456 : training accuracy 0.665000, and val accuracy 0.665000\n",
      "epochs 1490 / 2500: loss 0.973848 : training accuracy 0.671100, and val accuracy 0.671100\n",
      "epochs 1491 / 2500: loss 0.967135 : training accuracy 0.656500, and val accuracy 0.656500\n",
      "epochs 1492 / 2500: loss 0.982507 : training accuracy 0.638300, and val accuracy 0.638300\n",
      "epochs 1493 / 2500: loss 0.950081 : training accuracy 0.655300, and val accuracy 0.655300\n",
      "epochs 1494 / 2500: loss 0.965298 : training accuracy 0.645000, and val accuracy 0.645000\n",
      "epochs 1495 / 2500: loss 1.002436 : training accuracy 0.652000, and val accuracy 0.652000\n",
      "epochs 1496 / 2500: loss 1.007217 : training accuracy 0.642100, and val accuracy 0.642100\n",
      "epochs 1497 / 2500: loss 1.004652 : training accuracy 0.647800, and val accuracy 0.647800\n",
      "epochs 1498 / 2500: loss 1.014182 : training accuracy 0.648200, and val accuracy 0.648200\n",
      "epochs 1499 / 2500: loss 0.991022 : training accuracy 0.658800, and val accuracy 0.658800\n",
      "epochs 1500 / 2500: loss 0.980045 : training accuracy 0.650500, and val accuracy 0.650500\n",
      "epochs 1501 / 2500: loss 0.976014 : training accuracy 0.651700, and val accuracy 0.651700\n",
      "epochs 1502 / 2500: loss 0.975841 : training accuracy 0.645900, and val accuracy 0.645900\n",
      "epochs 1503 / 2500: loss 0.965886 : training accuracy 0.650800, and val accuracy 0.650800\n",
      "epochs 1504 / 2500: loss 0.962935 : training accuracy 0.657900, and val accuracy 0.657900\n",
      "epochs 1505 / 2500: loss 1.005750 : training accuracy 0.636200, and val accuracy 0.636200\n",
      "epochs 1506 / 2500: loss 0.961113 : training accuracy 0.652100, and val accuracy 0.652100\n",
      "epochs 1507 / 2500: loss 0.963448 : training accuracy 0.660500, and val accuracy 0.660500\n",
      "epochs 1508 / 2500: loss 0.970653 : training accuracy 0.650600, and val accuracy 0.650600\n",
      "epochs 1509 / 2500: loss 1.020605 : training accuracy 0.650400, and val accuracy 0.650400\n",
      "epochs 1510 / 2500: loss 0.980982 : training accuracy 0.658000, and val accuracy 0.658000\n",
      "epochs 1511 / 2500: loss 1.069164 : training accuracy 0.641900, and val accuracy 0.641900\n",
      "epochs 1512 / 2500: loss 0.993882 : training accuracy 0.656100, and val accuracy 0.656100\n",
      "epochs 1513 / 2500: loss 1.019855 : training accuracy 0.644100, and val accuracy 0.644100\n",
      "epochs 1514 / 2500: loss 0.973156 : training accuracy 0.654800, and val accuracy 0.654800\n",
      "epochs 1515 / 2500: loss 0.974998 : training accuracy 0.659500, and val accuracy 0.659500\n",
      "epochs 1516 / 2500: loss 1.004716 : training accuracy 0.647900, and val accuracy 0.647900\n",
      "epochs 1517 / 2500: loss 0.988546 : training accuracy 0.674200, and val accuracy 0.674200\n",
      "epochs 1518 / 2500: loss 0.992304 : training accuracy 0.676300, and val accuracy 0.676300\n",
      "epochs 1519 / 2500: loss 0.982867 : training accuracy 0.671900, and val accuracy 0.671900\n",
      "epochs 1520 / 2500: loss 0.993910 : training accuracy 0.657900, and val accuracy 0.657900\n",
      "epochs 1521 / 2500: loss 0.981518 : training accuracy 0.652900, and val accuracy 0.652900\n",
      "epochs 1522 / 2500: loss 0.961036 : training accuracy 0.654900, and val accuracy 0.654900\n",
      "epochs 1523 / 2500: loss 1.021761 : training accuracy 0.650100, and val accuracy 0.650100\n",
      "epochs 1524 / 2500: loss 1.012257 : training accuracy 0.653800, and val accuracy 0.653800\n",
      "epochs 1525 / 2500: loss 1.005424 : training accuracy 0.642600, and val accuracy 0.642600\n",
      "epochs 1526 / 2500: loss 0.949965 : training accuracy 0.672200, and val accuracy 0.672200\n",
      "epochs 1527 / 2500: loss 0.976944 : training accuracy 0.652800, and val accuracy 0.652800\n",
      "epochs 1528 / 2500: loss 0.983702 : training accuracy 0.641900, and val accuracy 0.641900\n",
      "epochs 1529 / 2500: loss 0.962057 : training accuracy 0.649200, and val accuracy 0.649200\n",
      "epochs 1530 / 2500: loss 0.991919 : training accuracy 0.646600, and val accuracy 0.646600\n",
      "epochs 1531 / 2500: loss 0.971488 : training accuracy 0.653800, and val accuracy 0.653800\n",
      "epochs 1532 / 2500: loss 0.964037 : training accuracy 0.666600, and val accuracy 0.666600\n",
      "epochs 1533 / 2500: loss 0.957573 : training accuracy 0.671000, and val accuracy 0.671000\n",
      "epochs 1534 / 2500: loss 0.967431 : training accuracy 0.644700, and val accuracy 0.644700\n",
      "epochs 1535 / 2500: loss 0.962583 : training accuracy 0.654800, and val accuracy 0.654800\n",
      "epochs 1536 / 2500: loss 0.945645 : training accuracy 0.656100, and val accuracy 0.656100\n",
      "epochs 1537 / 2500: loss 0.984710 : training accuracy 0.640200, and val accuracy 0.640200\n",
      "epochs 1538 / 2500: loss 1.009357 : training accuracy 0.644900, and val accuracy 0.644900\n",
      "epochs 1539 / 2500: loss 0.974496 : training accuracy 0.652100, and val accuracy 0.652100\n",
      "epochs 1540 / 2500: loss 0.967528 : training accuracy 0.658500, and val accuracy 0.658500\n",
      "epochs 1541 / 2500: loss 0.965590 : training accuracy 0.663700, and val accuracy 0.663700\n",
      "epochs 1542 / 2500: loss 0.953648 : training accuracy 0.666200, and val accuracy 0.666200\n",
      "epochs 1543 / 2500: loss 0.937742 : training accuracy 0.674900, and val accuracy 0.674900\n",
      "epochs 1544 / 2500: loss 1.009804 : training accuracy 0.655500, and val accuracy 0.655500\n",
      "epochs 1545 / 2500: loss 0.955963 : training accuracy 0.676600, and val accuracy 0.676600\n",
      "epochs 1546 / 2500: loss 0.981186 : training accuracy 0.675100, and val accuracy 0.675100\n",
      "epochs 1547 / 2500: loss 0.983364 : training accuracy 0.664900, and val accuracy 0.664900\n",
      "epochs 1548 / 2500: loss 0.982465 : training accuracy 0.659100, and val accuracy 0.659100\n",
      "epochs 1549 / 2500: loss 0.977802 : training accuracy 0.657500, and val accuracy 0.657500\n",
      "epochs 1550 / 2500: loss 1.042042 : training accuracy 0.642200, and val accuracy 0.642200\n",
      "epochs 1551 / 2500: loss 1.022614 : training accuracy 0.631300, and val accuracy 0.631300\n",
      "epochs 1552 / 2500: loss 0.991934 : training accuracy 0.649200, and val accuracy 0.649200\n",
      "epochs 1553 / 2500: loss 0.967738 : training accuracy 0.664100, and val accuracy 0.664100\n",
      "epochs 1554 / 2500: loss 0.966962 : training accuracy 0.659000, and val accuracy 0.659000\n",
      "epochs 1555 / 2500: loss 0.967111 : training accuracy 0.656700, and val accuracy 0.656700\n",
      "epochs 1556 / 2500: loss 0.951015 : training accuracy 0.659200, and val accuracy 0.659200\n",
      "epochs 1557 / 2500: loss 0.957688 : training accuracy 0.666700, and val accuracy 0.666700\n",
      "epochs 1558 / 2500: loss 0.950437 : training accuracy 0.663800, and val accuracy 0.663800\n",
      "epochs 1559 / 2500: loss 0.996278 : training accuracy 0.652500, and val accuracy 0.652500\n",
      "epochs 1560 / 2500: loss 0.975631 : training accuracy 0.656900, and val accuracy 0.656900\n",
      "epochs 1561 / 2500: loss 1.049992 : training accuracy 0.664200, and val accuracy 0.664200\n",
      "epochs 1562 / 2500: loss 0.987792 : training accuracy 0.673200, and val accuracy 0.673200\n",
      "epochs 1563 / 2500: loss 0.988984 : training accuracy 0.651200, and val accuracy 0.651200\n",
      "epochs 1564 / 2500: loss 0.962056 : training accuracy 0.647100, and val accuracy 0.647100\n",
      "epochs 1565 / 2500: loss 1.033494 : training accuracy 0.639000, and val accuracy 0.639000\n",
      "epochs 1566 / 2500: loss 0.977562 : training accuracy 0.657500, and val accuracy 0.657500\n",
      "epochs 1567 / 2500: loss 0.961574 : training accuracy 0.664100, and val accuracy 0.664100\n",
      "epochs 1568 / 2500: loss 0.961328 : training accuracy 0.650200, and val accuracy 0.650200\n",
      "epochs 1569 / 2500: loss 1.026843 : training accuracy 0.633500, and val accuracy 0.633500\n",
      "epochs 1570 / 2500: loss 0.999197 : training accuracy 0.634200, and val accuracy 0.634200\n",
      "epochs 1571 / 2500: loss 0.949578 : training accuracy 0.664600, and val accuracy 0.664600\n",
      "epochs 1572 / 2500: loss 0.920868 : training accuracy 0.681600, and val accuracy 0.681600\n",
      "epochs 1573 / 2500: loss 0.934168 : training accuracy 0.668100, and val accuracy 0.668100\n",
      "epochs 1574 / 2500: loss 0.967758 : training accuracy 0.664400, and val accuracy 0.664400\n",
      "epochs 1575 / 2500: loss 1.020076 : training accuracy 0.654600, and val accuracy 0.654600\n",
      "epochs 1576 / 2500: loss 0.950631 : training accuracy 0.667700, and val accuracy 0.667700\n",
      "epochs 1577 / 2500: loss 0.931615 : training accuracy 0.672200, and val accuracy 0.672200\n",
      "epochs 1578 / 2500: loss 0.966201 : training accuracy 0.668000, and val accuracy 0.668000\n",
      "epochs 1579 / 2500: loss 0.968716 : training accuracy 0.649700, and val accuracy 0.649700\n",
      "epochs 1580 / 2500: loss 0.963058 : training accuracy 0.660500, and val accuracy 0.660500\n",
      "epochs 1581 / 2500: loss 0.931923 : training accuracy 0.671000, and val accuracy 0.671000\n",
      "epochs 1582 / 2500: loss 0.935192 : training accuracy 0.675500, and val accuracy 0.675500\n",
      "epochs 1583 / 2500: loss 0.965497 : training accuracy 0.670200, and val accuracy 0.670200\n",
      "epochs 1584 / 2500: loss 0.988638 : training accuracy 0.660800, and val accuracy 0.660800\n",
      "epochs 1585 / 2500: loss 0.978671 : training accuracy 0.658600, and val accuracy 0.658600\n",
      "epochs 1586 / 2500: loss 0.950810 : training accuracy 0.668400, and val accuracy 0.668400\n",
      "epochs 1587 / 2500: loss 1.006029 : training accuracy 0.662600, and val accuracy 0.662600\n",
      "epochs 1588 / 2500: loss 0.984754 : training accuracy 0.672600, and val accuracy 0.672600\n",
      "epochs 1589 / 2500: loss 0.978350 : training accuracy 0.680100, and val accuracy 0.680100\n",
      "epochs 1590 / 2500: loss 0.996410 : training accuracy 0.661000, and val accuracy 0.661000\n",
      "epochs 1591 / 2500: loss 0.966683 : training accuracy 0.646800, and val accuracy 0.646800\n",
      "epochs 1592 / 2500: loss 0.999121 : training accuracy 0.641300, and val accuracy 0.641300\n",
      "epochs 1593 / 2500: loss 0.981043 : training accuracy 0.652300, and val accuracy 0.652300\n",
      "epochs 1594 / 2500: loss 0.976569 : training accuracy 0.663400, and val accuracy 0.663400\n",
      "epochs 1595 / 2500: loss 0.938642 : training accuracy 0.665300, and val accuracy 0.665300\n",
      "epochs 1596 / 2500: loss 0.928416 : training accuracy 0.676700, and val accuracy 0.676700\n",
      "epochs 1597 / 2500: loss 0.951723 : training accuracy 0.665100, and val accuracy 0.665100\n",
      "epochs 1598 / 2500: loss 0.929085 : training accuracy 0.673800, and val accuracy 0.673800\n",
      "epochs 1599 / 2500: loss 1.048214 : training accuracy 0.653600, and val accuracy 0.653600\n",
      "epochs 1600 / 2500: loss 0.980099 : training accuracy 0.668100, and val accuracy 0.668100\n",
      "epochs 1601 / 2500: loss 0.989531 : training accuracy 0.664300, and val accuracy 0.664300\n",
      "epochs 1602 / 2500: loss 0.967823 : training accuracy 0.671400, and val accuracy 0.671400\n",
      "epochs 1603 / 2500: loss 0.984322 : training accuracy 0.665700, and val accuracy 0.665700\n",
      "epochs 1604 / 2500: loss 0.945875 : training accuracy 0.674000, and val accuracy 0.674000\n",
      "epochs 1605 / 2500: loss 0.959984 : training accuracy 0.650400, and val accuracy 0.650400\n",
      "epochs 1606 / 2500: loss 0.996079 : training accuracy 0.634800, and val accuracy 0.634800\n",
      "epochs 1607 / 2500: loss 0.967070 : training accuracy 0.668500, and val accuracy 0.668500\n",
      "epochs 1608 / 2500: loss 0.951845 : training accuracy 0.679100, and val accuracy 0.679100\n",
      "epochs 1609 / 2500: loss 0.974007 : training accuracy 0.658500, and val accuracy 0.658500\n",
      "epochs 1610 / 2500: loss 1.019339 : training accuracy 0.650600, and val accuracy 0.650600\n",
      "epochs 1611 / 2500: loss 0.991420 : training accuracy 0.673600, and val accuracy 0.673600\n",
      "epochs 1612 / 2500: loss 0.964075 : training accuracy 0.654400, and val accuracy 0.654400\n",
      "epochs 1613 / 2500: loss 0.977804 : training accuracy 0.659500, and val accuracy 0.659500\n",
      "epochs 1614 / 2500: loss 0.974744 : training accuracy 0.656600, and val accuracy 0.656600\n",
      "epochs 1615 / 2500: loss 0.955442 : training accuracy 0.663400, and val accuracy 0.663400\n",
      "epochs 1616 / 2500: loss 0.936574 : training accuracy 0.666600, and val accuracy 0.666600\n",
      "epochs 1617 / 2500: loss 0.924708 : training accuracy 0.677200, and val accuracy 0.677200\n",
      "epochs 1618 / 2500: loss 0.975500 : training accuracy 0.647000, and val accuracy 0.647000\n",
      "epochs 1619 / 2500: loss 0.968690 : training accuracy 0.655400, and val accuracy 0.655400\n",
      "epochs 1620 / 2500: loss 0.970771 : training accuracy 0.665400, and val accuracy 0.665400\n",
      "epochs 1621 / 2500: loss 0.929697 : training accuracy 0.670000, and val accuracy 0.670000\n",
      "epochs 1622 / 2500: loss 0.975054 : training accuracy 0.652700, and val accuracy 0.652700\n",
      "epochs 1623 / 2500: loss 0.960206 : training accuracy 0.673100, and val accuracy 0.673100\n",
      "epochs 1624 / 2500: loss 1.043404 : training accuracy 0.636900, and val accuracy 0.636900\n",
      "epochs 1625 / 2500: loss 1.010332 : training accuracy 0.660100, and val accuracy 0.660100\n",
      "epochs 1626 / 2500: loss 0.972011 : training accuracy 0.664300, and val accuracy 0.664300\n",
      "epochs 1627 / 2500: loss 0.955472 : training accuracy 0.663700, and val accuracy 0.663700\n",
      "epochs 1628 / 2500: loss 1.046153 : training accuracy 0.640100, and val accuracy 0.640100\n",
      "epochs 1629 / 2500: loss 0.959807 : training accuracy 0.668300, and val accuracy 0.668300\n",
      "epochs 1630 / 2500: loss 0.919488 : training accuracy 0.669100, and val accuracy 0.669100\n",
      "epochs 1631 / 2500: loss 0.965377 : training accuracy 0.648500, and val accuracy 0.648500\n",
      "epochs 1632 / 2500: loss 0.921971 : training accuracy 0.664900, and val accuracy 0.664900\n",
      "epochs 1633 / 2500: loss 0.931617 : training accuracy 0.653700, and val accuracy 0.653700\n",
      "epochs 1634 / 2500: loss 0.946336 : training accuracy 0.675500, and val accuracy 0.675500\n",
      "epochs 1635 / 2500: loss 0.969586 : training accuracy 0.669400, and val accuracy 0.669400\n",
      "epochs 1636 / 2500: loss 0.967808 : training accuracy 0.681500, and val accuracy 0.681500\n",
      "epochs 1637 / 2500: loss 0.950686 : training accuracy 0.680600, and val accuracy 0.680600\n",
      "epochs 1638 / 2500: loss 0.916150 : training accuracy 0.680000, and val accuracy 0.680000\n",
      "epochs 1639 / 2500: loss 0.909319 : training accuracy 0.679100, and val accuracy 0.679100\n",
      "epochs 1640 / 2500: loss 1.019713 : training accuracy 0.645400, and val accuracy 0.645400\n",
      "epochs 1641 / 2500: loss 0.970556 : training accuracy 0.673000, and val accuracy 0.673000\n",
      "epochs 1642 / 2500: loss 1.004959 : training accuracy 0.676900, and val accuracy 0.676900\n",
      "epochs 1643 / 2500: loss 0.951528 : training accuracy 0.674600, and val accuracy 0.674600\n",
      "epochs 1644 / 2500: loss 0.946194 : training accuracy 0.674200, and val accuracy 0.674200\n",
      "epochs 1645 / 2500: loss 0.938894 : training accuracy 0.678100, and val accuracy 0.678100\n",
      "epochs 1646 / 2500: loss 0.925752 : training accuracy 0.686300, and val accuracy 0.686300\n",
      "epochs 1647 / 2500: loss 0.922929 : training accuracy 0.687700, and val accuracy 0.687700\n",
      "epochs 1648 / 2500: loss 0.932859 : training accuracy 0.680700, and val accuracy 0.680700\n",
      "epochs 1649 / 2500: loss 1.061599 : training accuracy 0.655900, and val accuracy 0.655900\n",
      "epochs 1650 / 2500: loss 1.003758 : training accuracy 0.675600, and val accuracy 0.675600\n",
      "epochs 1651 / 2500: loss 0.990283 : training accuracy 0.673400, and val accuracy 0.673400\n",
      "epochs 1652 / 2500: loss 0.933038 : training accuracy 0.682600, and val accuracy 0.682600\n",
      "epochs 1653 / 2500: loss 0.927082 : training accuracy 0.678600, and val accuracy 0.678600\n",
      "epochs 1654 / 2500: loss 0.914418 : training accuracy 0.670700, and val accuracy 0.670700\n",
      "epochs 1655 / 2500: loss 0.926441 : training accuracy 0.664800, and val accuracy 0.664800\n",
      "epochs 1656 / 2500: loss 0.899901 : training accuracy 0.676600, and val accuracy 0.676600\n",
      "epochs 1657 / 2500: loss 0.962837 : training accuracy 0.676100, and val accuracy 0.676100\n",
      "epochs 1658 / 2500: loss 0.955055 : training accuracy 0.669500, and val accuracy 0.669500\n",
      "epochs 1659 / 2500: loss 0.948273 : training accuracy 0.663900, and val accuracy 0.663900\n",
      "epochs 1660 / 2500: loss 0.978581 : training accuracy 0.659600, and val accuracy 0.659600\n",
      "epochs 1661 / 2500: loss 0.948324 : training accuracy 0.665900, and val accuracy 0.665900\n",
      "epochs 1662 / 2500: loss 0.931161 : training accuracy 0.670800, and val accuracy 0.670800\n",
      "epochs 1663 / 2500: loss 0.937865 : training accuracy 0.687100, and val accuracy 0.687100\n",
      "epochs 1664 / 2500: loss 0.933878 : training accuracy 0.686900, and val accuracy 0.686900\n",
      "epochs 1665 / 2500: loss 0.965802 : training accuracy 0.666100, and val accuracy 0.666100\n",
      "epochs 1666 / 2500: loss 0.993347 : training accuracy 0.652100, and val accuracy 0.652100\n",
      "epochs 1667 / 2500: loss 0.943912 : training accuracy 0.672500, and val accuracy 0.672500\n",
      "epochs 1668 / 2500: loss 0.924320 : training accuracy 0.683300, and val accuracy 0.683300\n",
      "epochs 1669 / 2500: loss 0.909495 : training accuracy 0.686000, and val accuracy 0.686000\n",
      "epochs 1670 / 2500: loss 0.950697 : training accuracy 0.665200, and val accuracy 0.665200\n",
      "epochs 1671 / 2500: loss 0.970207 : training accuracy 0.662900, and val accuracy 0.662900\n",
      "epochs 1672 / 2500: loss 0.953430 : training accuracy 0.652700, and val accuracy 0.652700\n",
      "epochs 1673 / 2500: loss 0.907042 : training accuracy 0.678200, and val accuracy 0.678200\n",
      "epochs 1674 / 2500: loss 0.916854 : training accuracy 0.678800, and val accuracy 0.678800\n",
      "epochs 1675 / 2500: loss 0.929158 : training accuracy 0.672400, and val accuracy 0.672400\n",
      "epochs 1676 / 2500: loss 0.946135 : training accuracy 0.684800, and val accuracy 0.684800\n",
      "epochs 1677 / 2500: loss 0.931440 : training accuracy 0.669900, and val accuracy 0.669900\n",
      "epochs 1678 / 2500: loss 0.925867 : training accuracy 0.671200, and val accuracy 0.671200\n",
      "epochs 1679 / 2500: loss 0.911424 : training accuracy 0.676300, and val accuracy 0.676300\n",
      "epochs 1680 / 2500: loss 0.920650 : training accuracy 0.669700, and val accuracy 0.669700\n",
      "epochs 1681 / 2500: loss 0.919502 : training accuracy 0.674600, and val accuracy 0.674600\n",
      "epochs 1682 / 2500: loss 0.921850 : training accuracy 0.665800, and val accuracy 0.665800\n",
      "epochs 1683 / 2500: loss 0.999985 : training accuracy 0.647100, and val accuracy 0.647100\n",
      "epochs 1684 / 2500: loss 0.964139 : training accuracy 0.672800, and val accuracy 0.672800\n",
      "epochs 1685 / 2500: loss 0.937304 : training accuracy 0.681800, and val accuracy 0.681800\n",
      "epochs 1686 / 2500: loss 0.950512 : training accuracy 0.669200, and val accuracy 0.669200\n",
      "epochs 1687 / 2500: loss 0.907456 : training accuracy 0.679200, and val accuracy 0.679200\n",
      "epochs 1688 / 2500: loss 0.962599 : training accuracy 0.664700, and val accuracy 0.664700\n",
      "epochs 1689 / 2500: loss 0.946478 : training accuracy 0.662100, and val accuracy 0.662100\n",
      "epochs 1690 / 2500: loss 0.932727 : training accuracy 0.675600, and val accuracy 0.675600\n",
      "epochs 1691 / 2500: loss 0.917046 : training accuracy 0.687700, and val accuracy 0.687700\n",
      "epochs 1692 / 2500: loss 0.999010 : training accuracy 0.669100, and val accuracy 0.669100\n",
      "epochs 1693 / 2500: loss 0.952546 : training accuracy 0.645900, and val accuracy 0.645900\n",
      "epochs 1694 / 2500: loss 0.918220 : training accuracy 0.669900, and val accuracy 0.669900\n",
      "epochs 1695 / 2500: loss 0.955452 : training accuracy 0.649100, and val accuracy 0.649100\n",
      "epochs 1696 / 2500: loss 0.925870 : training accuracy 0.669100, and val accuracy 0.669100\n",
      "epochs 1697 / 2500: loss 0.934369 : training accuracy 0.672700, and val accuracy 0.672700\n",
      "epochs 1698 / 2500: loss 0.897353 : training accuracy 0.690500, and val accuracy 0.690500\n",
      "epochs 1699 / 2500: loss 0.934852 : training accuracy 0.671200, and val accuracy 0.671200\n",
      "epochs 1700 / 2500: loss 0.959193 : training accuracy 0.662400, and val accuracy 0.662400\n",
      "epochs 1701 / 2500: loss 0.923070 : training accuracy 0.668100, and val accuracy 0.668100\n",
      "epochs 1702 / 2500: loss 0.972570 : training accuracy 0.650400, and val accuracy 0.650400\n",
      "epochs 1703 / 2500: loss 0.963438 : training accuracy 0.665500, and val accuracy 0.665500\n",
      "epochs 1704 / 2500: loss 0.930944 : training accuracy 0.676300, and val accuracy 0.676300\n",
      "epochs 1705 / 2500: loss 0.964754 : training accuracy 0.680500, and val accuracy 0.680500\n",
      "epochs 1706 / 2500: loss 0.961727 : training accuracy 0.674800, and val accuracy 0.674800\n",
      "epochs 1707 / 2500: loss 0.961482 : training accuracy 0.662900, and val accuracy 0.662900\n",
      "epochs 1708 / 2500: loss 0.928099 : training accuracy 0.684300, and val accuracy 0.684300\n",
      "epochs 1709 / 2500: loss 0.957364 : training accuracy 0.681500, and val accuracy 0.681500\n",
      "epochs 1710 / 2500: loss 0.930204 : training accuracy 0.693600, and val accuracy 0.693600\n",
      "epochs 1711 / 2500: loss 0.954113 : training accuracy 0.690700, and val accuracy 0.690700\n",
      "epochs 1712 / 2500: loss 0.950638 : training accuracy 0.670200, and val accuracy 0.670200\n",
      "epochs 1713 / 2500: loss 0.918613 : training accuracy 0.678300, and val accuracy 0.678300\n",
      "epochs 1714 / 2500: loss 0.872063 : training accuracy 0.698300, and val accuracy 0.698300\n",
      "epochs 1715 / 2500: loss 0.890133 : training accuracy 0.688300, and val accuracy 0.688300\n",
      "epochs 1716 / 2500: loss 0.900601 : training accuracy 0.682200, and val accuracy 0.682200\n",
      "epochs 1717 / 2500: loss 0.911440 : training accuracy 0.684100, and val accuracy 0.684100\n",
      "epochs 1718 / 2500: loss 0.916655 : training accuracy 0.676200, and val accuracy 0.676200\n",
      "epochs 1719 / 2500: loss 0.903246 : training accuracy 0.673400, and val accuracy 0.673400\n",
      "epochs 1720 / 2500: loss 0.910603 : training accuracy 0.676200, and val accuracy 0.676200\n",
      "epochs 1721 / 2500: loss 0.944077 : training accuracy 0.672400, and val accuracy 0.672400\n",
      "epochs 1722 / 2500: loss 0.943491 : training accuracy 0.691000, and val accuracy 0.691000\n",
      "epochs 1723 / 2500: loss 0.944517 : training accuracy 0.684700, and val accuracy 0.684700\n",
      "epochs 1724 / 2500: loss 0.954257 : training accuracy 0.687600, and val accuracy 0.687600\n",
      "epochs 1725 / 2500: loss 0.970786 : training accuracy 0.668200, and val accuracy 0.668200\n",
      "epochs 1726 / 2500: loss 0.954311 : training accuracy 0.684200, and val accuracy 0.684200\n",
      "epochs 1727 / 2500: loss 0.952039 : training accuracy 0.670200, and val accuracy 0.670200\n",
      "epochs 1728 / 2500: loss 0.924302 : training accuracy 0.669900, and val accuracy 0.669900\n",
      "epochs 1729 / 2500: loss 0.899313 : training accuracy 0.687700, and val accuracy 0.687700\n",
      "epochs 1730 / 2500: loss 0.898608 : training accuracy 0.690100, and val accuracy 0.690100\n",
      "epochs 1731 / 2500: loss 0.913894 : training accuracy 0.685900, and val accuracy 0.685900\n",
      "epochs 1732 / 2500: loss 0.958975 : training accuracy 0.669000, and val accuracy 0.669000\n",
      "epochs 1733 / 2500: loss 0.928380 : training accuracy 0.679100, and val accuracy 0.679100\n",
      "epochs 1734 / 2500: loss 0.918258 : training accuracy 0.686000, and val accuracy 0.686000\n",
      "epochs 1735 / 2500: loss 0.918541 : training accuracy 0.692900, and val accuracy 0.692900\n",
      "epochs 1736 / 2500: loss 0.911676 : training accuracy 0.689700, and val accuracy 0.689700\n",
      "epochs 1737 / 2500: loss 0.907363 : training accuracy 0.683600, and val accuracy 0.683600\n",
      "epochs 1738 / 2500: loss 0.889029 : training accuracy 0.687400, and val accuracy 0.687400\n",
      "epochs 1739 / 2500: loss 0.929743 : training accuracy 0.667300, and val accuracy 0.667300\n",
      "epochs 1740 / 2500: loss 0.938983 : training accuracy 0.655400, and val accuracy 0.655400\n",
      "epochs 1741 / 2500: loss 0.994841 : training accuracy 0.677200, and val accuracy 0.677200\n",
      "epochs 1742 / 2500: loss 0.944397 : training accuracy 0.677600, and val accuracy 0.677600\n",
      "epochs 1743 / 2500: loss 0.910886 : training accuracy 0.689900, and val accuracy 0.689900\n",
      "epochs 1744 / 2500: loss 0.928581 : training accuracy 0.702600, and val accuracy 0.702600\n",
      "epochs 1745 / 2500: loss 0.909827 : training accuracy 0.687700, and val accuracy 0.687700\n",
      "epochs 1746 / 2500: loss 0.894810 : training accuracy 0.692100, and val accuracy 0.692100\n",
      "epochs 1747 / 2500: loss 0.947569 : training accuracy 0.678800, and val accuracy 0.678800\n",
      "epochs 1748 / 2500: loss 0.967114 : training accuracy 0.669300, and val accuracy 0.669300\n",
      "epochs 1749 / 2500: loss 0.924001 : training accuracy 0.681600, and val accuracy 0.681600\n",
      "epochs 1750 / 2500: loss 0.917823 : training accuracy 0.689200, and val accuracy 0.689200\n",
      "epochs 1751 / 2500: loss 0.953928 : training accuracy 0.673500, and val accuracy 0.673500\n",
      "epochs 1752 / 2500: loss 0.943754 : training accuracy 0.652800, and val accuracy 0.652800\n",
      "epochs 1753 / 2500: loss 0.916923 : training accuracy 0.669500, and val accuracy 0.669500\n",
      "epochs 1754 / 2500: loss 0.970284 : training accuracy 0.671200, and val accuracy 0.671200\n",
      "epochs 1755 / 2500: loss 0.936519 : training accuracy 0.674700, and val accuracy 0.674700\n",
      "epochs 1756 / 2500: loss 0.910947 : training accuracy 0.673500, and val accuracy 0.673500\n",
      "epochs 1757 / 2500: loss 0.892633 : training accuracy 0.682000, and val accuracy 0.682000\n",
      "epochs 1758 / 2500: loss 0.889654 : training accuracy 0.684800, and val accuracy 0.684800\n",
      "epochs 1759 / 2500: loss 0.967890 : training accuracy 0.643900, and val accuracy 0.643900\n",
      "epochs 1760 / 2500: loss 0.933051 : training accuracy 0.657500, and val accuracy 0.657500\n",
      "epochs 1761 / 2500: loss 0.928543 : training accuracy 0.678900, and val accuracy 0.678900\n",
      "epochs 1762 / 2500: loss 0.879398 : training accuracy 0.704100, and val accuracy 0.704100\n",
      "epochs 1763 / 2500: loss 0.872635 : training accuracy 0.697700, and val accuracy 0.697700\n",
      "epochs 1764 / 2500: loss 0.921194 : training accuracy 0.678900, and val accuracy 0.678900\n",
      "epochs 1765 / 2500: loss 0.934660 : training accuracy 0.673800, and val accuracy 0.673800\n",
      "epochs 1766 / 2500: loss 0.884915 : training accuracy 0.694900, and val accuracy 0.694900\n",
      "epochs 1767 / 2500: loss 0.900008 : training accuracy 0.692900, and val accuracy 0.692900\n",
      "epochs 1768 / 2500: loss 0.973083 : training accuracy 0.652200, and val accuracy 0.652200\n",
      "epochs 1769 / 2500: loss 0.910265 : training accuracy 0.682500, and val accuracy 0.682500\n",
      "epochs 1770 / 2500: loss 0.879254 : training accuracy 0.682400, and val accuracy 0.682400\n",
      "epochs 1771 / 2500: loss 0.912111 : training accuracy 0.660900, and val accuracy 0.660900\n",
      "epochs 1772 / 2500: loss 0.905377 : training accuracy 0.670400, and val accuracy 0.670400\n",
      "epochs 1773 / 2500: loss 0.894361 : training accuracy 0.691900, and val accuracy 0.691900\n",
      "epochs 1774 / 2500: loss 0.885614 : training accuracy 0.693800, and val accuracy 0.693800\n",
      "epochs 1775 / 2500: loss 0.881909 : training accuracy 0.696200, and val accuracy 0.696200\n",
      "epochs 1776 / 2500: loss 0.914390 : training accuracy 0.669900, and val accuracy 0.669900\n",
      "epochs 1777 / 2500: loss 0.887366 : training accuracy 0.684400, and val accuracy 0.684400\n",
      "epochs 1778 / 2500: loss 0.895633 : training accuracy 0.696100, and val accuracy 0.696100\n",
      "epochs 1779 / 2500: loss 0.869599 : training accuracy 0.703700, and val accuracy 0.703700\n",
      "epochs 1780 / 2500: loss 0.904436 : training accuracy 0.674000, and val accuracy 0.674000\n",
      "epochs 1781 / 2500: loss 0.887564 : training accuracy 0.680200, and val accuracy 0.680200\n",
      "epochs 1782 / 2500: loss 0.874843 : training accuracy 0.685800, and val accuracy 0.685800\n",
      "epochs 1783 / 2500: loss 0.869049 : training accuracy 0.690800, and val accuracy 0.690800\n",
      "epochs 1784 / 2500: loss 0.907995 : training accuracy 0.665400, and val accuracy 0.665400\n",
      "epochs 1785 / 2500: loss 0.871593 : training accuracy 0.669500, and val accuracy 0.669500\n",
      "epochs 1786 / 2500: loss 0.898184 : training accuracy 0.684200, and val accuracy 0.684200\n",
      "epochs 1787 / 2500: loss 0.886393 : training accuracy 0.677100, and val accuracy 0.677100\n",
      "epochs 1788 / 2500: loss 0.895500 : training accuracy 0.681000, and val accuracy 0.681000\n",
      "epochs 1789 / 2500: loss 0.968652 : training accuracy 0.658400, and val accuracy 0.658400\n",
      "epochs 1790 / 2500: loss 0.917301 : training accuracy 0.674100, and val accuracy 0.674100\n",
      "epochs 1791 / 2500: loss 0.939163 : training accuracy 0.665700, and val accuracy 0.665700\n",
      "epochs 1792 / 2500: loss 0.896089 : training accuracy 0.691600, and val accuracy 0.691600\n",
      "epochs 1793 / 2500: loss 0.911779 : training accuracy 0.689100, and val accuracy 0.689100\n",
      "epochs 1794 / 2500: loss 0.906149 : training accuracy 0.682200, and val accuracy 0.682200\n",
      "epochs 1795 / 2500: loss 0.889670 : training accuracy 0.691100, and val accuracy 0.691100\n",
      "epochs 1796 / 2500: loss 0.872734 : training accuracy 0.689000, and val accuracy 0.689000\n",
      "epochs 1797 / 2500: loss 0.883827 : training accuracy 0.670300, and val accuracy 0.670300\n",
      "epochs 1798 / 2500: loss 0.904224 : training accuracy 0.682400, and val accuracy 0.682400\n",
      "epochs 1799 / 2500: loss 0.896638 : training accuracy 0.684800, and val accuracy 0.684800\n",
      "epochs 1800 / 2500: loss 0.909850 : training accuracy 0.676500, and val accuracy 0.676500\n",
      "epochs 1801 / 2500: loss 0.896486 : training accuracy 0.685000, and val accuracy 0.685000\n",
      "epochs 1802 / 2500: loss 0.898838 : training accuracy 0.677900, and val accuracy 0.677900\n",
      "epochs 1803 / 2500: loss 0.953637 : training accuracy 0.654000, and val accuracy 0.654000\n",
      "epochs 1804 / 2500: loss 0.966901 : training accuracy 0.657900, and val accuracy 0.657900\n",
      "epochs 1805 / 2500: loss 0.918891 : training accuracy 0.678400, and val accuracy 0.678400\n",
      "epochs 1806 / 2500: loss 0.908494 : training accuracy 0.685100, and val accuracy 0.685100\n",
      "epochs 1807 / 2500: loss 0.920746 : training accuracy 0.688100, and val accuracy 0.688100\n",
      "epochs 1808 / 2500: loss 0.888626 : training accuracy 0.692400, and val accuracy 0.692400\n",
      "epochs 1809 / 2500: loss 0.908509 : training accuracy 0.679500, and val accuracy 0.679500\n",
      "epochs 1810 / 2500: loss 0.920535 : training accuracy 0.678500, and val accuracy 0.678500\n",
      "epochs 1811 / 2500: loss 0.875077 : training accuracy 0.695000, and val accuracy 0.695000\n",
      "epochs 1812 / 2500: loss 0.883639 : training accuracy 0.685100, and val accuracy 0.685100\n",
      "epochs 1813 / 2500: loss 0.863469 : training accuracy 0.697800, and val accuracy 0.697800\n",
      "epochs 1814 / 2500: loss 0.848820 : training accuracy 0.702000, and val accuracy 0.702000\n",
      "epochs 1815 / 2500: loss 0.982411 : training accuracy 0.640900, and val accuracy 0.640900\n",
      "epochs 1816 / 2500: loss 0.894111 : training accuracy 0.688300, and val accuracy 0.688300\n",
      "epochs 1817 / 2500: loss 0.859109 : training accuracy 0.695300, and val accuracy 0.695300\n",
      "epochs 1818 / 2500: loss 0.862681 : training accuracy 0.702200, and val accuracy 0.702200\n",
      "epochs 1819 / 2500: loss 0.872704 : training accuracy 0.698200, and val accuracy 0.698200\n",
      "epochs 1820 / 2500: loss 0.916377 : training accuracy 0.658500, and val accuracy 0.658500\n",
      "epochs 1821 / 2500: loss 0.919325 : training accuracy 0.662900, and val accuracy 0.662900\n",
      "epochs 1822 / 2500: loss 0.898347 : training accuracy 0.680200, and val accuracy 0.680200\n",
      "epochs 1823 / 2500: loss 0.872367 : training accuracy 0.692200, and val accuracy 0.692200\n",
      "epochs 1824 / 2500: loss 0.860700 : training accuracy 0.694800, and val accuracy 0.694800\n",
      "epochs 1825 / 2500: loss 0.892042 : training accuracy 0.707300, and val accuracy 0.707300\n",
      "epochs 1826 / 2500: loss 0.887299 : training accuracy 0.703100, and val accuracy 0.703100\n",
      "epochs 1827 / 2500: loss 0.928407 : training accuracy 0.698400, and val accuracy 0.698400\n",
      "epochs 1828 / 2500: loss 0.985439 : training accuracy 0.686300, and val accuracy 0.686300\n",
      "epochs 1829 / 2500: loss 0.973420 : training accuracy 0.689600, and val accuracy 0.689600\n",
      "epochs 1830 / 2500: loss 0.864509 : training accuracy 0.714900, and val accuracy 0.714900\n",
      "epochs 1831 / 2500: loss 0.880398 : training accuracy 0.713200, and val accuracy 0.713200\n",
      "epochs 1832 / 2500: loss 0.887178 : training accuracy 0.696300, and val accuracy 0.696300\n",
      "epochs 1833 / 2500: loss 0.888340 : training accuracy 0.687500, and val accuracy 0.687500\n",
      "epochs 1834 / 2500: loss 0.871201 : training accuracy 0.688700, and val accuracy 0.688700\n",
      "epochs 1835 / 2500: loss 0.913371 : training accuracy 0.670900, and val accuracy 0.670900\n",
      "epochs 1836 / 2500: loss 0.910076 : training accuracy 0.678500, and val accuracy 0.678500\n",
      "epochs 1837 / 2500: loss 0.876468 : training accuracy 0.685300, and val accuracy 0.685300\n",
      "epochs 1838 / 2500: loss 0.867756 : training accuracy 0.682100, and val accuracy 0.682100\n",
      "epochs 1839 / 2500: loss 0.862476 : training accuracy 0.693600, and val accuracy 0.693600\n",
      "epochs 1840 / 2500: loss 0.868202 : training accuracy 0.688300, and val accuracy 0.688300\n",
      "epochs 1841 / 2500: loss 0.923256 : training accuracy 0.686400, and val accuracy 0.686400\n",
      "epochs 1842 / 2500: loss 0.889018 : training accuracy 0.699800, and val accuracy 0.699800\n",
      "epochs 1843 / 2500: loss 0.878108 : training accuracy 0.702100, and val accuracy 0.702100\n",
      "epochs 1844 / 2500: loss 0.876069 : training accuracy 0.704100, and val accuracy 0.704100\n",
      "epochs 1845 / 2500: loss 0.983285 : training accuracy 0.666900, and val accuracy 0.666900\n",
      "epochs 1846 / 2500: loss 0.926285 : training accuracy 0.703000, and val accuracy 0.703000\n",
      "epochs 1847 / 2500: loss 0.913768 : training accuracy 0.700300, and val accuracy 0.700300\n",
      "epochs 1848 / 2500: loss 0.883200 : training accuracy 0.697300, and val accuracy 0.697300\n",
      "epochs 1849 / 2500: loss 0.857761 : training accuracy 0.697900, and val accuracy 0.697900\n",
      "epochs 1850 / 2500: loss 0.893474 : training accuracy 0.682900, and val accuracy 0.682900\n",
      "epochs 1851 / 2500: loss 0.858655 : training accuracy 0.706300, and val accuracy 0.706300\n",
      "epochs 1852 / 2500: loss 0.900278 : training accuracy 0.679600, and val accuracy 0.679600\n",
      "epochs 1853 / 2500: loss 0.936621 : training accuracy 0.672200, and val accuracy 0.672200\n",
      "epochs 1854 / 2500: loss 0.905374 : training accuracy 0.686400, and val accuracy 0.686400\n",
      "epochs 1855 / 2500: loss 0.910827 : training accuracy 0.689200, and val accuracy 0.689200\n",
      "epochs 1856 / 2500: loss 0.960827 : training accuracy 0.658800, and val accuracy 0.658800\n",
      "epochs 1857 / 2500: loss 0.903841 : training accuracy 0.683000, and val accuracy 0.683000\n",
      "epochs 1858 / 2500: loss 0.888795 : training accuracy 0.691400, and val accuracy 0.691400\n",
      "epochs 1859 / 2500: loss 0.924141 : training accuracy 0.681800, and val accuracy 0.681800\n",
      "epochs 1860 / 2500: loss 0.893401 : training accuracy 0.687200, and val accuracy 0.687200\n",
      "epochs 1861 / 2500: loss 0.917303 : training accuracy 0.688100, and val accuracy 0.688100\n",
      "epochs 1862 / 2500: loss 0.836423 : training accuracy 0.714900, and val accuracy 0.714900\n",
      "epochs 1863 / 2500: loss 0.840859 : training accuracy 0.715700, and val accuracy 0.715700\n",
      "epochs 1864 / 2500: loss 0.844765 : training accuracy 0.711600, and val accuracy 0.711600\n",
      "epochs 1865 / 2500: loss 0.860354 : training accuracy 0.713200, and val accuracy 0.713200\n",
      "epochs 1866 / 2500: loss 0.861445 : training accuracy 0.709300, and val accuracy 0.709300\n",
      "epochs 1867 / 2500: loss 0.853104 : training accuracy 0.696000, and val accuracy 0.696000\n",
      "epochs 1868 / 2500: loss 0.885394 : training accuracy 0.697700, and val accuracy 0.697700\n",
      "epochs 1869 / 2500: loss 0.921055 : training accuracy 0.686200, and val accuracy 0.686200\n",
      "epochs 1870 / 2500: loss 0.909251 : training accuracy 0.664600, and val accuracy 0.664600\n",
      "epochs 1871 / 2500: loss 0.904345 : training accuracy 0.677100, and val accuracy 0.677100\n",
      "epochs 1872 / 2500: loss 0.886501 : training accuracy 0.685300, and val accuracy 0.685300\n",
      "epochs 1873 / 2500: loss 0.900818 : training accuracy 0.680600, and val accuracy 0.680600\n",
      "epochs 1874 / 2500: loss 0.904930 : training accuracy 0.689300, and val accuracy 0.689300\n",
      "epochs 1875 / 2500: loss 0.926524 : training accuracy 0.680600, and val accuracy 0.680600\n",
      "epochs 1876 / 2500: loss 0.837438 : training accuracy 0.713300, and val accuracy 0.713300\n",
      "epochs 1877 / 2500: loss 0.899649 : training accuracy 0.693100, and val accuracy 0.693100\n",
      "epochs 1878 / 2500: loss 0.869832 : training accuracy 0.704500, and val accuracy 0.704500\n",
      "epochs 1879 / 2500: loss 0.956618 : training accuracy 0.682100, and val accuracy 0.682100\n",
      "epochs 1880 / 2500: loss 0.905815 : training accuracy 0.691200, and val accuracy 0.691200\n",
      "epochs 1881 / 2500: loss 0.892640 : training accuracy 0.704600, and val accuracy 0.704600\n",
      "epochs 1882 / 2500: loss 0.864404 : training accuracy 0.703000, and val accuracy 0.703000\n",
      "epochs 1883 / 2500: loss 0.836553 : training accuracy 0.713000, and val accuracy 0.713000\n",
      "epochs 1884 / 2500: loss 0.857404 : training accuracy 0.703400, and val accuracy 0.703400\n",
      "epochs 1885 / 2500: loss 0.912542 : training accuracy 0.662800, and val accuracy 0.662800\n",
      "epochs 1886 / 2500: loss 0.892792 : training accuracy 0.681600, and val accuracy 0.681600\n",
      "epochs 1887 / 2500: loss 0.890703 : training accuracy 0.685600, and val accuracy 0.685600\n",
      "epochs 1888 / 2500: loss 0.922240 : training accuracy 0.671300, and val accuracy 0.671300\n",
      "epochs 1889 / 2500: loss 0.916326 : training accuracy 0.689000, and val accuracy 0.689000\n",
      "epochs 1890 / 2500: loss 0.857247 : training accuracy 0.715000, and val accuracy 0.715000\n",
      "epochs 1891 / 2500: loss 0.851434 : training accuracy 0.699900, and val accuracy 0.699900\n",
      "epochs 1892 / 2500: loss 0.888641 : training accuracy 0.684900, and val accuracy 0.684900\n",
      "epochs 1893 / 2500: loss 0.921226 : training accuracy 0.677800, and val accuracy 0.677800\n",
      "epochs 1894 / 2500: loss 0.896441 : training accuracy 0.688500, and val accuracy 0.688500\n",
      "epochs 1895 / 2500: loss 0.934827 : training accuracy 0.680500, and val accuracy 0.680500\n",
      "epochs 1896 / 2500: loss 0.852660 : training accuracy 0.704100, and val accuracy 0.704100\n",
      "epochs 1897 / 2500: loss 0.917088 : training accuracy 0.662100, and val accuracy 0.662100\n",
      "epochs 1898 / 2500: loss 0.891055 : training accuracy 0.678600, and val accuracy 0.678600\n",
      "epochs 1899 / 2500: loss 0.861703 : training accuracy 0.686000, and val accuracy 0.686000\n",
      "epochs 1900 / 2500: loss 0.864398 : training accuracy 0.700600, and val accuracy 0.700600\n",
      "epochs 1901 / 2500: loss 0.833450 : training accuracy 0.705000, and val accuracy 0.705000\n",
      "epochs 1902 / 2500: loss 0.840399 : training accuracy 0.699700, and val accuracy 0.699700\n",
      "epochs 1903 / 2500: loss 0.878147 : training accuracy 0.693400, and val accuracy 0.693400\n",
      "epochs 1904 / 2500: loss 0.871400 : training accuracy 0.702900, and val accuracy 0.702900\n",
      "epochs 1905 / 2500: loss 0.868885 : training accuracy 0.708100, and val accuracy 0.708100\n",
      "epochs 1906 / 2500: loss 0.965102 : training accuracy 0.687100, and val accuracy 0.687100\n",
      "epochs 1907 / 2500: loss 0.904699 : training accuracy 0.697700, and val accuracy 0.697700\n",
      "epochs 1908 / 2500: loss 0.893968 : training accuracy 0.715500, and val accuracy 0.715500\n",
      "epochs 1909 / 2500: loss 0.852291 : training accuracy 0.719300, and val accuracy 0.719300\n",
      "epochs 1910 / 2500: loss 0.842063 : training accuracy 0.713900, and val accuracy 0.713900\n",
      "epochs 1911 / 2500: loss 0.850326 : training accuracy 0.712100, and val accuracy 0.712100\n",
      "epochs 1912 / 2500: loss 0.904687 : training accuracy 0.700500, and val accuracy 0.700500\n",
      "epochs 1913 / 2500: loss 0.881675 : training accuracy 0.708100, and val accuracy 0.708100\n",
      "epochs 1914 / 2500: loss 0.866685 : training accuracy 0.707400, and val accuracy 0.707400\n",
      "epochs 1915 / 2500: loss 0.872534 : training accuracy 0.704400, and val accuracy 0.704400\n",
      "epochs 1916 / 2500: loss 0.897092 : training accuracy 0.682400, and val accuracy 0.682400\n",
      "epochs 1917 / 2500: loss 0.869426 : training accuracy 0.703200, and val accuracy 0.703200\n",
      "epochs 1918 / 2500: loss 0.861526 : training accuracy 0.703200, and val accuracy 0.703200\n",
      "epochs 1919 / 2500: loss 0.875379 : training accuracy 0.694700, and val accuracy 0.694700\n",
      "epochs 1920 / 2500: loss 0.853028 : training accuracy 0.695200, and val accuracy 0.695200\n",
      "epochs 1921 / 2500: loss 0.908207 : training accuracy 0.686300, and val accuracy 0.686300\n",
      "epochs 1922 / 2500: loss 0.982036 : training accuracy 0.675700, and val accuracy 0.675700\n",
      "epochs 1923 / 2500: loss 0.948452 : training accuracy 0.685300, and val accuracy 0.685300\n",
      "epochs 1924 / 2500: loss 0.910560 : training accuracy 0.668800, and val accuracy 0.668800\n",
      "epochs 1925 / 2500: loss 0.879978 : training accuracy 0.683000, and val accuracy 0.683000\n",
      "epochs 1926 / 2500: loss 0.856146 : training accuracy 0.693700, and val accuracy 0.693700\n",
      "epochs 1927 / 2500: loss 0.841147 : training accuracy 0.700200, and val accuracy 0.700200\n",
      "epochs 1928 / 2500: loss 0.824925 : training accuracy 0.704600, and val accuracy 0.704600\n",
      "epochs 1929 / 2500: loss 0.824288 : training accuracy 0.713300, and val accuracy 0.713300\n",
      "epochs 1930 / 2500: loss 0.833638 : training accuracy 0.722600, and val accuracy 0.722600\n",
      "epochs 1931 / 2500: loss 0.821271 : training accuracy 0.708900, and val accuracy 0.708900\n",
      "epochs 1932 / 2500: loss 0.856458 : training accuracy 0.700200, and val accuracy 0.700200\n",
      "epochs 1933 / 2500: loss 0.869361 : training accuracy 0.697700, and val accuracy 0.697700\n",
      "epochs 1934 / 2500: loss 0.854476 : training accuracy 0.683900, and val accuracy 0.683900\n",
      "epochs 1935 / 2500: loss 0.863481 : training accuracy 0.686800, and val accuracy 0.686800\n",
      "epochs 1936 / 2500: loss 0.880206 : training accuracy 0.690500, and val accuracy 0.690500\n",
      "epochs 1937 / 2500: loss 0.853425 : training accuracy 0.707200, and val accuracy 0.707200\n",
      "epochs 1938 / 2500: loss 0.907345 : training accuracy 0.697500, and val accuracy 0.697500\n",
      "epochs 1939 / 2500: loss 0.961107 : training accuracy 0.685800, and val accuracy 0.685800\n",
      "epochs 1940 / 2500: loss 0.890850 : training accuracy 0.693200, and val accuracy 0.693200\n",
      "epochs 1941 / 2500: loss 0.851594 : training accuracy 0.704100, and val accuracy 0.704100\n",
      "epochs 1942 / 2500: loss 0.817155 : training accuracy 0.717500, and val accuracy 0.717500\n",
      "epochs 1943 / 2500: loss 0.852474 : training accuracy 0.703100, and val accuracy 0.703100\n",
      "epochs 1944 / 2500: loss 0.900965 : training accuracy 0.699800, and val accuracy 0.699800\n",
      "epochs 1945 / 2500: loss 0.871686 : training accuracy 0.709700, and val accuracy 0.709700\n",
      "epochs 1946 / 2500: loss 0.885222 : training accuracy 0.684100, and val accuracy 0.684100\n",
      "epochs 1947 / 2500: loss 0.915271 : training accuracy 0.697800, and val accuracy 0.697800\n",
      "epochs 1948 / 2500: loss 0.878189 : training accuracy 0.713800, and val accuracy 0.713800\n",
      "epochs 1949 / 2500: loss 0.822099 : training accuracy 0.704600, and val accuracy 0.704600\n",
      "epochs 1950 / 2500: loss 0.825213 : training accuracy 0.711600, and val accuracy 0.711600\n",
      "epochs 1951 / 2500: loss 0.902800 : training accuracy 0.692900, and val accuracy 0.692900\n",
      "epochs 1952 / 2500: loss 0.899480 : training accuracy 0.675600, and val accuracy 0.675600\n",
      "epochs 1953 / 2500: loss 0.851895 : training accuracy 0.685600, and val accuracy 0.685600\n",
      "epochs 1954 / 2500: loss 0.866694 : training accuracy 0.697200, and val accuracy 0.697200\n",
      "epochs 1955 / 2500: loss 0.843160 : training accuracy 0.692600, and val accuracy 0.692600\n",
      "epochs 1956 / 2500: loss 0.819507 : training accuracy 0.704800, and val accuracy 0.704800\n",
      "epochs 1957 / 2500: loss 0.856991 : training accuracy 0.697100, and val accuracy 0.697100\n",
      "epochs 1958 / 2500: loss 0.874373 : training accuracy 0.702500, and val accuracy 0.702500\n",
      "epochs 1959 / 2500: loss 0.952778 : training accuracy 0.697400, and val accuracy 0.697400\n",
      "epochs 1960 / 2500: loss 0.882872 : training accuracy 0.693300, and val accuracy 0.693300\n",
      "epochs 1961 / 2500: loss 0.846061 : training accuracy 0.707000, and val accuracy 0.707000\n",
      "epochs 1962 / 2500: loss 0.832695 : training accuracy 0.713800, and val accuracy 0.713800\n",
      "epochs 1963 / 2500: loss 0.873131 : training accuracy 0.703400, and val accuracy 0.703400\n",
      "epochs 1964 / 2500: loss 0.881901 : training accuracy 0.705600, and val accuracy 0.705600\n",
      "epochs 1965 / 2500: loss 0.912664 : training accuracy 0.695000, and val accuracy 0.695000\n",
      "epochs 1966 / 2500: loss 0.869297 : training accuracy 0.712000, and val accuracy 0.712000\n",
      "epochs 1967 / 2500: loss 0.826966 : training accuracy 0.704900, and val accuracy 0.704900\n",
      "epochs 1968 / 2500: loss 0.861112 : training accuracy 0.684100, and val accuracy 0.684100\n",
      "epochs 1969 / 2500: loss 0.889842 : training accuracy 0.686800, and val accuracy 0.686800\n",
      "epochs 1970 / 2500: loss 0.851221 : training accuracy 0.700800, and val accuracy 0.700800\n",
      "epochs 1971 / 2500: loss 0.858921 : training accuracy 0.713300, and val accuracy 0.713300\n",
      "epochs 1972 / 2500: loss 0.914247 : training accuracy 0.704800, and val accuracy 0.704800\n",
      "epochs 1973 / 2500: loss 0.875265 : training accuracy 0.712000, and val accuracy 0.712000\n",
      "epochs 1974 / 2500: loss 0.845386 : training accuracy 0.715800, and val accuracy 0.715800\n",
      "epochs 1975 / 2500: loss 0.827715 : training accuracy 0.722600, and val accuracy 0.722600\n",
      "epochs 1976 / 2500: loss 0.823421 : training accuracy 0.728900, and val accuracy 0.728900\n",
      "epochs 1977 / 2500: loss 0.813370 : training accuracy 0.727800, and val accuracy 0.727800\n",
      "epochs 1978 / 2500: loss 0.827844 : training accuracy 0.701300, and val accuracy 0.701300\n",
      "epochs 1979 / 2500: loss 0.833539 : training accuracy 0.697200, and val accuracy 0.697200\n",
      "epochs 1980 / 2500: loss 0.840573 : training accuracy 0.702400, and val accuracy 0.702400\n",
      "epochs 1981 / 2500: loss 0.822686 : training accuracy 0.709600, and val accuracy 0.709600\n",
      "epochs 1982 / 2500: loss 0.810028 : training accuracy 0.711200, and val accuracy 0.711200\n",
      "epochs 1983 / 2500: loss 0.865005 : training accuracy 0.678800, and val accuracy 0.678800\n",
      "epochs 1984 / 2500: loss 0.821651 : training accuracy 0.687000, and val accuracy 0.687000\n",
      "epochs 1985 / 2500: loss 0.857667 : training accuracy 0.709000, and val accuracy 0.709000\n",
      "epochs 1986 / 2500: loss 0.899557 : training accuracy 0.691600, and val accuracy 0.691600\n",
      "epochs 1987 / 2500: loss 0.885631 : training accuracy 0.701200, and val accuracy 0.701200\n",
      "epochs 1988 / 2500: loss 0.862762 : training accuracy 0.694500, and val accuracy 0.694500\n",
      "epochs 1989 / 2500: loss 0.872658 : training accuracy 0.694200, and val accuracy 0.694200\n",
      "epochs 1990 / 2500: loss 0.856713 : training accuracy 0.704000, and val accuracy 0.704000\n",
      "epochs 1991 / 2500: loss 0.827376 : training accuracy 0.719500, and val accuracy 0.719500\n",
      "epochs 1992 / 2500: loss 0.830800 : training accuracy 0.714700, and val accuracy 0.714700\n",
      "epochs 1993 / 2500: loss 0.991650 : training accuracy 0.652200, and val accuracy 0.652200\n",
      "epochs 1994 / 2500: loss 0.849669 : training accuracy 0.713800, and val accuracy 0.713800\n",
      "epochs 1995 / 2500: loss 0.795937 : training accuracy 0.733800, and val accuracy 0.733800\n",
      "epochs 1996 / 2500: loss 0.940241 : training accuracy 0.669300, and val accuracy 0.669300\n",
      "epochs 1997 / 2500: loss 0.868373 : training accuracy 0.684700, and val accuracy 0.684700\n",
      "epochs 1998 / 2500: loss 0.830830 : training accuracy 0.698300, and val accuracy 0.698300\n",
      "epochs 1999 / 2500: loss 0.803906 : training accuracy 0.700900, and val accuracy 0.700900\n",
      "epochs 2000 / 2500: loss 0.828951 : training accuracy 0.712200, and val accuracy 0.712200\n",
      "epochs 2001 / 2500: loss 0.877486 : training accuracy 0.706700, and val accuracy 0.706700\n",
      "epochs 2002 / 2500: loss 0.796225 : training accuracy 0.718500, and val accuracy 0.718500\n",
      "epochs 2003 / 2500: loss 0.945898 : training accuracy 0.655100, and val accuracy 0.655100\n",
      "epochs 2004 / 2500: loss 0.836086 : training accuracy 0.696300, and val accuracy 0.696300\n",
      "epochs 2005 / 2500: loss 0.865878 : training accuracy 0.695800, and val accuracy 0.695800\n",
      "epochs 2006 / 2500: loss 0.918626 : training accuracy 0.680100, and val accuracy 0.680100\n",
      "epochs 2007 / 2500: loss 0.862754 : training accuracy 0.692300, and val accuracy 0.692300\n",
      "epochs 2008 / 2500: loss 0.847636 : training accuracy 0.705700, and val accuracy 0.705700\n",
      "epochs 2009 / 2500: loss 0.827116 : training accuracy 0.714900, and val accuracy 0.714900\n",
      "epochs 2010 / 2500: loss 0.811313 : training accuracy 0.712600, and val accuracy 0.712600\n",
      "epochs 2011 / 2500: loss 0.844139 : training accuracy 0.669000, and val accuracy 0.669000\n",
      "epochs 2012 / 2500: loss 0.819150 : training accuracy 0.700900, and val accuracy 0.700900\n",
      "epochs 2013 / 2500: loss 0.832911 : training accuracy 0.715500, and val accuracy 0.715500\n",
      "epochs 2014 / 2500: loss 0.833392 : training accuracy 0.701900, and val accuracy 0.701900\n",
      "epochs 2015 / 2500: loss 0.829332 : training accuracy 0.704300, and val accuracy 0.704300\n",
      "epochs 2016 / 2500: loss 0.826984 : training accuracy 0.714700, and val accuracy 0.714700\n",
      "epochs 2017 / 2500: loss 0.828684 : training accuracy 0.710800, and val accuracy 0.710800\n",
      "epochs 2018 / 2500: loss 0.807491 : training accuracy 0.714000, and val accuracy 0.714000\n",
      "epochs 2019 / 2500: loss 0.863360 : training accuracy 0.700300, and val accuracy 0.700300\n",
      "epochs 2020 / 2500: loss 0.889592 : training accuracy 0.672500, and val accuracy 0.672500\n",
      "epochs 2021 / 2500: loss 0.872155 : training accuracy 0.682200, and val accuracy 0.682200\n",
      "epochs 2022 / 2500: loss 0.835771 : training accuracy 0.710700, and val accuracy 0.710700\n",
      "epochs 2023 / 2500: loss 0.849183 : training accuracy 0.689800, and val accuracy 0.689800\n",
      "epochs 2024 / 2500: loss 0.840605 : training accuracy 0.694500, and val accuracy 0.694500\n",
      "epochs 2025 / 2500: loss 0.850185 : training accuracy 0.702600, and val accuracy 0.702600\n",
      "epochs 2026 / 2500: loss 0.869975 : training accuracy 0.694200, and val accuracy 0.694200\n",
      "epochs 2027 / 2500: loss 0.846307 : training accuracy 0.697500, and val accuracy 0.697500\n",
      "epochs 2028 / 2500: loss 0.924513 : training accuracy 0.682700, and val accuracy 0.682700\n",
      "epochs 2029 / 2500: loss 0.834489 : training accuracy 0.696800, and val accuracy 0.696800\n",
      "epochs 2030 / 2500: loss 0.829830 : training accuracy 0.702700, and val accuracy 0.702700\n",
      "epochs 2031 / 2500: loss 0.812415 : training accuracy 0.710200, and val accuracy 0.710200\n",
      "epochs 2032 / 2500: loss 0.798257 : training accuracy 0.715600, and val accuracy 0.715600\n",
      "epochs 2033 / 2500: loss 0.796009 : training accuracy 0.717900, and val accuracy 0.717900\n",
      "epochs 2034 / 2500: loss 0.883266 : training accuracy 0.695900, and val accuracy 0.695900\n",
      "epochs 2035 / 2500: loss 0.855037 : training accuracy 0.705000, and val accuracy 0.705000\n",
      "epochs 2036 / 2500: loss 0.820214 : training accuracy 0.717400, and val accuracy 0.717400\n",
      "epochs 2037 / 2500: loss 0.796443 : training accuracy 0.721000, and val accuracy 0.721000\n",
      "epochs 2038 / 2500: loss 0.856051 : training accuracy 0.676900, and val accuracy 0.676900\n",
      "epochs 2039 / 2500: loss 0.833792 : training accuracy 0.703000, and val accuracy 0.703000\n",
      "epochs 2040 / 2500: loss 0.844203 : training accuracy 0.720000, and val accuracy 0.720000\n",
      "epochs 2041 / 2500: loss 0.875812 : training accuracy 0.707200, and val accuracy 0.707200\n",
      "epochs 2042 / 2500: loss 0.840697 : training accuracy 0.705400, and val accuracy 0.705400\n",
      "epochs 2043 / 2500: loss 0.820087 : training accuracy 0.718400, and val accuracy 0.718400\n",
      "epochs 2044 / 2500: loss 0.882259 : training accuracy 0.687800, and val accuracy 0.687800\n",
      "epochs 2045 / 2500: loss 0.906970 : training accuracy 0.673800, and val accuracy 0.673800\n",
      "epochs 2046 / 2500: loss 0.811737 : training accuracy 0.723900, and val accuracy 0.723900\n",
      "epochs 2047 / 2500: loss 0.826552 : training accuracy 0.700500, and val accuracy 0.700500\n",
      "epochs 2048 / 2500: loss 0.840721 : training accuracy 0.704900, and val accuracy 0.704900\n",
      "epochs 2049 / 2500: loss 0.813684 : training accuracy 0.719800, and val accuracy 0.719800\n",
      "epochs 2050 / 2500: loss 0.797835 : training accuracy 0.725300, and val accuracy 0.725300\n",
      "epochs 2051 / 2500: loss 0.779546 : training accuracy 0.731900, and val accuracy 0.731900\n",
      "epochs 2052 / 2500: loss 0.819733 : training accuracy 0.707900, and val accuracy 0.707900\n",
      "epochs 2053 / 2500: loss 0.870647 : training accuracy 0.689700, and val accuracy 0.689700\n",
      "epochs 2054 / 2500: loss 0.829498 : training accuracy 0.699100, and val accuracy 0.699100\n",
      "epochs 2055 / 2500: loss 0.795057 : training accuracy 0.701800, and val accuracy 0.701800\n",
      "epochs 2056 / 2500: loss 0.783739 : training accuracy 0.714000, and val accuracy 0.714000\n",
      "epochs 2057 / 2500: loss 0.817417 : training accuracy 0.711600, and val accuracy 0.711600\n",
      "epochs 2058 / 2500: loss 0.896707 : training accuracy 0.697200, and val accuracy 0.697200\n",
      "epochs 2059 / 2500: loss 0.840532 : training accuracy 0.696700, and val accuracy 0.696700\n",
      "epochs 2060 / 2500: loss 0.853311 : training accuracy 0.717400, and val accuracy 0.717400\n",
      "epochs 2061 / 2500: loss 0.838494 : training accuracy 0.725100, and val accuracy 0.725100\n",
      "epochs 2062 / 2500: loss 0.839313 : training accuracy 0.703400, and val accuracy 0.703400\n",
      "epochs 2063 / 2500: loss 0.825005 : training accuracy 0.691300, and val accuracy 0.691300\n",
      "epochs 2064 / 2500: loss 0.819827 : training accuracy 0.704600, and val accuracy 0.704600\n",
      "epochs 2065 / 2500: loss 0.811289 : training accuracy 0.705200, and val accuracy 0.705200\n",
      "epochs 2066 / 2500: loss 0.793500 : training accuracy 0.726500, and val accuracy 0.726500\n",
      "epochs 2067 / 2500: loss 0.791439 : training accuracy 0.724200, and val accuracy 0.724200\n",
      "epochs 2068 / 2500: loss 0.848548 : training accuracy 0.703900, and val accuracy 0.703900\n",
      "epochs 2069 / 2500: loss 0.843976 : training accuracy 0.714200, and val accuracy 0.714200\n",
      "epochs 2070 / 2500: loss 0.787724 : training accuracy 0.724600, and val accuracy 0.724600\n",
      "epochs 2071 / 2500: loss 0.851426 : training accuracy 0.701400, and val accuracy 0.701400\n",
      "epochs 2072 / 2500: loss 0.825993 : training accuracy 0.710000, and val accuracy 0.710000\n",
      "epochs 2073 / 2500: loss 0.838526 : training accuracy 0.713000, and val accuracy 0.713000\n",
      "epochs 2074 / 2500: loss 0.817411 : training accuracy 0.719400, and val accuracy 0.719400\n",
      "epochs 2075 / 2500: loss 0.802256 : training accuracy 0.726700, and val accuracy 0.726700\n",
      "epochs 2076 / 2500: loss 0.829024 : training accuracy 0.723300, and val accuracy 0.723300\n",
      "epochs 2077 / 2500: loss 0.902924 : training accuracy 0.667000, and val accuracy 0.667000\n",
      "epochs 2078 / 2500: loss 0.843747 : training accuracy 0.704800, and val accuracy 0.704800\n",
      "epochs 2079 / 2500: loss 0.782057 : training accuracy 0.730600, and val accuracy 0.730600\n",
      "epochs 2080 / 2500: loss 0.819906 : training accuracy 0.720800, and val accuracy 0.720800\n",
      "epochs 2081 / 2500: loss 0.809765 : training accuracy 0.726700, and val accuracy 0.726700\n",
      "epochs 2082 / 2500: loss 0.805236 : training accuracy 0.728100, and val accuracy 0.728100\n",
      "epochs 2083 / 2500: loss 0.846235 : training accuracy 0.700500, and val accuracy 0.700500\n",
      "epochs 2084 / 2500: loss 0.816693 : training accuracy 0.692600, and val accuracy 0.692600\n",
      "epochs 2085 / 2500: loss 0.810090 : training accuracy 0.695900, and val accuracy 0.695900\n",
      "epochs 2086 / 2500: loss 0.847161 : training accuracy 0.709600, and val accuracy 0.709600\n",
      "epochs 2087 / 2500: loss 0.848465 : training accuracy 0.711300, and val accuracy 0.711300\n",
      "epochs 2088 / 2500: loss 0.828423 : training accuracy 0.717300, and val accuracy 0.717300\n",
      "epochs 2089 / 2500: loss 0.847001 : training accuracy 0.712400, and val accuracy 0.712400\n",
      "epochs 2090 / 2500: loss 0.850857 : training accuracy 0.713200, and val accuracy 0.713200\n",
      "epochs 2091 / 2500: loss 0.784252 : training accuracy 0.734300, and val accuracy 0.734300\n",
      "epochs 2092 / 2500: loss 0.841295 : training accuracy 0.706100, and val accuracy 0.706100\n",
      "epochs 2093 / 2500: loss 0.859474 : training accuracy 0.688200, and val accuracy 0.688200\n",
      "epochs 2094 / 2500: loss 0.820261 : training accuracy 0.722600, and val accuracy 0.722600\n",
      "epochs 2095 / 2500: loss 0.820351 : training accuracy 0.723400, and val accuracy 0.723400\n",
      "epochs 2096 / 2500: loss 0.840549 : training accuracy 0.697400, and val accuracy 0.697400\n",
      "epochs 2097 / 2500: loss 0.818429 : training accuracy 0.711100, and val accuracy 0.711100\n",
      "epochs 2098 / 2500: loss 0.826422 : training accuracy 0.714900, and val accuracy 0.714900\n",
      "epochs 2099 / 2500: loss 0.816330 : training accuracy 0.695400, and val accuracy 0.695400\n",
      "epochs 2100 / 2500: loss 0.869955 : training accuracy 0.699200, and val accuracy 0.699200\n",
      "epochs 2101 / 2500: loss 0.857665 : training accuracy 0.715200, and val accuracy 0.715200\n",
      "epochs 2102 / 2500: loss 0.859354 : training accuracy 0.693500, and val accuracy 0.693500\n",
      "epochs 2103 / 2500: loss 0.834218 : training accuracy 0.695400, and val accuracy 0.695400\n",
      "epochs 2104 / 2500: loss 0.814656 : training accuracy 0.707500, and val accuracy 0.707500\n",
      "epochs 2105 / 2500: loss 0.791313 : training accuracy 0.735600, and val accuracy 0.735600\n",
      "epochs 2106 / 2500: loss 0.790949 : training accuracy 0.729100, and val accuracy 0.729100\n",
      "epochs 2107 / 2500: loss 0.858478 : training accuracy 0.701500, and val accuracy 0.701500\n",
      "epochs 2108 / 2500: loss 0.850067 : training accuracy 0.704600, and val accuracy 0.704600\n",
      "epochs 2109 / 2500: loss 0.766300 : training accuracy 0.738700, and val accuracy 0.738700\n",
      "epochs 2110 / 2500: loss 0.829027 : training accuracy 0.706700, and val accuracy 0.706700\n",
      "epochs 2111 / 2500: loss 0.784234 : training accuracy 0.718100, and val accuracy 0.718100\n",
      "epochs 2112 / 2500: loss 0.787217 : training accuracy 0.723800, and val accuracy 0.723800\n",
      "epochs 2113 / 2500: loss 0.780013 : training accuracy 0.724000, and val accuracy 0.724000\n",
      "epochs 2114 / 2500: loss 0.779476 : training accuracy 0.727800, and val accuracy 0.727800\n",
      "epochs 2115 / 2500: loss 0.783006 : training accuracy 0.725300, and val accuracy 0.725300\n",
      "epochs 2116 / 2500: loss 0.782094 : training accuracy 0.726200, and val accuracy 0.726200\n",
      "epochs 2117 / 2500: loss 0.835697 : training accuracy 0.722700, and val accuracy 0.722700\n",
      "epochs 2118 / 2500: loss 0.796362 : training accuracy 0.731300, and val accuracy 0.731300\n",
      "epochs 2119 / 2500: loss 0.835526 : training accuracy 0.706500, and val accuracy 0.706500\n",
      "epochs 2120 / 2500: loss 0.888227 : training accuracy 0.710300, and val accuracy 0.710300\n",
      "epochs 2121 / 2500: loss 0.780249 : training accuracy 0.723900, and val accuracy 0.723900\n",
      "epochs 2122 / 2500: loss 0.774953 : training accuracy 0.709500, and val accuracy 0.709500\n",
      "epochs 2123 / 2500: loss 0.758512 : training accuracy 0.725700, and val accuracy 0.725700\n",
      "epochs 2124 / 2500: loss 0.806825 : training accuracy 0.703500, and val accuracy 0.703500\n",
      "epochs 2125 / 2500: loss 0.823516 : training accuracy 0.716700, and val accuracy 0.716700\n",
      "epochs 2126 / 2500: loss 0.808524 : training accuracy 0.714700, and val accuracy 0.714700\n",
      "epochs 2127 / 2500: loss 0.795669 : training accuracy 0.719800, and val accuracy 0.719800\n",
      "epochs 2128 / 2500: loss 0.812611 : training accuracy 0.709800, and val accuracy 0.709800\n",
      "epochs 2129 / 2500: loss 0.786229 : training accuracy 0.728100, and val accuracy 0.728100\n",
      "epochs 2130 / 2500: loss 0.818698 : training accuracy 0.714500, and val accuracy 0.714500\n",
      "epochs 2131 / 2500: loss 0.835653 : training accuracy 0.711900, and val accuracy 0.711900\n",
      "epochs 2132 / 2500: loss 0.834718 : training accuracy 0.708000, and val accuracy 0.708000\n",
      "epochs 2133 / 2500: loss 0.821893 : training accuracy 0.720100, and val accuracy 0.720100\n",
      "epochs 2134 / 2500: loss 0.850404 : training accuracy 0.704500, and val accuracy 0.704500\n",
      "epochs 2135 / 2500: loss 0.844593 : training accuracy 0.716800, and val accuracy 0.716800\n",
      "epochs 2136 / 2500: loss 0.838266 : training accuracy 0.722800, and val accuracy 0.722800\n",
      "epochs 2137 / 2500: loss 0.794560 : training accuracy 0.731300, and val accuracy 0.731300\n",
      "epochs 2138 / 2500: loss 0.780264 : training accuracy 0.717400, and val accuracy 0.717400\n",
      "epochs 2139 / 2500: loss 0.790344 : training accuracy 0.707000, and val accuracy 0.707000\n",
      "epochs 2140 / 2500: loss 0.861383 : training accuracy 0.691200, and val accuracy 0.691200\n",
      "epochs 2141 / 2500: loss 0.844945 : training accuracy 0.711500, and val accuracy 0.711500\n",
      "epochs 2142 / 2500: loss 0.844718 : training accuracy 0.724800, and val accuracy 0.724800\n",
      "epochs 2143 / 2500: loss 0.787138 : training accuracy 0.728400, and val accuracy 0.728400\n",
      "epochs 2144 / 2500: loss 0.832259 : training accuracy 0.722900, and val accuracy 0.722900\n",
      "epochs 2145 / 2500: loss 0.802625 : training accuracy 0.731100, and val accuracy 0.731100\n",
      "epochs 2146 / 2500: loss 0.813169 : training accuracy 0.723200, and val accuracy 0.723200\n",
      "epochs 2147 / 2500: loss 0.819991 : training accuracy 0.729200, and val accuracy 0.729200\n",
      "epochs 2148 / 2500: loss 0.829130 : training accuracy 0.712800, and val accuracy 0.712800\n",
      "epochs 2149 / 2500: loss 0.780876 : training accuracy 0.730600, and val accuracy 0.730600\n",
      "epochs 2150 / 2500: loss 0.849477 : training accuracy 0.703800, and val accuracy 0.703800\n",
      "epochs 2151 / 2500: loss 0.834965 : training accuracy 0.693900, and val accuracy 0.693900\n",
      "epochs 2152 / 2500: loss 0.839914 : training accuracy 0.695900, and val accuracy 0.695900\n",
      "epochs 2153 / 2500: loss 0.804689 : training accuracy 0.709800, and val accuracy 0.709800\n",
      "epochs 2154 / 2500: loss 0.789281 : training accuracy 0.718100, and val accuracy 0.718100\n",
      "epochs 2155 / 2500: loss 0.759439 : training accuracy 0.734100, and val accuracy 0.734100\n",
      "epochs 2156 / 2500: loss 0.768026 : training accuracy 0.742400, and val accuracy 0.742400\n",
      "epochs 2157 / 2500: loss 0.803873 : training accuracy 0.713100, and val accuracy 0.713100\n",
      "epochs 2158 / 2500: loss 0.836339 : training accuracy 0.719100, and val accuracy 0.719100\n",
      "epochs 2159 / 2500: loss 0.785152 : training accuracy 0.729600, and val accuracy 0.729600\n",
      "epochs 2160 / 2500: loss 0.828364 : training accuracy 0.723900, and val accuracy 0.723900\n",
      "epochs 2161 / 2500: loss 0.842366 : training accuracy 0.724700, and val accuracy 0.724700\n",
      "epochs 2162 / 2500: loss 0.837521 : training accuracy 0.726200, and val accuracy 0.726200\n",
      "epochs 2163 / 2500: loss 0.765099 : training accuracy 0.744900, and val accuracy 0.744900\n",
      "epochs 2164 / 2500: loss 0.747375 : training accuracy 0.743600, and val accuracy 0.743600\n",
      "epochs 2165 / 2500: loss 0.933684 : training accuracy 0.677500, and val accuracy 0.677500\n",
      "epochs 2166 / 2500: loss 0.820309 : training accuracy 0.711900, and val accuracy 0.711900\n",
      "epochs 2167 / 2500: loss 0.802619 : training accuracy 0.712300, and val accuracy 0.712300\n",
      "epochs 2168 / 2500: loss 0.787787 : training accuracy 0.720300, and val accuracy 0.720300\n",
      "epochs 2169 / 2500: loss 0.788221 : training accuracy 0.723800, and val accuracy 0.723800\n",
      "epochs 2170 / 2500: loss 0.843331 : training accuracy 0.719900, and val accuracy 0.719900\n",
      "epochs 2171 / 2500: loss 0.804148 : training accuracy 0.726100, and val accuracy 0.726100\n",
      "epochs 2172 / 2500: loss 0.778265 : training accuracy 0.733500, and val accuracy 0.733500\n",
      "epochs 2173 / 2500: loss 0.775039 : training accuracy 0.735700, and val accuracy 0.735700\n",
      "epochs 2174 / 2500: loss 0.775196 : training accuracy 0.729200, and val accuracy 0.729200\n",
      "epochs 2175 / 2500: loss 0.871360 : training accuracy 0.699900, and val accuracy 0.699900\n",
      "epochs 2176 / 2500: loss 0.761011 : training accuracy 0.724800, and val accuracy 0.724800\n",
      "epochs 2177 / 2500: loss 0.892204 : training accuracy 0.690300, and val accuracy 0.690300\n",
      "epochs 2178 / 2500: loss 0.924354 : training accuracy 0.687700, and val accuracy 0.687700\n",
      "epochs 2179 / 2500: loss 0.860880 : training accuracy 0.713800, and val accuracy 0.713800\n",
      "epochs 2180 / 2500: loss 0.889190 : training accuracy 0.708300, and val accuracy 0.708300\n",
      "epochs 2181 / 2500: loss 0.846472 : training accuracy 0.701100, and val accuracy 0.701100\n",
      "epochs 2182 / 2500: loss 0.825951 : training accuracy 0.711900, and val accuracy 0.711900\n",
      "epochs 2183 / 2500: loss 0.833731 : training accuracy 0.716100, and val accuracy 0.716100\n",
      "epochs 2184 / 2500: loss 0.846749 : training accuracy 0.722200, and val accuracy 0.722200\n",
      "epochs 2185 / 2500: loss 0.827944 : training accuracy 0.737200, and val accuracy 0.737200\n",
      "epochs 2186 / 2500: loss 0.778405 : training accuracy 0.734600, and val accuracy 0.734600\n",
      "epochs 2187 / 2500: loss 0.819335 : training accuracy 0.713100, and val accuracy 0.713100\n",
      "epochs 2188 / 2500: loss 0.784771 : training accuracy 0.737200, and val accuracy 0.737200\n",
      "epochs 2189 / 2500: loss 0.794282 : training accuracy 0.734700, and val accuracy 0.734700\n",
      "epochs 2190 / 2500: loss 0.826560 : training accuracy 0.726100, and val accuracy 0.726100\n",
      "epochs 2191 / 2500: loss 0.782892 : training accuracy 0.731800, and val accuracy 0.731800\n",
      "epochs 2192 / 2500: loss 0.856668 : training accuracy 0.701000, and val accuracy 0.701000\n",
      "epochs 2193 / 2500: loss 0.834613 : training accuracy 0.716600, and val accuracy 0.716600\n",
      "epochs 2194 / 2500: loss 0.783229 : training accuracy 0.735500, and val accuracy 0.735500\n",
      "epochs 2195 / 2500: loss 0.790308 : training accuracy 0.720700, and val accuracy 0.720700\n",
      "epochs 2196 / 2500: loss 0.852945 : training accuracy 0.730700, and val accuracy 0.730700\n",
      "epochs 2197 / 2500: loss 0.827562 : training accuracy 0.733900, and val accuracy 0.733900\n",
      "epochs 2198 / 2500: loss 0.797678 : training accuracy 0.741000, and val accuracy 0.741000\n",
      "epochs 2199 / 2500: loss 0.763642 : training accuracy 0.742200, and val accuracy 0.742200\n",
      "epochs 2200 / 2500: loss 0.755294 : training accuracy 0.733900, and val accuracy 0.733900\n",
      "epochs 2201 / 2500: loss 0.788944 : training accuracy 0.722700, and val accuracy 0.722700\n",
      "epochs 2202 / 2500: loss 0.785876 : training accuracy 0.709500, and val accuracy 0.709500\n",
      "epochs 2203 / 2500: loss 0.815989 : training accuracy 0.721500, and val accuracy 0.721500\n",
      "epochs 2204 / 2500: loss 0.802688 : training accuracy 0.722500, and val accuracy 0.722500\n",
      "epochs 2205 / 2500: loss 0.828920 : training accuracy 0.713800, and val accuracy 0.713800\n",
      "epochs 2206 / 2500: loss 0.789296 : training accuracy 0.722700, and val accuracy 0.722700\n",
      "epochs 2207 / 2500: loss 0.755976 : training accuracy 0.724000, and val accuracy 0.724000\n",
      "epochs 2208 / 2500: loss 0.749608 : training accuracy 0.735800, and val accuracy 0.735800\n",
      "epochs 2209 / 2500: loss 0.742430 : training accuracy 0.743600, and val accuracy 0.743600\n",
      "epochs 2210 / 2500: loss 0.755165 : training accuracy 0.734600, and val accuracy 0.734600\n",
      "epochs 2211 / 2500: loss 0.803180 : training accuracy 0.722200, and val accuracy 0.722200\n",
      "epochs 2212 / 2500: loss 0.756530 : training accuracy 0.728200, and val accuracy 0.728200\n",
      "epochs 2213 / 2500: loss 0.766181 : training accuracy 0.730400, and val accuracy 0.730400\n",
      "epochs 2214 / 2500: loss 0.892792 : training accuracy 0.681300, and val accuracy 0.681300\n",
      "epochs 2215 / 2500: loss 0.767228 : training accuracy 0.719700, and val accuracy 0.719700\n",
      "epochs 2216 / 2500: loss 0.762718 : training accuracy 0.726700, and val accuracy 0.726700\n",
      "epochs 2217 / 2500: loss 0.748311 : training accuracy 0.737000, and val accuracy 0.737000\n",
      "epochs 2218 / 2500: loss 0.775261 : training accuracy 0.719000, and val accuracy 0.719000\n",
      "epochs 2219 / 2500: loss 0.782088 : training accuracy 0.729400, and val accuracy 0.729400\n",
      "epochs 2220 / 2500: loss 0.779604 : training accuracy 0.732900, and val accuracy 0.732900\n",
      "epochs 2221 / 2500: loss 0.762363 : training accuracy 0.741500, and val accuracy 0.741500\n",
      "epochs 2222 / 2500: loss 0.769796 : training accuracy 0.739000, and val accuracy 0.739000\n",
      "epochs 2223 / 2500: loss 0.925942 : training accuracy 0.694800, and val accuracy 0.694800\n",
      "epochs 2224 / 2500: loss 0.783091 : training accuracy 0.712900, and val accuracy 0.712900\n",
      "epochs 2225 / 2500: loss 0.833254 : training accuracy 0.698700, and val accuracy 0.698700\n",
      "epochs 2226 / 2500: loss 0.819989 : training accuracy 0.679000, and val accuracy 0.679000\n",
      "epochs 2227 / 2500: loss 0.840639 : training accuracy 0.700900, and val accuracy 0.700900\n",
      "epochs 2228 / 2500: loss 0.784914 : training accuracy 0.722600, and val accuracy 0.722600\n",
      "epochs 2229 / 2500: loss 0.732916 : training accuracy 0.739100, and val accuracy 0.739100\n",
      "epochs 2230 / 2500: loss 0.739554 : training accuracy 0.746700, and val accuracy 0.746700\n",
      "epochs 2231 / 2500: loss 0.826674 : training accuracy 0.739800, and val accuracy 0.739800\n",
      "epochs 2232 / 2500: loss 0.815537 : training accuracy 0.745400, and val accuracy 0.745400\n",
      "epochs 2233 / 2500: loss 0.781466 : training accuracy 0.746200, and val accuracy 0.746200\n",
      "epochs 2234 / 2500: loss 0.860087 : training accuracy 0.680100, and val accuracy 0.680100\n",
      "epochs 2235 / 2500: loss 0.792133 : training accuracy 0.724300, and val accuracy 0.724300\n",
      "epochs 2236 / 2500: loss 0.763800 : training accuracy 0.722500, and val accuracy 0.722500\n",
      "epochs 2237 / 2500: loss 0.781221 : training accuracy 0.731800, and val accuracy 0.731800\n",
      "epochs 2238 / 2500: loss 0.850760 : training accuracy 0.687700, and val accuracy 0.687700\n",
      "epochs 2239 / 2500: loss 0.804793 : training accuracy 0.709000, and val accuracy 0.709000\n",
      "epochs 2240 / 2500: loss 0.777095 : training accuracy 0.724700, and val accuracy 0.724700\n",
      "epochs 2241 / 2500: loss 0.759914 : training accuracy 0.737000, and val accuracy 0.737000\n",
      "epochs 2242 / 2500: loss 0.796976 : training accuracy 0.726500, and val accuracy 0.726500\n",
      "epochs 2243 / 2500: loss 0.823727 : training accuracy 0.701700, and val accuracy 0.701700\n",
      "epochs 2244 / 2500: loss 0.779475 : training accuracy 0.721800, and val accuracy 0.721800\n",
      "epochs 2245 / 2500: loss 0.752922 : training accuracy 0.727600, and val accuracy 0.727600\n",
      "epochs 2246 / 2500: loss 0.779146 : training accuracy 0.734500, and val accuracy 0.734500\n",
      "epochs 2247 / 2500: loss 0.854165 : training accuracy 0.726200, and val accuracy 0.726200\n",
      "epochs 2248 / 2500: loss 0.796161 : training accuracy 0.737100, and val accuracy 0.737100\n",
      "epochs 2249 / 2500: loss 0.810142 : training accuracy 0.717200, and val accuracy 0.717200\n",
      "epochs 2250 / 2500: loss 0.846232 : training accuracy 0.706900, and val accuracy 0.706900\n",
      "epochs 2251 / 2500: loss 0.786573 : training accuracy 0.731900, and val accuracy 0.731900\n",
      "epochs 2252 / 2500: loss 0.809563 : training accuracy 0.735600, and val accuracy 0.735600\n",
      "epochs 2253 / 2500: loss 0.753834 : training accuracy 0.748400, and val accuracy 0.748400\n",
      "epochs 2254 / 2500: loss 0.836533 : training accuracy 0.721200, and val accuracy 0.721200\n",
      "epochs 2255 / 2500: loss 0.792647 : training accuracy 0.740100, and val accuracy 0.740100\n",
      "epochs 2256 / 2500: loss 0.772773 : training accuracy 0.741900, and val accuracy 0.741900\n",
      "epochs 2257 / 2500: loss 0.752853 : training accuracy 0.743900, and val accuracy 0.743900\n",
      "epochs 2258 / 2500: loss 0.751655 : training accuracy 0.730400, and val accuracy 0.730400\n",
      "epochs 2259 / 2500: loss 0.775661 : training accuracy 0.726600, and val accuracy 0.726600\n",
      "epochs 2260 / 2500: loss 0.763301 : training accuracy 0.712100, and val accuracy 0.712100\n",
      "epochs 2261 / 2500: loss 0.744923 : training accuracy 0.731700, and val accuracy 0.731700\n",
      "epochs 2262 / 2500: loss 0.756108 : training accuracy 0.733300, and val accuracy 0.733300\n",
      "epochs 2263 / 2500: loss 0.756046 : training accuracy 0.732900, and val accuracy 0.732900\n",
      "epochs 2264 / 2500: loss 0.758150 : training accuracy 0.735900, and val accuracy 0.735900\n",
      "epochs 2265 / 2500: loss 0.863444 : training accuracy 0.715000, and val accuracy 0.715000\n",
      "epochs 2266 / 2500: loss 0.767139 : training accuracy 0.750900, and val accuracy 0.750900\n",
      "epochs 2267 / 2500: loss 0.755168 : training accuracy 0.746100, and val accuracy 0.746100\n",
      "epochs 2268 / 2500: loss 0.754031 : training accuracy 0.730200, and val accuracy 0.730200\n",
      "epochs 2269 / 2500: loss 0.768031 : training accuracy 0.726000, and val accuracy 0.726000\n",
      "epochs 2270 / 2500: loss 0.789958 : training accuracy 0.716600, and val accuracy 0.716600\n",
      "epochs 2271 / 2500: loss 0.801145 : training accuracy 0.736100, and val accuracy 0.736100\n",
      "epochs 2272 / 2500: loss 0.784380 : training accuracy 0.742200, and val accuracy 0.742200\n",
      "epochs 2273 / 2500: loss 0.790249 : training accuracy 0.756100, and val accuracy 0.756100\n",
      "epochs 2274 / 2500: loss 0.780217 : training accuracy 0.740600, and val accuracy 0.740600\n",
      "epochs 2275 / 2500: loss 0.795483 : training accuracy 0.737500, and val accuracy 0.737500\n",
      "epochs 2276 / 2500: loss 0.781259 : training accuracy 0.730900, and val accuracy 0.730900\n",
      "epochs 2277 / 2500: loss 0.780163 : training accuracy 0.724600, and val accuracy 0.724600\n",
      "epochs 2278 / 2500: loss 0.760138 : training accuracy 0.728500, and val accuracy 0.728500\n",
      "epochs 2279 / 2500: loss 0.813691 : training accuracy 0.711000, and val accuracy 0.711000\n",
      "epochs 2280 / 2500: loss 0.798927 : training accuracy 0.719400, and val accuracy 0.719400\n",
      "epochs 2281 / 2500: loss 0.744262 : training accuracy 0.744200, and val accuracy 0.744200\n",
      "epochs 2282 / 2500: loss 0.744768 : training accuracy 0.743400, and val accuracy 0.743400\n",
      "epochs 2283 / 2500: loss 0.782314 : training accuracy 0.721900, and val accuracy 0.721900\n",
      "epochs 2284 / 2500: loss 0.774993 : training accuracy 0.727600, and val accuracy 0.727600\n",
      "epochs 2285 / 2500: loss 0.830556 : training accuracy 0.718800, and val accuracy 0.718800\n",
      "epochs 2286 / 2500: loss 0.806704 : training accuracy 0.711900, and val accuracy 0.711900\n",
      "epochs 2287 / 2500: loss 0.771728 : training accuracy 0.727000, and val accuracy 0.727000\n",
      "epochs 2288 / 2500: loss 0.766480 : training accuracy 0.741900, and val accuracy 0.741900\n",
      "epochs 2289 / 2500: loss 0.791421 : training accuracy 0.721300, and val accuracy 0.721300\n",
      "epochs 2290 / 2500: loss 0.765572 : training accuracy 0.747300, and val accuracy 0.747300\n",
      "epochs 2291 / 2500: loss 0.838830 : training accuracy 0.729700, and val accuracy 0.729700\n",
      "epochs 2292 / 2500: loss 0.834581 : training accuracy 0.718700, and val accuracy 0.718700\n",
      "epochs 2293 / 2500: loss 0.791768 : training accuracy 0.715900, and val accuracy 0.715900\n",
      "epochs 2294 / 2500: loss 0.784341 : training accuracy 0.717700, and val accuracy 0.717700\n",
      "epochs 2295 / 2500: loss 0.750454 : training accuracy 0.728400, and val accuracy 0.728400\n",
      "epochs 2296 / 2500: loss 0.736033 : training accuracy 0.726100, and val accuracy 0.726100\n",
      "epochs 2297 / 2500: loss 0.726180 : training accuracy 0.732800, and val accuracy 0.732800\n",
      "epochs 2298 / 2500: loss 0.777932 : training accuracy 0.725600, and val accuracy 0.725600\n",
      "epochs 2299 / 2500: loss 0.879079 : training accuracy 0.685600, and val accuracy 0.685600\n",
      "epochs 2300 / 2500: loss 0.821679 : training accuracy 0.704900, and val accuracy 0.704900\n",
      "epochs 2301 / 2500: loss 0.742387 : training accuracy 0.743700, and val accuracy 0.743700\n",
      "epochs 2302 / 2500: loss 0.761804 : training accuracy 0.740800, and val accuracy 0.740800\n",
      "epochs 2303 / 2500: loss 0.767866 : training accuracy 0.733100, and val accuracy 0.733100\n",
      "epochs 2304 / 2500: loss 0.776186 : training accuracy 0.735000, and val accuracy 0.735000\n",
      "epochs 2305 / 2500: loss 0.794163 : training accuracy 0.743800, and val accuracy 0.743800\n",
      "epochs 2306 / 2500: loss 0.797075 : training accuracy 0.748100, and val accuracy 0.748100\n",
      "epochs 2307 / 2500: loss 0.800197 : training accuracy 0.738900, and val accuracy 0.738900\n",
      "epochs 2308 / 2500: loss 0.784691 : training accuracy 0.726800, and val accuracy 0.726800\n",
      "epochs 2309 / 2500: loss 0.719291 : training accuracy 0.744800, and val accuracy 0.744800\n",
      "epochs 2310 / 2500: loss 0.772387 : training accuracy 0.724500, and val accuracy 0.724500\n",
      "epochs 2311 / 2500: loss 0.824551 : training accuracy 0.701000, and val accuracy 0.701000\n",
      "epochs 2312 / 2500: loss 0.766358 : training accuracy 0.738500, and val accuracy 0.738500\n",
      "epochs 2313 / 2500: loss 0.730659 : training accuracy 0.757200, and val accuracy 0.757200\n",
      "epochs 2314 / 2500: loss 0.871485 : training accuracy 0.713700, and val accuracy 0.713700\n",
      "epochs 2315 / 2500: loss 0.793532 : training accuracy 0.733300, and val accuracy 0.733300\n",
      "epochs 2316 / 2500: loss 0.770542 : training accuracy 0.742000, and val accuracy 0.742000\n",
      "epochs 2317 / 2500: loss 0.774031 : training accuracy 0.734600, and val accuracy 0.734600\n",
      "epochs 2318 / 2500: loss 0.739518 : training accuracy 0.740400, and val accuracy 0.740400\n",
      "epochs 2319 / 2500: loss 0.769928 : training accuracy 0.714500, and val accuracy 0.714500\n",
      "epochs 2320 / 2500: loss 0.746682 : training accuracy 0.738700, and val accuracy 0.738700\n",
      "epochs 2321 / 2500: loss 0.707219 : training accuracy 0.747500, and val accuracy 0.747500\n",
      "epochs 2322 / 2500: loss 0.839066 : training accuracy 0.706000, and val accuracy 0.706000\n",
      "epochs 2323 / 2500: loss 0.755173 : training accuracy 0.731400, and val accuracy 0.731400\n",
      "epochs 2324 / 2500: loss 0.747040 : training accuracy 0.735100, and val accuracy 0.735100\n",
      "epochs 2325 / 2500: loss 0.872344 : training accuracy 0.708700, and val accuracy 0.708700\n",
      "epochs 2326 / 2500: loss 0.801169 : training accuracy 0.705000, and val accuracy 0.705000\n",
      "epochs 2327 / 2500: loss 0.784891 : training accuracy 0.718900, and val accuracy 0.718900\n",
      "epochs 2328 / 2500: loss 0.760388 : training accuracy 0.738600, and val accuracy 0.738600\n",
      "epochs 2329 / 2500: loss 0.748666 : training accuracy 0.746900, and val accuracy 0.746900\n",
      "epochs 2330 / 2500: loss 0.743831 : training accuracy 0.753800, and val accuracy 0.753800\n",
      "epochs 2331 / 2500: loss 0.764687 : training accuracy 0.734200, and val accuracy 0.734200\n",
      "epochs 2332 / 2500: loss 0.864132 : training accuracy 0.706600, and val accuracy 0.706600\n",
      "epochs 2333 / 2500: loss 0.797793 : training accuracy 0.718000, and val accuracy 0.718000\n",
      "epochs 2334 / 2500: loss 0.760936 : training accuracy 0.733800, and val accuracy 0.733800\n",
      "epochs 2335 / 2500: loss 0.801398 : training accuracy 0.723200, and val accuracy 0.723200\n",
      "epochs 2336 / 2500: loss 0.828622 : training accuracy 0.715600, and val accuracy 0.715600\n",
      "epochs 2337 / 2500: loss 0.749720 : training accuracy 0.745200, and val accuracy 0.745200\n",
      "epochs 2338 / 2500: loss 0.704806 : training accuracy 0.753000, and val accuracy 0.753000\n",
      "epochs 2339 / 2500: loss 0.763813 : training accuracy 0.741000, and val accuracy 0.741000\n",
      "epochs 2340 / 2500: loss 0.749579 : training accuracy 0.756900, and val accuracy 0.756900\n",
      "epochs 2341 / 2500: loss 0.757148 : training accuracy 0.745700, and val accuracy 0.745700\n",
      "epochs 2342 / 2500: loss 0.767112 : training accuracy 0.732600, and val accuracy 0.732600\n",
      "epochs 2343 / 2500: loss 0.818535 : training accuracy 0.728200, and val accuracy 0.728200\n",
      "epochs 2344 / 2500: loss 0.804724 : training accuracy 0.713600, and val accuracy 0.713600\n",
      "epochs 2345 / 2500: loss 0.792645 : training accuracy 0.720000, and val accuracy 0.720000\n",
      "epochs 2346 / 2500: loss 0.739769 : training accuracy 0.748600, and val accuracy 0.748600\n",
      "epochs 2347 / 2500: loss 0.796962 : training accuracy 0.738400, and val accuracy 0.738400\n",
      "epochs 2348 / 2500: loss 0.755633 : training accuracy 0.747700, and val accuracy 0.747700\n",
      "epochs 2349 / 2500: loss 0.781953 : training accuracy 0.723300, and val accuracy 0.723300\n",
      "epochs 2350 / 2500: loss 0.788820 : training accuracy 0.719200, and val accuracy 0.719200\n",
      "epochs 2351 / 2500: loss 0.817489 : training accuracy 0.721800, and val accuracy 0.721800\n",
      "epochs 2352 / 2500: loss 0.744380 : training accuracy 0.731400, and val accuracy 0.731400\n",
      "epochs 2353 / 2500: loss 0.737854 : training accuracy 0.732600, and val accuracy 0.732600\n",
      "epochs 2354 / 2500: loss 0.745685 : training accuracy 0.740900, and val accuracy 0.740900\n",
      "epochs 2355 / 2500: loss 0.731878 : training accuracy 0.754500, and val accuracy 0.754500\n",
      "epochs 2356 / 2500: loss 0.714889 : training accuracy 0.760400, and val accuracy 0.760400\n",
      "epochs 2357 / 2500: loss 0.749331 : training accuracy 0.719800, and val accuracy 0.719800\n",
      "epochs 2358 / 2500: loss 0.723015 : training accuracy 0.739800, and val accuracy 0.739800\n",
      "epochs 2359 / 2500: loss 0.727760 : training accuracy 0.742900, and val accuracy 0.742900\n",
      "epochs 2360 / 2500: loss 0.735816 : training accuracy 0.739700, and val accuracy 0.739700\n",
      "epochs 2361 / 2500: loss 0.805100 : training accuracy 0.680300, and val accuracy 0.680300\n",
      "epochs 2362 / 2500: loss 0.770148 : training accuracy 0.721400, and val accuracy 0.721400\n",
      "epochs 2363 / 2500: loss 0.755808 : training accuracy 0.737000, and val accuracy 0.737000\n",
      "epochs 2364 / 2500: loss 0.711071 : training accuracy 0.743800, and val accuracy 0.743800\n",
      "epochs 2365 / 2500: loss 0.760049 : training accuracy 0.742000, and val accuracy 0.742000\n",
      "epochs 2366 / 2500: loss 0.729862 : training accuracy 0.746500, and val accuracy 0.746500\n",
      "epochs 2367 / 2500: loss 0.711170 : training accuracy 0.742700, and val accuracy 0.742700\n",
      "epochs 2368 / 2500: loss 0.789451 : training accuracy 0.726800, and val accuracy 0.726800\n",
      "epochs 2369 / 2500: loss 0.735219 : training accuracy 0.743600, and val accuracy 0.743600\n",
      "epochs 2370 / 2500: loss 0.840485 : training accuracy 0.729000, and val accuracy 0.729000\n",
      "epochs 2371 / 2500: loss 0.783026 : training accuracy 0.737200, and val accuracy 0.737200\n",
      "epochs 2372 / 2500: loss 0.759564 : training accuracy 0.740600, and val accuracy 0.740600\n",
      "epochs 2373 / 2500: loss 0.848386 : training accuracy 0.704100, and val accuracy 0.704100\n",
      "epochs 2374 / 2500: loss 0.681843 : training accuracy 0.761000, and val accuracy 0.761000\n",
      "epochs 2375 / 2500: loss 0.711311 : training accuracy 0.752000, and val accuracy 0.752000\n",
      "epochs 2376 / 2500: loss 0.839479 : training accuracy 0.734300, and val accuracy 0.734300\n",
      "epochs 2377 / 2500: loss 0.820905 : training accuracy 0.740200, and val accuracy 0.740200\n",
      "epochs 2378 / 2500: loss 0.775823 : training accuracy 0.749600, and val accuracy 0.749600\n",
      "epochs 2379 / 2500: loss 0.703476 : training accuracy 0.762100, and val accuracy 0.762100\n",
      "epochs 2380 / 2500: loss 0.729460 : training accuracy 0.738200, and val accuracy 0.738200\n",
      "epochs 2381 / 2500: loss 0.698239 : training accuracy 0.752400, and val accuracy 0.752400\n",
      "epochs 2382 / 2500: loss 0.694014 : training accuracy 0.761000, and val accuracy 0.761000\n",
      "epochs 2383 / 2500: loss 0.782433 : training accuracy 0.715000, and val accuracy 0.715000\n",
      "epochs 2384 / 2500: loss 0.746718 : training accuracy 0.733500, and val accuracy 0.733500\n",
      "epochs 2385 / 2500: loss 0.781535 : training accuracy 0.724600, and val accuracy 0.724600\n",
      "epochs 2386 / 2500: loss 0.732462 : training accuracy 0.740300, and val accuracy 0.740300\n",
      "epochs 2387 / 2500: loss 0.739057 : training accuracy 0.740600, and val accuracy 0.740600\n",
      "epochs 2388 / 2500: loss 0.733818 : training accuracy 0.748900, and val accuracy 0.748900\n",
      "epochs 2389 / 2500: loss 0.715667 : training accuracy 0.748500, and val accuracy 0.748500\n",
      "epochs 2390 / 2500: loss 0.839688 : training accuracy 0.713000, and val accuracy 0.713000\n",
      "epochs 2391 / 2500: loss 0.769222 : training accuracy 0.721900, and val accuracy 0.721900\n",
      "epochs 2392 / 2500: loss 0.755479 : training accuracy 0.735600, and val accuracy 0.735600\n",
      "epochs 2393 / 2500: loss 0.754512 : training accuracy 0.730900, and val accuracy 0.730900\n",
      "epochs 2394 / 2500: loss 0.753027 : training accuracy 0.732200, and val accuracy 0.732200\n",
      "epochs 2395 / 2500: loss 0.818731 : training accuracy 0.746600, and val accuracy 0.746600\n",
      "epochs 2396 / 2500: loss 0.773958 : training accuracy 0.735100, and val accuracy 0.735100\n",
      "epochs 2397 / 2500: loss 0.744495 : training accuracy 0.740500, and val accuracy 0.740500\n",
      "epochs 2398 / 2500: loss 0.737633 : training accuracy 0.748100, and val accuracy 0.748100\n",
      "epochs 2399 / 2500: loss 0.739720 : training accuracy 0.759400, and val accuracy 0.759400\n",
      "epochs 2400 / 2500: loss 0.722046 : training accuracy 0.753100, and val accuracy 0.753100\n",
      "epochs 2401 / 2500: loss 0.694699 : training accuracy 0.773800, and val accuracy 0.773800\n",
      "epochs 2402 / 2500: loss 0.755742 : training accuracy 0.739200, and val accuracy 0.739200\n",
      "epochs 2403 / 2500: loss 0.840081 : training accuracy 0.716700, and val accuracy 0.716700\n",
      "epochs 2404 / 2500: loss 0.777128 : training accuracy 0.742700, and val accuracy 0.742700\n",
      "epochs 2405 / 2500: loss 0.711920 : training accuracy 0.752000, and val accuracy 0.752000\n",
      "epochs 2406 / 2500: loss 0.729063 : training accuracy 0.740700, and val accuracy 0.740700\n",
      "epochs 2407 / 2500: loss 0.732213 : training accuracy 0.747800, and val accuracy 0.747800\n",
      "epochs 2408 / 2500: loss 0.739818 : training accuracy 0.743700, and val accuracy 0.743700\n",
      "epochs 2409 / 2500: loss 0.718348 : training accuracy 0.746700, and val accuracy 0.746700\n",
      "epochs 2410 / 2500: loss 0.749756 : training accuracy 0.726200, and val accuracy 0.726200\n",
      "epochs 2411 / 2500: loss 0.713847 : training accuracy 0.750700, and val accuracy 0.750700\n",
      "epochs 2412 / 2500: loss 0.784225 : training accuracy 0.728700, and val accuracy 0.728700\n",
      "epochs 2413 / 2500: loss 0.737086 : training accuracy 0.726500, and val accuracy 0.726500\n",
      "epochs 2414 / 2500: loss 0.730700 : training accuracy 0.728900, and val accuracy 0.728900\n",
      "epochs 2415 / 2500: loss 0.722808 : training accuracy 0.733900, and val accuracy 0.733900\n",
      "epochs 2416 / 2500: loss 0.731296 : training accuracy 0.736000, and val accuracy 0.736000\n",
      "epochs 2417 / 2500: loss 0.850213 : training accuracy 0.695200, and val accuracy 0.695200\n",
      "epochs 2418 / 2500: loss 0.800920 : training accuracy 0.735600, and val accuracy 0.735600\n",
      "epochs 2419 / 2500: loss 0.714527 : training accuracy 0.757900, and val accuracy 0.757900\n",
      "epochs 2420 / 2500: loss 0.771607 : training accuracy 0.721500, and val accuracy 0.721500\n",
      "epochs 2421 / 2500: loss 0.728306 : training accuracy 0.746600, and val accuracy 0.746600\n",
      "epochs 2422 / 2500: loss 0.705134 : training accuracy 0.760000, and val accuracy 0.760000\n",
      "epochs 2423 / 2500: loss 0.689024 : training accuracy 0.766000, and val accuracy 0.766000\n",
      "epochs 2424 / 2500: loss 0.737443 : training accuracy 0.737400, and val accuracy 0.737400\n",
      "epochs 2425 / 2500: loss 0.873219 : training accuracy 0.684200, and val accuracy 0.684200\n",
      "epochs 2426 / 2500: loss 0.779647 : training accuracy 0.731400, and val accuracy 0.731400\n",
      "epochs 2427 / 2500: loss 0.724002 : training accuracy 0.755800, and val accuracy 0.755800\n",
      "epochs 2428 / 2500: loss 0.679236 : training accuracy 0.771300, and val accuracy 0.771300\n",
      "epochs 2429 / 2500: loss 0.675821 : training accuracy 0.755600, and val accuracy 0.755600\n",
      "epochs 2430 / 2500: loss 0.832622 : training accuracy 0.730200, and val accuracy 0.730200\n",
      "epochs 2431 / 2500: loss 0.804060 : training accuracy 0.737000, and val accuracy 0.737000\n",
      "epochs 2432 / 2500: loss 0.750731 : training accuracy 0.758600, and val accuracy 0.758600\n",
      "epochs 2433 / 2500: loss 0.750424 : training accuracy 0.754800, and val accuracy 0.754800\n",
      "epochs 2434 / 2500: loss 0.734677 : training accuracy 0.754100, and val accuracy 0.754100\n",
      "epochs 2435 / 2500: loss 0.714551 : training accuracy 0.764900, and val accuracy 0.764900\n",
      "epochs 2436 / 2500: loss 0.688364 : training accuracy 0.770400, and val accuracy 0.770400\n",
      "epochs 2437 / 2500: loss 0.746402 : training accuracy 0.733300, and val accuracy 0.733300\n",
      "epochs 2438 / 2500: loss 0.761924 : training accuracy 0.746800, and val accuracy 0.746800\n",
      "epochs 2439 / 2500: loss 0.711851 : training accuracy 0.756900, and val accuracy 0.756900\n",
      "epochs 2440 / 2500: loss 0.763284 : training accuracy 0.745700, and val accuracy 0.745700\n",
      "epochs 2441 / 2500: loss 0.739527 : training accuracy 0.739300, and val accuracy 0.739300\n",
      "epochs 2442 / 2500: loss 0.725981 : training accuracy 0.745800, and val accuracy 0.745800\n",
      "epochs 2443 / 2500: loss 0.782019 : training accuracy 0.745100, and val accuracy 0.745100\n",
      "epochs 2444 / 2500: loss 0.734953 : training accuracy 0.744500, and val accuracy 0.744500\n",
      "epochs 2445 / 2500: loss 0.725101 : training accuracy 0.742200, and val accuracy 0.742200\n",
      "epochs 2446 / 2500: loss 0.705567 : training accuracy 0.759800, and val accuracy 0.759800\n",
      "epochs 2447 / 2500: loss 0.719649 : training accuracy 0.753400, and val accuracy 0.753400\n",
      "epochs 2448 / 2500: loss 0.730526 : training accuracy 0.751300, and val accuracy 0.751300\n",
      "epochs 2449 / 2500: loss 0.710133 : training accuracy 0.757500, and val accuracy 0.757500\n",
      "epochs 2450 / 2500: loss 0.757665 : training accuracy 0.741500, and val accuracy 0.741500\n",
      "epochs 2451 / 2500: loss 0.721874 : training accuracy 0.750800, and val accuracy 0.750800\n",
      "epochs 2452 / 2500: loss 0.708386 : training accuracy 0.763700, and val accuracy 0.763700\n",
      "epochs 2453 / 2500: loss 0.730493 : training accuracy 0.729800, and val accuracy 0.729800\n",
      "epochs 2454 / 2500: loss 0.712707 : training accuracy 0.755700, and val accuracy 0.755700\n",
      "epochs 2455 / 2500: loss 0.762884 : training accuracy 0.741700, and val accuracy 0.741700\n",
      "epochs 2456 / 2500: loss 0.723190 : training accuracy 0.758200, and val accuracy 0.758200\n",
      "epochs 2457 / 2500: loss 0.741829 : training accuracy 0.758300, and val accuracy 0.758300\n",
      "epochs 2458 / 2500: loss 0.728056 : training accuracy 0.762700, and val accuracy 0.762700\n",
      "epochs 2459 / 2500: loss 0.763542 : training accuracy 0.739900, and val accuracy 0.739900\n",
      "epochs 2460 / 2500: loss 0.738211 : training accuracy 0.740700, and val accuracy 0.740700\n",
      "epochs 2461 / 2500: loss 0.762586 : training accuracy 0.739500, and val accuracy 0.739500\n",
      "epochs 2462 / 2500: loss 0.742247 : training accuracy 0.753200, and val accuracy 0.753200\n",
      "epochs 2463 / 2500: loss 0.723401 : training accuracy 0.760700, and val accuracy 0.760700\n",
      "epochs 2464 / 2500: loss 0.797252 : training accuracy 0.733600, and val accuracy 0.733600\n",
      "epochs 2465 / 2500: loss 0.783595 : training accuracy 0.737000, and val accuracy 0.737000\n",
      "epochs 2466 / 2500: loss 0.715460 : training accuracy 0.742300, and val accuracy 0.742300\n",
      "epochs 2467 / 2500: loss 0.694283 : training accuracy 0.748400, and val accuracy 0.748400\n",
      "epochs 2468 / 2500: loss 0.683736 : training accuracy 0.749600, and val accuracy 0.749600\n",
      "epochs 2469 / 2500: loss 0.726172 : training accuracy 0.749400, and val accuracy 0.749400\n",
      "epochs 2470 / 2500: loss 0.713379 : training accuracy 0.742800, and val accuracy 0.742800\n",
      "epochs 2471 / 2500: loss 0.699791 : training accuracy 0.745700, and val accuracy 0.745700\n",
      "epochs 2472 / 2500: loss 0.718667 : training accuracy 0.750300, and val accuracy 0.750300\n",
      "epochs 2473 / 2500: loss 0.690726 : training accuracy 0.759500, and val accuracy 0.759500\n",
      "epochs 2474 / 2500: loss 0.695742 : training accuracy 0.752700, and val accuracy 0.752700\n",
      "epochs 2475 / 2500: loss 0.729299 : training accuracy 0.744800, and val accuracy 0.744800\n",
      "epochs 2476 / 2500: loss 0.684594 : training accuracy 0.781300, and val accuracy 0.781300\n",
      "epochs 2477 / 2500: loss 0.712489 : training accuracy 0.738900, and val accuracy 0.738900\n",
      "epochs 2478 / 2500: loss 0.739253 : training accuracy 0.742100, and val accuracy 0.742100\n",
      "epochs 2479 / 2500: loss 0.702045 : training accuracy 0.754400, and val accuracy 0.754400\n",
      "epochs 2480 / 2500: loss 0.683417 : training accuracy 0.765100, and val accuracy 0.765100\n",
      "epochs 2481 / 2500: loss 0.753156 : training accuracy 0.742300, and val accuracy 0.742300\n",
      "epochs 2482 / 2500: loss 0.869777 : training accuracy 0.709900, and val accuracy 0.709900\n",
      "epochs 2483 / 2500: loss 0.827527 : training accuracy 0.744700, and val accuracy 0.744700\n",
      "epochs 2484 / 2500: loss 0.770037 : training accuracy 0.756800, and val accuracy 0.756800\n",
      "epochs 2485 / 2500: loss 0.778977 : training accuracy 0.765900, and val accuracy 0.765900\n",
      "epochs 2486 / 2500: loss 0.742161 : training accuracy 0.762500, and val accuracy 0.762500\n",
      "epochs 2487 / 2500: loss 0.798861 : training accuracy 0.722100, and val accuracy 0.722100\n",
      "epochs 2488 / 2500: loss 0.742920 : training accuracy 0.722900, and val accuracy 0.722900\n",
      "epochs 2489 / 2500: loss 0.695068 : training accuracy 0.755000, and val accuracy 0.755000\n",
      "epochs 2490 / 2500: loss 0.712334 : training accuracy 0.753100, and val accuracy 0.753100\n",
      "epochs 2491 / 2500: loss 0.697186 : training accuracy 0.757100, and val accuracy 0.757100\n",
      "epochs 2492 / 2500: loss 0.709273 : training accuracy 0.767200, and val accuracy 0.767200\n",
      "epochs 2493 / 2500: loss 0.771350 : training accuracy 0.740000, and val accuracy 0.740000\n",
      "epochs 2494 / 2500: loss 0.718751 : training accuracy 0.750400, and val accuracy 0.750400\n",
      "epochs 2495 / 2500: loss 0.726720 : training accuracy 0.750900, and val accuracy 0.750900\n",
      "epochs 2496 / 2500: loss 0.718021 : training accuracy 0.758400, and val accuracy 0.758400\n",
      "epochs 2497 / 2500: loss 0.733736 : training accuracy 0.752100, and val accuracy 0.752100\n",
      "epochs 2498 / 2500: loss 0.731468 : training accuracy 0.751800, and val accuracy 0.751800\n",
      "epochs 2499 / 2500: loss 0.697623 : training accuracy 0.756100, and val accuracy 0.756100\n"
     ]
    }
   ],
   "source": [
    "net=Two_layer_NN(32*32*3, 50, 10)\n",
    "loss,g=net.loss_grad(x_train,y=y_train)\n",
    "print loss\n",
    "net.train(x_train,y_train,x_train,y_train,learning_rate=1e-4,learning_rate_decay=1,num_epochs=2500,reg=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1=net.params['w1']\n",
    "b1=net.params['b1']\n",
    "w2=net.params['w2']\n",
    "b2=net.params['b2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy2(X, y):\n",
    "        \n",
    "    y_pred = None\n",
    "        \n",
    "    z1 = np.dot(X,w1) + b1\n",
    "    a1 = np.maximum(0, z1) # pass through ReLU activation function\n",
    "    scores = np.dot(a1,w2) + b2\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    acc = (y_pred == y).mean()\n",
    "    \n",
    "    return y_pred,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3974\n"
     ]
    }
   ],
   "source": [
    "pred,acc=accuracy2(x_test,y_test)\n",
    "print acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0 / 400: loss 2.302561 : training accuracy 0.134000, and val accuracy 0.134000\n",
      "epochs 1 / 400: loss 2.302511 : training accuracy 0.101100, and val accuracy 0.101100\n",
      "epochs 2 / 400: loss 2.302441 : training accuracy 0.100600, and val accuracy 0.100600\n",
      "epochs 3 / 400: loss 2.302339 : training accuracy 0.100600, and val accuracy 0.100600\n",
      "epochs 4 / 400: loss 2.302181 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 5 / 400: loss 2.301932 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 6 / 400: loss 2.301537 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 7 / 400: loss 2.300919 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 8 / 400: loss 2.299973 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 9 / 400: loss 2.298586 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 10 / 400: loss 2.296664 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 11 / 400: loss 2.294167 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 12 / 400: loss 2.291085 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "epochs 13 / 400: loss 2.287336 : training accuracy 0.103700, and val accuracy 0.103700\n",
      "epochs 14 / 400: loss 2.282725 : training accuracy 0.121000, and val accuracy 0.121000\n",
      "epochs 15 / 400: loss 2.277075 : training accuracy 0.107300, and val accuracy 0.107300\n",
      "epochs 16 / 400: loss 2.270365 : training accuracy 0.114900, and val accuracy 0.114900\n",
      "epochs 17 / 400: loss 2.262686 : training accuracy 0.124200, and val accuracy 0.124200\n",
      "epochs 18 / 400: loss 2.254083 : training accuracy 0.131500, and val accuracy 0.131500\n",
      "epochs 19 / 400: loss 2.244389 : training accuracy 0.139900, and val accuracy 0.139900\n",
      "epochs 20 / 400: loss 2.233319 : training accuracy 0.150900, and val accuracy 0.150900\n",
      "epochs 21 / 400: loss 2.220652 : training accuracy 0.162000, and val accuracy 0.162000\n",
      "epochs 22 / 400: loss 2.206510 : training accuracy 0.171400, and val accuracy 0.171400\n",
      "epochs 23 / 400: loss 2.191525 : training accuracy 0.176000, and val accuracy 0.176000\n",
      "epochs 24 / 400: loss 2.176587 : training accuracy 0.179400, and val accuracy 0.179400\n",
      "epochs 25 / 400: loss 2.162484 : training accuracy 0.181800, and val accuracy 0.181800\n",
      "epochs 26 / 400: loss 2.149732 : training accuracy 0.184400, and val accuracy 0.184400\n",
      "epochs 27 / 400: loss 2.138517 : training accuracy 0.186000, and val accuracy 0.186000\n",
      "epochs 28 / 400: loss 2.128779 : training accuracy 0.188200, and val accuracy 0.188200\n",
      "epochs 29 / 400: loss 2.120341 : training accuracy 0.189300, and val accuracy 0.189300\n",
      "epochs 30 / 400: loss 2.112971 : training accuracy 0.191000, and val accuracy 0.191000\n",
      "epochs 31 / 400: loss 2.106431 : training accuracy 0.192400, and val accuracy 0.192400\n",
      "epochs 32 / 400: loss 2.100512 : training accuracy 0.193400, and val accuracy 0.193400\n",
      "epochs 33 / 400: loss 2.095067 : training accuracy 0.194100, and val accuracy 0.194100\n",
      "epochs 34 / 400: loss 2.089963 : training accuracy 0.196700, and val accuracy 0.196700\n",
      "epochs 35 / 400: loss 2.085101 : training accuracy 0.199400, and val accuracy 0.199400\n",
      "epochs 36 / 400: loss 2.080391 : training accuracy 0.203100, and val accuracy 0.203100\n",
      "epochs 37 / 400: loss 2.075772 : training accuracy 0.210500, and val accuracy 0.210500\n",
      "epochs 38 / 400: loss 2.071191 : training accuracy 0.215800, and val accuracy 0.215800\n",
      "epochs 39 / 400: loss 2.066590 : training accuracy 0.222800, and val accuracy 0.222800\n",
      "epochs 40 / 400: loss 2.061946 : training accuracy 0.228800, and val accuracy 0.228800\n",
      "epochs 41 / 400: loss 2.057237 : training accuracy 0.234700, and val accuracy 0.234700\n",
      "epochs 42 / 400: loss 2.052458 : training accuracy 0.239100, and val accuracy 0.239100\n",
      "epochs 43 / 400: loss 2.047621 : training accuracy 0.241200, and val accuracy 0.241200\n",
      "epochs 44 / 400: loss 2.042745 : training accuracy 0.248000, and val accuracy 0.248000\n",
      "epochs 45 / 400: loss 2.037860 : training accuracy 0.250200, and val accuracy 0.250200\n",
      "epochs 46 / 400: loss 2.032988 : training accuracy 0.252900, and val accuracy 0.252900\n",
      "epochs 47 / 400: loss 2.028169 : training accuracy 0.254900, and val accuracy 0.254900\n",
      "epochs 48 / 400: loss 2.023424 : training accuracy 0.256200, and val accuracy 0.256200\n",
      "epochs 49 / 400: loss 2.018775 : training accuracy 0.256300, and val accuracy 0.256300\n",
      "epochs 50 / 400: loss 2.014241 : training accuracy 0.257700, and val accuracy 0.257700\n",
      "epochs 51 / 400: loss 2.009832 : training accuracy 0.259000, and val accuracy 0.259000\n",
      "epochs 52 / 400: loss 2.005548 : training accuracy 0.260500, and val accuracy 0.260500\n",
      "epochs 53 / 400: loss 2.001386 : training accuracy 0.262300, and val accuracy 0.262300\n",
      "epochs 54 / 400: loss 1.997336 : training accuracy 0.264200, and val accuracy 0.264200\n",
      "epochs 55 / 400: loss 1.993387 : training accuracy 0.266400, and val accuracy 0.266400\n",
      "epochs 56 / 400: loss 1.989526 : training accuracy 0.268600, and val accuracy 0.268600\n",
      "epochs 57 / 400: loss 1.985747 : training accuracy 0.270900, and val accuracy 0.270900\n",
      "epochs 58 / 400: loss 1.982034 : training accuracy 0.272500, and val accuracy 0.272500\n",
      "epochs 59 / 400: loss 1.978378 : training accuracy 0.274000, and val accuracy 0.274000\n",
      "epochs 60 / 400: loss 1.974772 : training accuracy 0.276000, and val accuracy 0.276000\n",
      "epochs 61 / 400: loss 1.971209 : training accuracy 0.278800, and val accuracy 0.278800\n",
      "epochs 62 / 400: loss 1.967698 : training accuracy 0.280600, and val accuracy 0.280600\n",
      "epochs 63 / 400: loss 1.964238 : training accuracy 0.282500, and val accuracy 0.282500\n",
      "epochs 64 / 400: loss 1.960831 : training accuracy 0.283900, and val accuracy 0.283900\n",
      "epochs 65 / 400: loss 1.957474 : training accuracy 0.285300, and val accuracy 0.285300\n",
      "epochs 66 / 400: loss 1.954167 : training accuracy 0.287100, and val accuracy 0.287100\n",
      "epochs 67 / 400: loss 1.950918 : training accuracy 0.288300, and val accuracy 0.288300\n",
      "epochs 68 / 400: loss 1.947725 : training accuracy 0.289300, and val accuracy 0.289300\n",
      "epochs 69 / 400: loss 1.944595 : training accuracy 0.291400, and val accuracy 0.291400\n",
      "epochs 70 / 400: loss 1.941533 : training accuracy 0.293100, and val accuracy 0.293100\n",
      "epochs 71 / 400: loss 1.938541 : training accuracy 0.293600, and val accuracy 0.293600\n",
      "epochs 72 / 400: loss 1.935618 : training accuracy 0.294800, and val accuracy 0.294800\n",
      "epochs 73 / 400: loss 1.932761 : training accuracy 0.296600, and val accuracy 0.296600\n",
      "epochs 74 / 400: loss 1.929971 : training accuracy 0.298700, and val accuracy 0.298700\n",
      "epochs 75 / 400: loss 1.927259 : training accuracy 0.299900, and val accuracy 0.299900\n",
      "epochs 76 / 400: loss 1.924622 : training accuracy 0.300000, and val accuracy 0.300000\n",
      "epochs 77 / 400: loss 1.922050 : training accuracy 0.301300, and val accuracy 0.301300\n",
      "epochs 78 / 400: loss 1.919541 : training accuracy 0.303000, and val accuracy 0.303000\n",
      "epochs 79 / 400: loss 1.917089 : training accuracy 0.304900, and val accuracy 0.304900\n",
      "epochs 80 / 400: loss 1.914684 : training accuracy 0.305300, and val accuracy 0.305300\n",
      "epochs 81 / 400: loss 1.912317 : training accuracy 0.306200, and val accuracy 0.306200\n",
      "epochs 82 / 400: loss 1.909977 : training accuracy 0.307600, and val accuracy 0.307600\n",
      "epochs 83 / 400: loss 1.907664 : training accuracy 0.308400, and val accuracy 0.308400\n",
      "epochs 84 / 400: loss 1.905361 : training accuracy 0.308600, and val accuracy 0.308600\n",
      "epochs 85 / 400: loss 1.903068 : training accuracy 0.310300, and val accuracy 0.310300\n",
      "epochs 86 / 400: loss 1.900786 : training accuracy 0.311000, and val accuracy 0.311000\n",
      "epochs 87 / 400: loss 1.898507 : training accuracy 0.312400, and val accuracy 0.312400\n",
      "epochs 88 / 400: loss 1.896227 : training accuracy 0.313800, and val accuracy 0.313800\n",
      "epochs 89 / 400: loss 1.893944 : training accuracy 0.315600, and val accuracy 0.315600\n",
      "epochs 90 / 400: loss 1.891648 : training accuracy 0.316600, and val accuracy 0.316600\n",
      "epochs 91 / 400: loss 1.889341 : training accuracy 0.317900, and val accuracy 0.317900\n",
      "epochs 92 / 400: loss 1.887013 : training accuracy 0.319300, and val accuracy 0.319300\n",
      "epochs 93 / 400: loss 1.884667 : training accuracy 0.320900, and val accuracy 0.320900\n",
      "epochs 94 / 400: loss 1.882317 : training accuracy 0.321500, and val accuracy 0.321500\n",
      "epochs 95 / 400: loss 1.879972 : training accuracy 0.322200, and val accuracy 0.322200\n",
      "epochs 96 / 400: loss 1.877620 : training accuracy 0.323600, and val accuracy 0.323600\n",
      "epochs 97 / 400: loss 1.875250 : training accuracy 0.324500, and val accuracy 0.324500\n",
      "epochs 98 / 400: loss 1.872878 : training accuracy 0.324200, and val accuracy 0.324200\n",
      "epochs 99 / 400: loss 1.870495 : training accuracy 0.325400, and val accuracy 0.325400\n",
      "epochs 100 / 400: loss 1.868097 : training accuracy 0.326500, and val accuracy 0.326500\n",
      "epochs 101 / 400: loss 1.865685 : training accuracy 0.328300, and val accuracy 0.328300\n",
      "epochs 102 / 400: loss 1.863272 : training accuracy 0.328500, and val accuracy 0.328500\n",
      "epochs 103 / 400: loss 1.860856 : training accuracy 0.329300, and val accuracy 0.329300\n",
      "epochs 104 / 400: loss 1.858432 : training accuracy 0.330400, and val accuracy 0.330400\n",
      "epochs 105 / 400: loss 1.856002 : training accuracy 0.331300, and val accuracy 0.331300\n",
      "epochs 106 / 400: loss 1.853572 : training accuracy 0.332700, and val accuracy 0.332700\n",
      "epochs 107 / 400: loss 1.851158 : training accuracy 0.333600, and val accuracy 0.333600\n",
      "epochs 108 / 400: loss 1.848744 : training accuracy 0.334300, and val accuracy 0.334300\n",
      "epochs 109 / 400: loss 1.846335 : training accuracy 0.335400, and val accuracy 0.335400\n",
      "epochs 110 / 400: loss 1.843939 : training accuracy 0.337700, and val accuracy 0.337700\n",
      "epochs 111 / 400: loss 1.841555 : training accuracy 0.338500, and val accuracy 0.338500\n",
      "epochs 112 / 400: loss 1.839188 : training accuracy 0.339600, and val accuracy 0.339600\n",
      "epochs 113 / 400: loss 1.836838 : training accuracy 0.340600, and val accuracy 0.340600\n",
      "epochs 114 / 400: loss 1.834505 : training accuracy 0.341200, and val accuracy 0.341200\n",
      "epochs 115 / 400: loss 1.832191 : training accuracy 0.342800, and val accuracy 0.342800\n",
      "epochs 116 / 400: loss 1.829897 : training accuracy 0.344600, and val accuracy 0.344600\n",
      "epochs 117 / 400: loss 1.827618 : training accuracy 0.344700, and val accuracy 0.344700\n",
      "epochs 118 / 400: loss 1.825360 : training accuracy 0.346100, and val accuracy 0.346100\n",
      "epochs 119 / 400: loss 1.823121 : training accuracy 0.347500, and val accuracy 0.347500\n",
      "epochs 120 / 400: loss 1.820902 : training accuracy 0.348400, and val accuracy 0.348400\n",
      "epochs 121 / 400: loss 1.818706 : training accuracy 0.348700, and val accuracy 0.348700\n",
      "epochs 122 / 400: loss 1.816536 : training accuracy 0.349600, and val accuracy 0.349600\n",
      "epochs 123 / 400: loss 1.814392 : training accuracy 0.350700, and val accuracy 0.350700\n",
      "epochs 124 / 400: loss 1.812272 : training accuracy 0.350400, and val accuracy 0.350400\n",
      "epochs 125 / 400: loss 1.810179 : training accuracy 0.350800, and val accuracy 0.350800\n",
      "epochs 126 / 400: loss 1.808097 : training accuracy 0.351400, and val accuracy 0.351400\n",
      "epochs 127 / 400: loss 1.806022 : training accuracy 0.353000, and val accuracy 0.353000\n",
      "epochs 128 / 400: loss 1.803964 : training accuracy 0.353500, and val accuracy 0.353500\n",
      "epochs 129 / 400: loss 1.801918 : training accuracy 0.354400, and val accuracy 0.354400\n",
      "epochs 130 / 400: loss 1.799898 : training accuracy 0.355000, and val accuracy 0.355000\n",
      "epochs 131 / 400: loss 1.797897 : training accuracy 0.356300, and val accuracy 0.356300\n",
      "epochs 132 / 400: loss 1.795921 : training accuracy 0.357500, and val accuracy 0.357500\n",
      "epochs 133 / 400: loss 1.793963 : training accuracy 0.358100, and val accuracy 0.358100\n",
      "epochs 134 / 400: loss 1.792024 : training accuracy 0.357900, and val accuracy 0.357900\n",
      "epochs 135 / 400: loss 1.790102 : training accuracy 0.358400, and val accuracy 0.358400\n",
      "epochs 136 / 400: loss 1.788199 : training accuracy 0.358800, and val accuracy 0.358800\n",
      "epochs 137 / 400: loss 1.786309 : training accuracy 0.359400, and val accuracy 0.359400\n",
      "epochs 138 / 400: loss 1.784440 : training accuracy 0.359400, and val accuracy 0.359400\n",
      "epochs 139 / 400: loss 1.782591 : training accuracy 0.359600, and val accuracy 0.359600\n",
      "epochs 140 / 400: loss 1.780749 : training accuracy 0.360500, and val accuracy 0.360500\n",
      "epochs 141 / 400: loss 1.778921 : training accuracy 0.361000, and val accuracy 0.361000\n",
      "epochs 142 / 400: loss 1.777110 : training accuracy 0.361700, and val accuracy 0.361700\n",
      "epochs 143 / 400: loss 1.775332 : training accuracy 0.363100, and val accuracy 0.363100\n",
      "epochs 144 / 400: loss 1.773616 : training accuracy 0.363800, and val accuracy 0.363800\n",
      "epochs 145 / 400: loss 1.772045 : training accuracy 0.364700, and val accuracy 0.364700\n",
      "epochs 146 / 400: loss 1.770866 : training accuracy 0.364000, and val accuracy 0.364000\n",
      "epochs 147 / 400: loss 1.770929 : training accuracy 0.364500, and val accuracy 0.364500\n",
      "epochs 148 / 400: loss 1.774258 : training accuracy 0.361700, and val accuracy 0.361700\n",
      "epochs 149 / 400: loss 1.781031 : training accuracy 0.361200, and val accuracy 0.361200\n",
      "epochs 150 / 400: loss 1.783193 : training accuracy 0.360900, and val accuracy 0.360900\n",
      "epochs 151 / 400: loss 1.778755 : training accuracy 0.362600, and val accuracy 0.362600\n",
      "epochs 152 / 400: loss 1.773807 : training accuracy 0.363300, and val accuracy 0.363300\n",
      "epochs 153 / 400: loss 1.769936 : training accuracy 0.364700, and val accuracy 0.364700\n",
      "epochs 154 / 400: loss 1.766858 : training accuracy 0.366100, and val accuracy 0.366100\n",
      "epochs 155 / 400: loss 1.764372 : training accuracy 0.366300, and val accuracy 0.366300\n",
      "epochs 156 / 400: loss 1.762304 : training accuracy 0.367100, and val accuracy 0.367100\n",
      "epochs 157 / 400: loss 1.760786 : training accuracy 0.368700, and val accuracy 0.368700\n",
      "epochs 158 / 400: loss 1.759530 : training accuracy 0.369300, and val accuracy 0.369300\n",
      "epochs 159 / 400: loss 1.757987 : training accuracy 0.369800, and val accuracy 0.369800\n",
      "epochs 160 / 400: loss 1.756283 : training accuracy 0.370200, and val accuracy 0.370200\n",
      "epochs 161 / 400: loss 1.754658 : training accuracy 0.371500, and val accuracy 0.371500\n",
      "epochs 162 / 400: loss 1.753558 : training accuracy 0.372500, and val accuracy 0.372500\n",
      "epochs 163 / 400: loss 1.752884 : training accuracy 0.372900, and val accuracy 0.372900\n",
      "epochs 164 / 400: loss 1.751929 : training accuracy 0.373700, and val accuracy 0.373700\n",
      "epochs 165 / 400: loss 1.750904 : training accuracy 0.373400, and val accuracy 0.373400\n",
      "epochs 166 / 400: loss 1.749165 : training accuracy 0.374600, and val accuracy 0.374600\n",
      "epochs 167 / 400: loss 1.747041 : training accuracy 0.374900, and val accuracy 0.374900\n",
      "epochs 168 / 400: loss 1.745172 : training accuracy 0.374600, and val accuracy 0.374600\n",
      "epochs 169 / 400: loss 1.743720 : training accuracy 0.375500, and val accuracy 0.375500\n",
      "epochs 170 / 400: loss 1.742253 : training accuracy 0.376000, and val accuracy 0.376000\n",
      "epochs 171 / 400: loss 1.740761 : training accuracy 0.375600, and val accuracy 0.375600\n",
      "epochs 172 / 400: loss 1.739140 : training accuracy 0.377300, and val accuracy 0.377300\n",
      "epochs 173 / 400: loss 1.737285 : training accuracy 0.378000, and val accuracy 0.378000\n",
      "epochs 174 / 400: loss 1.735291 : training accuracy 0.379200, and val accuracy 0.379200\n",
      "epochs 175 / 400: loss 1.733837 : training accuracy 0.380000, and val accuracy 0.380000\n",
      "epochs 176 / 400: loss 1.732159 : training accuracy 0.381100, and val accuracy 0.381100\n",
      "epochs 177 / 400: loss 1.730602 : training accuracy 0.382500, and val accuracy 0.382500\n",
      "epochs 178 / 400: loss 1.729022 : training accuracy 0.383200, and val accuracy 0.383200\n",
      "epochs 179 / 400: loss 1.727094 : training accuracy 0.384600, and val accuracy 0.384600\n",
      "epochs 180 / 400: loss 1.725050 : training accuracy 0.384600, and val accuracy 0.384600\n",
      "epochs 181 / 400: loss 1.723153 : training accuracy 0.385000, and val accuracy 0.385000\n",
      "epochs 182 / 400: loss 1.721456 : training accuracy 0.385100, and val accuracy 0.385100\n",
      "epochs 183 / 400: loss 1.719852 : training accuracy 0.386000, and val accuracy 0.386000\n",
      "epochs 184 / 400: loss 1.718399 : training accuracy 0.386600, and val accuracy 0.386600\n",
      "epochs 185 / 400: loss 1.717126 : training accuracy 0.387900, and val accuracy 0.387900\n",
      "epochs 186 / 400: loss 1.715930 : training accuracy 0.388300, and val accuracy 0.388300\n",
      "epochs 187 / 400: loss 1.714698 : training accuracy 0.388800, and val accuracy 0.388800\n",
      "epochs 188 / 400: loss 1.713167 : training accuracy 0.389800, and val accuracy 0.389800\n",
      "epochs 189 / 400: loss 1.711648 : training accuracy 0.390800, and val accuracy 0.390800\n",
      "epochs 190 / 400: loss 1.710351 : training accuracy 0.391300, and val accuracy 0.391300\n",
      "epochs 191 / 400: loss 1.709131 : training accuracy 0.392200, and val accuracy 0.392200\n",
      "epochs 192 / 400: loss 1.707819 : training accuracy 0.392800, and val accuracy 0.392800\n",
      "epochs 193 / 400: loss 1.705973 : training accuracy 0.392900, and val accuracy 0.392900\n",
      "epochs 194 / 400: loss 1.704249 : training accuracy 0.394000, and val accuracy 0.394000\n",
      "epochs 195 / 400: loss 1.702610 : training accuracy 0.395000, and val accuracy 0.395000\n",
      "epochs 196 / 400: loss 1.701123 : training accuracy 0.395400, and val accuracy 0.395400\n",
      "epochs 197 / 400: loss 1.699735 : training accuracy 0.395800, and val accuracy 0.395800\n",
      "epochs 198 / 400: loss 1.698503 : training accuracy 0.395700, and val accuracy 0.395700\n",
      "epochs 199 / 400: loss 1.697341 : training accuracy 0.396700, and val accuracy 0.396700\n",
      "epochs 200 / 400: loss 1.696125 : training accuracy 0.397200, and val accuracy 0.397200\n",
      "epochs 201 / 400: loss 1.695052 : training accuracy 0.397800, and val accuracy 0.397800\n",
      "epochs 202 / 400: loss 1.693827 : training accuracy 0.398600, and val accuracy 0.398600\n",
      "epochs 203 / 400: loss 1.692231 : training accuracy 0.398800, and val accuracy 0.398800\n",
      "epochs 204 / 400: loss 1.690586 : training accuracy 0.399200, and val accuracy 0.399200\n",
      "epochs 205 / 400: loss 1.689195 : training accuracy 0.400100, and val accuracy 0.400100\n",
      "epochs 206 / 400: loss 1.687637 : training accuracy 0.401100, and val accuracy 0.401100\n",
      "epochs 207 / 400: loss 1.686132 : training accuracy 0.401600, and val accuracy 0.401600\n",
      "epochs 208 / 400: loss 1.684898 : training accuracy 0.402200, and val accuracy 0.402200\n",
      "epochs 209 / 400: loss 1.683785 : training accuracy 0.403100, and val accuracy 0.403100\n",
      "epochs 210 / 400: loss 1.682597 : training accuracy 0.403400, and val accuracy 0.403400\n",
      "epochs 211 / 400: loss 1.681342 : training accuracy 0.404400, and val accuracy 0.404400\n",
      "epochs 212 / 400: loss 1.679837 : training accuracy 0.404900, and val accuracy 0.404900\n",
      "epochs 213 / 400: loss 1.678223 : training accuracy 0.405200, and val accuracy 0.405200\n",
      "epochs 214 / 400: loss 1.676815 : training accuracy 0.405400, and val accuracy 0.405400\n",
      "epochs 215 / 400: loss 1.675563 : training accuracy 0.405800, and val accuracy 0.405800\n",
      "epochs 216 / 400: loss 1.674387 : training accuracy 0.405800, and val accuracy 0.405800\n",
      "epochs 217 / 400: loss 1.672911 : training accuracy 0.405600, and val accuracy 0.405600\n",
      "epochs 218 / 400: loss 1.671609 : training accuracy 0.405600, and val accuracy 0.405600\n",
      "epochs 219 / 400: loss 1.670570 : training accuracy 0.405700, and val accuracy 0.405700\n",
      "epochs 220 / 400: loss 1.669817 : training accuracy 0.406700, and val accuracy 0.406700\n",
      "epochs 221 / 400: loss 1.668903 : training accuracy 0.407300, and val accuracy 0.407300\n",
      "epochs 222 / 400: loss 1.667591 : training accuracy 0.408300, and val accuracy 0.408300\n",
      "epochs 223 / 400: loss 1.665951 : training accuracy 0.409100, and val accuracy 0.409100\n",
      "epochs 224 / 400: loss 1.664206 : training accuracy 0.409300, and val accuracy 0.409300\n",
      "epochs 225 / 400: loss 1.662404 : training accuracy 0.409000, and val accuracy 0.409000\n",
      "epochs 226 / 400: loss 1.660776 : training accuracy 0.410600, and val accuracy 0.410600\n",
      "epochs 227 / 400: loss 1.659574 : training accuracy 0.411500, and val accuracy 0.411500\n",
      "epochs 228 / 400: loss 1.658623 : training accuracy 0.411900, and val accuracy 0.411900\n",
      "epochs 229 / 400: loss 1.657560 : training accuracy 0.411800, and val accuracy 0.411800\n",
      "epochs 230 / 400: loss 1.656711 : training accuracy 0.412300, and val accuracy 0.412300\n",
      "epochs 231 / 400: loss 1.657106 : training accuracy 0.411700, and val accuracy 0.411700\n",
      "epochs 232 / 400: loss 1.660110 : training accuracy 0.411800, and val accuracy 0.411800\n",
      "epochs 233 / 400: loss 1.663557 : training accuracy 0.408800, and val accuracy 0.408800\n",
      "epochs 234 / 400: loss 1.660362 : training accuracy 0.409700, and val accuracy 0.409700\n",
      "epochs 235 / 400: loss 1.655183 : training accuracy 0.412300, and val accuracy 0.412300\n",
      "epochs 236 / 400: loss 1.651848 : training accuracy 0.414200, and val accuracy 0.414200\n",
      "epochs 237 / 400: loss 1.649862 : training accuracy 0.414100, and val accuracy 0.414100\n",
      "epochs 238 / 400: loss 1.647978 : training accuracy 0.415400, and val accuracy 0.415400\n",
      "epochs 239 / 400: loss 1.646401 : training accuracy 0.415500, and val accuracy 0.415500\n",
      "epochs 240 / 400: loss 1.644898 : training accuracy 0.415700, and val accuracy 0.415700\n",
      "epochs 241 / 400: loss 1.643119 : training accuracy 0.416200, and val accuracy 0.416200\n",
      "epochs 242 / 400: loss 1.641634 : training accuracy 0.416800, and val accuracy 0.416800\n",
      "epochs 243 / 400: loss 1.640164 : training accuracy 0.417400, and val accuracy 0.417400\n",
      "epochs 244 / 400: loss 1.638895 : training accuracy 0.418900, and val accuracy 0.418900\n",
      "epochs 245 / 400: loss 1.638200 : training accuracy 0.418500, and val accuracy 0.418500\n",
      "epochs 246 / 400: loss 1.640026 : training accuracy 0.417900, and val accuracy 0.417900\n",
      "epochs 247 / 400: loss 1.649697 : training accuracy 0.411300, and val accuracy 0.411300\n",
      "epochs 248 / 400: loss 1.656906 : training accuracy 0.409100, and val accuracy 0.409100\n",
      "epochs 249 / 400: loss 1.651171 : training accuracy 0.411800, and val accuracy 0.411800\n",
      "epochs 250 / 400: loss 1.645479 : training accuracy 0.416400, and val accuracy 0.416400\n",
      "epochs 251 / 400: loss 1.641747 : training accuracy 0.418100, and val accuracy 0.418100\n",
      "epochs 252 / 400: loss 1.638277 : training accuracy 0.418800, and val accuracy 0.418800\n",
      "epochs 253 / 400: loss 1.635160 : training accuracy 0.420600, and val accuracy 0.420600\n",
      "epochs 254 / 400: loss 1.633531 : training accuracy 0.421400, and val accuracy 0.421400\n",
      "epochs 255 / 400: loss 1.631777 : training accuracy 0.422600, and val accuracy 0.422600\n",
      "epochs 256 / 400: loss 1.629558 : training accuracy 0.423400, and val accuracy 0.423400\n",
      "epochs 257 / 400: loss 1.628301 : training accuracy 0.424900, and val accuracy 0.424900\n",
      "epochs 258 / 400: loss 1.629835 : training accuracy 0.423300, and val accuracy 0.423300\n",
      "epochs 259 / 400: loss 1.638954 : training accuracy 0.419200, and val accuracy 0.419200\n",
      "epochs 260 / 400: loss 1.649860 : training accuracy 0.411200, and val accuracy 0.411200\n",
      "epochs 261 / 400: loss 1.642354 : training accuracy 0.415100, and val accuracy 0.415100\n",
      "epochs 262 / 400: loss 1.632308 : training accuracy 0.421500, and val accuracy 0.421500\n",
      "epochs 263 / 400: loss 1.626878 : training accuracy 0.424000, and val accuracy 0.424000\n",
      "epochs 264 / 400: loss 1.624904 : training accuracy 0.423800, and val accuracy 0.423800\n",
      "epochs 265 / 400: loss 1.624096 : training accuracy 0.424400, and val accuracy 0.424400\n",
      "epochs 266 / 400: loss 1.623433 : training accuracy 0.424000, and val accuracy 0.424000\n",
      "epochs 267 / 400: loss 1.622487 : training accuracy 0.424900, and val accuracy 0.424900\n",
      "epochs 268 / 400: loss 1.620896 : training accuracy 0.425200, and val accuracy 0.425200\n",
      "epochs 269 / 400: loss 1.619158 : training accuracy 0.426100, and val accuracy 0.426100\n",
      "epochs 270 / 400: loss 1.617384 : training accuracy 0.426500, and val accuracy 0.426500\n",
      "epochs 271 / 400: loss 1.615887 : training accuracy 0.426700, and val accuracy 0.426700\n",
      "epochs 272 / 400: loss 1.614968 : training accuracy 0.427700, and val accuracy 0.427700\n",
      "epochs 273 / 400: loss 1.613525 : training accuracy 0.428700, and val accuracy 0.428700\n",
      "epochs 274 / 400: loss 1.612294 : training accuracy 0.429000, and val accuracy 0.429000\n",
      "epochs 275 / 400: loss 1.612518 : training accuracy 0.429300, and val accuracy 0.429300\n",
      "epochs 276 / 400: loss 1.615762 : training accuracy 0.431900, and val accuracy 0.431900\n",
      "epochs 277 / 400: loss 1.622687 : training accuracy 0.427000, and val accuracy 0.427000\n",
      "epochs 278 / 400: loss 1.621681 : training accuracy 0.426600, and val accuracy 0.426600\n",
      "epochs 279 / 400: loss 1.619207 : training accuracy 0.425100, and val accuracy 0.425100\n",
      "epochs 280 / 400: loss 1.619645 : training accuracy 0.424000, and val accuracy 0.424000\n",
      "epochs 281 / 400: loss 1.616938 : training accuracy 0.424700, and val accuracy 0.424700\n",
      "epochs 282 / 400: loss 1.609828 : training accuracy 0.426400, and val accuracy 0.426400\n",
      "epochs 283 / 400: loss 1.606327 : training accuracy 0.426700, and val accuracy 0.426700\n",
      "epochs 284 / 400: loss 1.603569 : training accuracy 0.428800, and val accuracy 0.428800\n",
      "epochs 285 / 400: loss 1.601676 : training accuracy 0.429400, and val accuracy 0.429400\n",
      "epochs 286 / 400: loss 1.600019 : training accuracy 0.430500, and val accuracy 0.430500\n",
      "epochs 287 / 400: loss 1.598221 : training accuracy 0.432600, and val accuracy 0.432600\n",
      "epochs 288 / 400: loss 1.596690 : training accuracy 0.434900, and val accuracy 0.434900\n",
      "epochs 289 / 400: loss 1.595067 : training accuracy 0.435800, and val accuracy 0.435800\n",
      "epochs 290 / 400: loss 1.593979 : training accuracy 0.436100, and val accuracy 0.436100\n",
      "epochs 291 / 400: loss 1.593817 : training accuracy 0.435600, and val accuracy 0.435600\n",
      "epochs 292 / 400: loss 1.598099 : training accuracy 0.431100, and val accuracy 0.431100\n",
      "epochs 293 / 400: loss 1.606476 : training accuracy 0.430300, and val accuracy 0.430300\n",
      "epochs 294 / 400: loss 1.604732 : training accuracy 0.431500, and val accuracy 0.431500\n",
      "epochs 295 / 400: loss 1.600314 : training accuracy 0.433600, and val accuracy 0.433600\n",
      "epochs 296 / 400: loss 1.602557 : training accuracy 0.433600, and val accuracy 0.433600\n",
      "epochs 297 / 400: loss 1.605120 : training accuracy 0.432200, and val accuracy 0.432200\n",
      "epochs 298 / 400: loss 1.598116 : training accuracy 0.434100, and val accuracy 0.434100\n",
      "epochs 299 / 400: loss 1.592878 : training accuracy 0.436800, and val accuracy 0.436800\n",
      "epochs 300 / 400: loss 1.589391 : training accuracy 0.438500, and val accuracy 0.438500\n",
      "epochs 301 / 400: loss 1.587638 : training accuracy 0.438200, and val accuracy 0.438200\n",
      "epochs 302 / 400: loss 1.587007 : training accuracy 0.438000, and val accuracy 0.438000\n",
      "epochs 303 / 400: loss 1.585236 : training accuracy 0.438000, and val accuracy 0.438000\n",
      "epochs 304 / 400: loss 1.583265 : training accuracy 0.439900, and val accuracy 0.439900\n",
      "epochs 305 / 400: loss 1.582341 : training accuracy 0.441000, and val accuracy 0.441000\n",
      "epochs 306 / 400: loss 1.581383 : training accuracy 0.441500, and val accuracy 0.441500\n",
      "epochs 307 / 400: loss 1.579349 : training accuracy 0.441300, and val accuracy 0.441300\n",
      "epochs 308 / 400: loss 1.577930 : training accuracy 0.444200, and val accuracy 0.444200\n",
      "epochs 309 / 400: loss 1.580952 : training accuracy 0.441800, and val accuracy 0.441800\n",
      "epochs 310 / 400: loss 1.589504 : training accuracy 0.437900, and val accuracy 0.437900\n",
      "epochs 311 / 400: loss 1.589059 : training accuracy 0.440600, and val accuracy 0.440600\n",
      "epochs 312 / 400: loss 1.583309 : training accuracy 0.440100, and val accuracy 0.440100\n",
      "epochs 313 / 400: loss 1.586185 : training accuracy 0.435500, and val accuracy 0.435500\n",
      "epochs 314 / 400: loss 1.585236 : training accuracy 0.437500, and val accuracy 0.437500\n",
      "epochs 315 / 400: loss 1.577439 : training accuracy 0.440800, and val accuracy 0.440800\n",
      "epochs 316 / 400: loss 1.573628 : training accuracy 0.443500, and val accuracy 0.443500\n",
      "epochs 317 / 400: loss 1.570828 : training accuracy 0.444300, and val accuracy 0.444300\n",
      "epochs 318 / 400: loss 1.568314 : training accuracy 0.446300, and val accuracy 0.446300\n",
      "epochs 319 / 400: loss 1.565776 : training accuracy 0.447300, and val accuracy 0.447300\n",
      "epochs 320 / 400: loss 1.564020 : training accuracy 0.449200, and val accuracy 0.449200\n",
      "epochs 321 / 400: loss 1.562771 : training accuracy 0.450200, and val accuracy 0.450200\n",
      "epochs 322 / 400: loss 1.560770 : training accuracy 0.451500, and val accuracy 0.451500\n",
      "epochs 323 / 400: loss 1.558386 : training accuracy 0.451900, and val accuracy 0.451900\n",
      "epochs 324 / 400: loss 1.557407 : training accuracy 0.452000, and val accuracy 0.452000\n",
      "epochs 325 / 400: loss 1.560373 : training accuracy 0.448300, and val accuracy 0.448300\n",
      "epochs 326 / 400: loss 1.572850 : training accuracy 0.445100, and val accuracy 0.445100\n",
      "epochs 327 / 400: loss 1.574914 : training accuracy 0.445200, and val accuracy 0.445200\n",
      "epochs 328 / 400: loss 1.568797 : training accuracy 0.449100, and val accuracy 0.449100\n",
      "epochs 329 / 400: loss 1.563644 : training accuracy 0.447800, and val accuracy 0.447800\n",
      "epochs 330 / 400: loss 1.564115 : training accuracy 0.448400, and val accuracy 0.448400\n",
      "epochs 331 / 400: loss 1.576202 : training accuracy 0.442500, and val accuracy 0.442500\n",
      "epochs 332 / 400: loss 1.567287 : training accuracy 0.448500, and val accuracy 0.448500\n",
      "epochs 333 / 400: loss 1.560081 : training accuracy 0.451200, and val accuracy 0.451200\n",
      "epochs 334 / 400: loss 1.557119 : training accuracy 0.452700, and val accuracy 0.452700\n",
      "epochs 335 / 400: loss 1.554550 : training accuracy 0.453300, and val accuracy 0.453300\n",
      "epochs 336 / 400: loss 1.552386 : training accuracy 0.453700, and val accuracy 0.453700\n",
      "epochs 337 / 400: loss 1.551665 : training accuracy 0.453900, and val accuracy 0.453900\n",
      "epochs 338 / 400: loss 1.550065 : training accuracy 0.454200, and val accuracy 0.454200\n",
      "epochs 339 / 400: loss 1.548653 : training accuracy 0.454000, and val accuracy 0.454000\n",
      "epochs 340 / 400: loss 1.547609 : training accuracy 0.454400, and val accuracy 0.454400\n",
      "epochs 341 / 400: loss 1.545952 : training accuracy 0.455100, and val accuracy 0.455100\n",
      "epochs 342 / 400: loss 1.543758 : training accuracy 0.455900, and val accuracy 0.455900\n",
      "epochs 343 / 400: loss 1.542490 : training accuracy 0.457200, and val accuracy 0.457200\n",
      "epochs 344 / 400: loss 1.541815 : training accuracy 0.458700, and val accuracy 0.458700\n",
      "epochs 345 / 400: loss 1.542603 : training accuracy 0.455500, and val accuracy 0.455500\n",
      "epochs 346 / 400: loss 1.549028 : training accuracy 0.457300, and val accuracy 0.457300\n",
      "epochs 347 / 400: loss 1.552344 : training accuracy 0.454200, and val accuracy 0.454200\n",
      "epochs 348 / 400: loss 1.552214 : training accuracy 0.455700, and val accuracy 0.455700\n",
      "epochs 349 / 400: loss 1.550021 : training accuracy 0.455800, and val accuracy 0.455800\n",
      "epochs 350 / 400: loss 1.545857 : training accuracy 0.458300, and val accuracy 0.458300\n",
      "epochs 351 / 400: loss 1.540141 : training accuracy 0.461300, and val accuracy 0.461300\n",
      "epochs 352 / 400: loss 1.536536 : training accuracy 0.463100, and val accuracy 0.463100\n",
      "epochs 353 / 400: loss 1.534705 : training accuracy 0.465800, and val accuracy 0.465800\n",
      "epochs 354 / 400: loss 1.533284 : training accuracy 0.461900, and val accuracy 0.461900\n",
      "epochs 355 / 400: loss 1.531335 : training accuracy 0.462500, and val accuracy 0.462500\n",
      "epochs 356 / 400: loss 1.529058 : training accuracy 0.461600, and val accuracy 0.461600\n",
      "epochs 357 / 400: loss 1.527402 : training accuracy 0.461900, and val accuracy 0.461900\n",
      "epochs 358 / 400: loss 1.530727 : training accuracy 0.459300, and val accuracy 0.459300\n",
      "epochs 359 / 400: loss 1.552590 : training accuracy 0.445900, and val accuracy 0.445900\n",
      "epochs 360 / 400: loss 1.549857 : training accuracy 0.445700, and val accuracy 0.445700\n",
      "epochs 361 / 400: loss 1.551331 : training accuracy 0.448800, and val accuracy 0.448800\n",
      "epochs 362 / 400: loss 1.539161 : training accuracy 0.454100, and val accuracy 0.454100\n",
      "epochs 363 / 400: loss 1.530990 : training accuracy 0.458300, and val accuracy 0.458300\n",
      "epochs 364 / 400: loss 1.526997 : training accuracy 0.461400, and val accuracy 0.461400\n",
      "epochs 365 / 400: loss 1.524792 : training accuracy 0.462300, and val accuracy 0.462300\n",
      "epochs 366 / 400: loss 1.522576 : training accuracy 0.463700, and val accuracy 0.463700\n",
      "epochs 367 / 400: loss 1.520956 : training accuracy 0.463700, and val accuracy 0.463700\n",
      "epochs 368 / 400: loss 1.519674 : training accuracy 0.465800, and val accuracy 0.465800\n",
      "epochs 369 / 400: loss 1.517848 : training accuracy 0.466100, and val accuracy 0.466100\n",
      "epochs 370 / 400: loss 1.515902 : training accuracy 0.467100, and val accuracy 0.467100\n",
      "epochs 371 / 400: loss 1.513083 : training accuracy 0.468600, and val accuracy 0.468600\n",
      "epochs 372 / 400: loss 1.511891 : training accuracy 0.469300, and val accuracy 0.469300\n",
      "epochs 373 / 400: loss 1.510784 : training accuracy 0.470000, and val accuracy 0.470000\n",
      "epochs 374 / 400: loss 1.515699 : training accuracy 0.468900, and val accuracy 0.468900\n",
      "epochs 375 / 400: loss 1.534105 : training accuracy 0.465100, and val accuracy 0.465100\n",
      "epochs 376 / 400: loss 1.522178 : training accuracy 0.470100, and val accuracy 0.470100\n",
      "epochs 377 / 400: loss 1.521664 : training accuracy 0.469800, and val accuracy 0.469800\n",
      "epochs 378 / 400: loss 1.521802 : training accuracy 0.469900, and val accuracy 0.469900\n",
      "epochs 379 / 400: loss 1.518112 : training accuracy 0.469000, and val accuracy 0.469000\n",
      "epochs 380 / 400: loss 1.511693 : training accuracy 0.470800, and val accuracy 0.470800\n",
      "epochs 381 / 400: loss 1.511151 : training accuracy 0.470600, and val accuracy 0.470600\n",
      "epochs 382 / 400: loss 1.541321 : training accuracy 0.454000, and val accuracy 0.454000\n",
      "epochs 383 / 400: loss 1.517259 : training accuracy 0.465900, and val accuracy 0.465900\n",
      "epochs 384 / 400: loss 1.510690 : training accuracy 0.467000, and val accuracy 0.467000\n",
      "epochs 385 / 400: loss 1.509529 : training accuracy 0.467500, and val accuracy 0.467500\n",
      "epochs 386 / 400: loss 1.506342 : training accuracy 0.469500, and val accuracy 0.469500\n",
      "epochs 387 / 400: loss 1.503332 : training accuracy 0.469900, and val accuracy 0.469900\n",
      "epochs 388 / 400: loss 1.501162 : training accuracy 0.470400, and val accuracy 0.470400\n",
      "epochs 389 / 400: loss 1.500659 : training accuracy 0.471500, and val accuracy 0.471500\n",
      "epochs 390 / 400: loss 1.504096 : training accuracy 0.471700, and val accuracy 0.471700\n",
      "epochs 391 / 400: loss 1.503443 : training accuracy 0.473600, and val accuracy 0.473600\n",
      "epochs 392 / 400: loss 1.503444 : training accuracy 0.472400, and val accuracy 0.472400\n",
      "epochs 393 / 400: loss 1.503437 : training accuracy 0.473100, and val accuracy 0.473100\n",
      "epochs 394 / 400: loss 1.502041 : training accuracy 0.473400, and val accuracy 0.473400\n",
      "epochs 395 / 400: loss 1.502176 : training accuracy 0.471800, and val accuracy 0.471800\n",
      "epochs 396 / 400: loss 1.502303 : training accuracy 0.471900, and val accuracy 0.471900\n",
      "epochs 397 / 400: loss 1.501363 : training accuracy 0.472100, and val accuracy 0.472100\n",
      "epochs 398 / 400: loss 1.500649 : training accuracy 0.471800, and val accuracy 0.471800\n",
      "epochs 399 / 400: loss 1.501645 : training accuracy 0.471800, and val accuracy 0.471800\n"
     ]
    }
   ],
   "source": [
    "net.train(x_train,y_train,x_train,y_train,learning_rate=1e-4,learning_rate_decay=1,num_epochs=400,reg=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0 / 100: loss 2.302581 : training accuracy 0.085800, and val accuracy 0.085800\n",
      "epochs 1 / 100: loss 2.302525 : training accuracy 0.104700, and val accuracy 0.104700\n",
      "epochs 2 / 100: loss 2.302460 : training accuracy 0.103800, and val accuracy 0.103800\n",
      "epochs 3 / 100: loss 2.302372 : training accuracy 0.106900, and val accuracy 0.106900\n",
      "epochs 4 / 100: loss 2.302245 : training accuracy 0.118300, and val accuracy 0.118300\n",
      "epochs 5 / 100: loss 2.302050 : training accuracy 0.128900, and val accuracy 0.128900\n",
      "epochs 6 / 100: loss 2.301746 : training accuracy 0.129200, and val accuracy 0.129200\n",
      "epochs 7 / 100: loss 2.301272 : training accuracy 0.121300, and val accuracy 0.121300\n",
      "epochs 8 / 100: loss 2.300543 : training accuracy 0.112000, and val accuracy 0.112000\n",
      "epochs 9 / 100: loss 2.299455 : training accuracy 0.104600, and val accuracy 0.104600\n",
      "epochs 10 / 100: loss 2.297903 : training accuracy 0.101600, and val accuracy 0.101600\n",
      "epochs 11 / 100: loss 2.295816 : training accuracy 0.101500, and val accuracy 0.101500\n",
      "epochs 12 / 100: loss 2.293169 : training accuracy 0.105900, and val accuracy 0.105900\n",
      "epochs 13 / 100: loss 2.289921 : training accuracy 0.116000, and val accuracy 0.116000\n",
      "epochs 14 / 100: loss 2.285912 : training accuracy 0.128800, and val accuracy 0.128800\n",
      "epochs 15 / 100: loss 2.280858 : training accuracy 0.128400, and val accuracy 0.128400\n",
      "epochs 16 / 100: loss 2.274531 : training accuracy 0.126500, and val accuracy 0.126500\n",
      "epochs 17 / 100: loss 2.266878 : training accuracy 0.129400, and val accuracy 0.129400\n",
      "epochs 18 / 100: loss 2.257933 : training accuracy 0.136900, and val accuracy 0.136900\n",
      "epochs 19 / 100: loss 2.247662 : training accuracy 0.145300, and val accuracy 0.145300\n",
      "epochs 20 / 100: loss 2.235928 : training accuracy 0.153300, and val accuracy 0.153300\n",
      "epochs 21 / 100: loss 2.222661 : training accuracy 0.160000, and val accuracy 0.160000\n",
      "epochs 22 / 100: loss 2.208096 : training accuracy 0.163600, and val accuracy 0.163600\n",
      "epochs 23 / 100: loss 2.192829 : training accuracy 0.168300, and val accuracy 0.168300\n",
      "epochs 24 / 100: loss 2.177662 : training accuracy 0.171300, and val accuracy 0.171300\n",
      "epochs 25 / 100: loss 2.163350 : training accuracy 0.180400, and val accuracy 0.180400\n",
      "epochs 26 / 100: loss 2.150379 : training accuracy 0.191700, and val accuracy 0.191700\n",
      "epochs 27 / 100: loss 2.138949 : training accuracy 0.190900, and val accuracy 0.190900\n",
      "epochs 28 / 100: loss 2.128993 : training accuracy 0.192300, and val accuracy 0.192300\n",
      "epochs 29 / 100: loss 2.120316 : training accuracy 0.194400, and val accuracy 0.194400\n",
      "epochs 30 / 100: loss 2.112675 : training accuracy 0.195500, and val accuracy 0.195500\n",
      "epochs 31 / 100: loss 2.105843 : training accuracy 0.196700, and val accuracy 0.196700\n",
      "epochs 32 / 100: loss 2.099611 : training accuracy 0.198100, and val accuracy 0.198100\n",
      "epochs 33 / 100: loss 2.093811 : training accuracy 0.201600, and val accuracy 0.201600\n",
      "epochs 34 / 100: loss 2.088317 : training accuracy 0.208000, and val accuracy 0.208000\n",
      "epochs 35 / 100: loss 2.083017 : training accuracy 0.212300, and val accuracy 0.212300\n",
      "epochs 36 / 100: loss 2.077842 : training accuracy 0.217600, and val accuracy 0.217600\n",
      "epochs 37 / 100: loss 2.072730 : training accuracy 0.224600, and val accuracy 0.224600\n",
      "epochs 38 / 100: loss 2.067636 : training accuracy 0.229400, and val accuracy 0.229400\n",
      "epochs 39 / 100: loss 2.062535 : training accuracy 0.235800, and val accuracy 0.235800\n",
      "epochs 40 / 100: loss 2.057398 : training accuracy 0.240300, and val accuracy 0.240300\n",
      "epochs 41 / 100: loss 2.052231 : training accuracy 0.242900, and val accuracy 0.242900\n",
      "epochs 42 / 100: loss 2.047044 : training accuracy 0.247300, and val accuracy 0.247300\n",
      "epochs 43 / 100: loss 2.041858 : training accuracy 0.249900, and val accuracy 0.249900\n",
      "epochs 44 / 100: loss 2.036697 : training accuracy 0.253300, and val accuracy 0.253300\n",
      "epochs 45 / 100: loss 2.031580 : training accuracy 0.254600, and val accuracy 0.254600\n",
      "epochs 46 / 100: loss 2.026537 : training accuracy 0.255000, and val accuracy 0.255000\n",
      "epochs 47 / 100: loss 2.021592 : training accuracy 0.255900, and val accuracy 0.255900\n",
      "epochs 48 / 100: loss 2.016763 : training accuracy 0.256500, and val accuracy 0.256500\n",
      "epochs 49 / 100: loss 2.012058 : training accuracy 0.258400, and val accuracy 0.258400\n",
      "epochs 50 / 100: loss 2.007473 : training accuracy 0.259400, and val accuracy 0.259400\n",
      "epochs 51 / 100: loss 2.002998 : training accuracy 0.261900, and val accuracy 0.261900\n",
      "epochs 52 / 100: loss 1.998632 : training accuracy 0.264700, and val accuracy 0.264700\n",
      "epochs 53 / 100: loss 1.994366 : training accuracy 0.267100, and val accuracy 0.267100\n",
      "epochs 54 / 100: loss 1.990186 : training accuracy 0.269600, and val accuracy 0.269600\n",
      "epochs 55 / 100: loss 1.986088 : training accuracy 0.272500, and val accuracy 0.272500\n",
      "epochs 56 / 100: loss 1.982058 : training accuracy 0.274400, and val accuracy 0.274400\n",
      "epochs 57 / 100: loss 1.978097 : training accuracy 0.275900, and val accuracy 0.275900\n",
      "epochs 58 / 100: loss 1.974196 : training accuracy 0.277700, and val accuracy 0.277700\n",
      "epochs 59 / 100: loss 1.970358 : training accuracy 0.281500, and val accuracy 0.281500\n",
      "epochs 60 / 100: loss 1.966588 : training accuracy 0.283900, and val accuracy 0.283900\n",
      "epochs 61 / 100: loss 1.962893 : training accuracy 0.284700, and val accuracy 0.284700\n",
      "epochs 62 / 100: loss 1.959276 : training accuracy 0.285500, and val accuracy 0.285500\n",
      "epochs 63 / 100: loss 1.955743 : training accuracy 0.286700, and val accuracy 0.286700\n",
      "epochs 64 / 100: loss 1.952298 : training accuracy 0.289800, and val accuracy 0.289800\n",
      "epochs 65 / 100: loss 1.948951 : training accuracy 0.291700, and val accuracy 0.291700\n",
      "epochs 66 / 100: loss 1.945707 : training accuracy 0.293600, and val accuracy 0.293600\n",
      "epochs 67 / 100: loss 1.942560 : training accuracy 0.294900, and val accuracy 0.294900\n",
      "epochs 68 / 100: loss 1.939516 : training accuracy 0.296000, and val accuracy 0.296000\n",
      "epochs 69 / 100: loss 1.936574 : training accuracy 0.296500, and val accuracy 0.296500\n",
      "epochs 70 / 100: loss 1.933734 : training accuracy 0.296900, and val accuracy 0.296900\n",
      "epochs 71 / 100: loss 1.930989 : training accuracy 0.298500, and val accuracy 0.298500\n",
      "epochs 72 / 100: loss 1.928333 : training accuracy 0.300500, and val accuracy 0.300500\n",
      "epochs 73 / 100: loss 1.925761 : training accuracy 0.301300, and val accuracy 0.301300\n",
      "epochs 74 / 100: loss 1.923259 : training accuracy 0.302900, and val accuracy 0.302900\n",
      "epochs 75 / 100: loss 1.920824 : training accuracy 0.305000, and val accuracy 0.305000\n",
      "epochs 76 / 100: loss 1.918445 : training accuracy 0.306200, and val accuracy 0.306200\n",
      "epochs 77 / 100: loss 1.916121 : training accuracy 0.306900, and val accuracy 0.306900\n",
      "epochs 78 / 100: loss 1.913840 : training accuracy 0.308600, and val accuracy 0.308600\n",
      "epochs 79 / 100: loss 1.911596 : training accuracy 0.309200, and val accuracy 0.309200\n",
      "epochs 80 / 100: loss 1.909385 : training accuracy 0.310000, and val accuracy 0.310000\n",
      "epochs 81 / 100: loss 1.907202 : training accuracy 0.311300, and val accuracy 0.311300\n",
      "epochs 82 / 100: loss 1.905039 : training accuracy 0.312400, and val accuracy 0.312400\n",
      "epochs 83 / 100: loss 1.902890 : training accuracy 0.313400, and val accuracy 0.313400\n",
      "epochs 84 / 100: loss 1.900753 : training accuracy 0.315000, and val accuracy 0.315000\n",
      "epochs 85 / 100: loss 1.898629 : training accuracy 0.315600, and val accuracy 0.315600\n",
      "epochs 86 / 100: loss 1.896513 : training accuracy 0.316700, and val accuracy 0.316700\n",
      "epochs 87 / 100: loss 1.894399 : training accuracy 0.317100, and val accuracy 0.317100\n",
      "epochs 88 / 100: loss 1.892282 : training accuracy 0.316600, and val accuracy 0.316600\n",
      "epochs 89 / 100: loss 1.890164 : training accuracy 0.318200, and val accuracy 0.318200\n",
      "epochs 90 / 100: loss 1.888045 : training accuracy 0.318500, and val accuracy 0.318500\n",
      "epochs 91 / 100: loss 1.885921 : training accuracy 0.319600, and val accuracy 0.319600\n",
      "epochs 92 / 100: loss 1.883790 : training accuracy 0.321100, and val accuracy 0.321100\n",
      "epochs 93 / 100: loss 1.881649 : training accuracy 0.321800, and val accuracy 0.321800\n",
      "epochs 94 / 100: loss 1.879490 : training accuracy 0.323500, and val accuracy 0.323500\n",
      "epochs 95 / 100: loss 1.877331 : training accuracy 0.325000, and val accuracy 0.325000\n",
      "epochs 96 / 100: loss 1.875175 : training accuracy 0.326500, and val accuracy 0.326500\n",
      "epochs 97 / 100: loss 1.873019 : training accuracy 0.328300, and val accuracy 0.328300\n",
      "epochs 98 / 100: loss 1.870854 : training accuracy 0.329600, and val accuracy 0.329600\n",
      "epochs 99 / 100: loss 1.868689 : training accuracy 0.331000, and val accuracy 0.331000\n"
     ]
    }
   ],
   "source": [
    "net.train(x_train,y_train,x_train,y_train,learning_rate=1e-4,learning_rate_decay=1,num_epochs=100,reg=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5: loss 12.517379 : training accuracy 0.107100, and val accuracy 0.107100\n",
      "iteration 1 / 5: loss 2.302585 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 2 / 5: loss 2.302585 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 3 / 5: loss 2.302585 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 4 / 5: loss 2.302585 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 0 / 5: loss 2.302620 : training accuracy 0.094400, and val accuracy 0.094400\n",
      "iteration 1 / 5: loss 2.302616 : training accuracy 0.093700, and val accuracy 0.093700\n",
      "iteration 2 / 5: loss 2.302614 : training accuracy 0.093100, and val accuracy 0.093100\n",
      "iteration 3 / 5: loss 2.302614 : training accuracy 0.092600, and val accuracy 0.092600\n",
      "iteration 4 / 5: loss 2.302614 : training accuracy 0.092600, and val accuracy 0.092600\n",
      "iteration 0 / 5: loss 2.302599 : training accuracy 0.105400, and val accuracy 0.105400\n",
      "iteration 1 / 5: loss 2.302540 : training accuracy 0.101600, and val accuracy 0.101600\n",
      "iteration 2 / 5: loss 2.302519 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 3 / 5: loss 2.302512 : training accuracy 0.106000, and val accuracy 0.106000\n",
      "iteration 4 / 5: loss 2.302509 : training accuracy 0.107100, and val accuracy 0.107100\n",
      "iteration 0 / 5: loss 2.309350 : training accuracy 0.098900, and val accuracy 0.098900\n",
      "iteration 1 / 5: loss 2.309258 : training accuracy 0.099900, and val accuracy 0.099900\n",
      "iteration 2 / 5: loss 2.309226 : training accuracy 0.102500, and val accuracy 0.102500\n",
      "iteration 3 / 5: loss 2.309215 : training accuracy 0.103100, and val accuracy 0.103100\n",
      "iteration 4 / 5: loss 2.309211 : training accuracy 0.103500, and val accuracy 0.103500\n",
      "iteration 0 / 5: loss 2.314425 : training accuracy 0.100500, and val accuracy 0.100500\n",
      "iteration 1 / 5: loss 2.314420 : training accuracy 0.100400, and val accuracy 0.100400\n",
      "iteration 2 / 5: loss 2.314419 : training accuracy 0.100400, and val accuracy 0.100400\n",
      "iteration 3 / 5: loss 2.314418 : training accuracy 0.100400, and val accuracy 0.100400\n",
      "iteration 4 / 5: loss 2.314418 : training accuracy 0.100400, and val accuracy 0.100400\n",
      "iteration 0 / 5: loss 2.302634 : training accuracy 0.065600, and val accuracy 0.065600\n",
      "iteration 1 / 5: loss 2.302634 : training accuracy 0.066500, and val accuracy 0.066500\n",
      "iteration 2 / 5: loss 2.302634 : training accuracy 0.066600, and val accuracy 0.066600\n",
      "iteration 3 / 5: loss 2.302633 : training accuracy 0.066800, and val accuracy 0.066800\n",
      "iteration 4 / 5: loss 2.302633 : training accuracy 0.066900, and val accuracy 0.066900\n",
      "iteration 0 / 5: loss 2.302554 : training accuracy 0.100000, and val accuracy 0.100000\n",
      "iteration 1 / 5: loss 2.302475 : training accuracy 0.100200, and val accuracy 0.100200\n",
      "iteration 2 / 5: loss 2.302446 : training accuracy 0.100200, and val accuracy 0.100200\n",
      "iteration 3 / 5: loss 2.302435 : training accuracy 0.100200, and val accuracy 0.100200\n",
      "iteration 4 / 5: loss 2.302431 : training accuracy 0.100200, and val accuracy 0.100200\n",
      "iteration 0 / 5: loss 3.859153 : training accuracy 0.114000, and val accuracy 0.114000\n",
      "iteration 1 / 5: loss 2.302585 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 2 / 5: loss 2.302584 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 3 / 5: loss 2.302584 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 4 / 5: loss 2.302584 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 0 / 5: loss 2.302580 : training accuracy 0.124100, and val accuracy 0.124100\n",
      "iteration 1 / 5: loss 2.302324 : training accuracy 0.113100, and val accuracy 0.113100\n",
      "iteration 2 / 5: loss 2.302127 : training accuracy 0.116400, and val accuracy 0.116400\n",
      "iteration 3 / 5: loss 2.302030 : training accuracy 0.116200, and val accuracy 0.116200\n",
      "iteration 4 / 5: loss 2.301991 : training accuracy 0.116300, and val accuracy 0.116300\n",
      "iteration 0 / 5: loss 2.302582 : training accuracy 0.100400, and val accuracy 0.100400\n",
      "iteration 1 / 5: loss 2.302579 : training accuracy 0.100300, and val accuracy 0.100300\n",
      "iteration 2 / 5: loss 2.302578 : training accuracy 0.100000, and val accuracy 0.100000\n",
      "iteration 3 / 5: loss 2.302578 : training accuracy 0.099900, and val accuracy 0.099900\n",
      "iteration 4 / 5: loss 2.302578 : training accuracy 0.100000, and val accuracy 0.100000\n",
      "iteration 0 / 5: loss 65.554348 : training accuracy 0.126900, and val accuracy 0.126900\n",
      "iteration 1 / 5: loss 10.024226 : training accuracy 0.127500, and val accuracy 0.127500\n",
      "iteration 2 / 5: loss 6.113940 : training accuracy 0.127400, and val accuracy 0.127400\n",
      "iteration 3 / 5: loss 5.291420 : training accuracy 0.127500, and val accuracy 0.127500\n",
      "iteration 4 / 5: loss 5.049519 : training accuracy 0.127600, and val accuracy 0.127600\n",
      "iteration 0 / 5: loss 2.302600 : training accuracy 0.114400, and val accuracy 0.114400\n",
      "iteration 1 / 5: loss 2.302599 : training accuracy 0.114300, and val accuracy 0.114300\n",
      "iteration 2 / 5: loss 2.302599 : training accuracy 0.114200, and val accuracy 0.114200\n",
      "iteration 3 / 5: loss 2.302599 : training accuracy 0.114300, and val accuracy 0.114300\n",
      "iteration 4 / 5: loss 2.302599 : training accuracy 0.114200, and val accuracy 0.114200\n",
      "iteration 0 / 5: loss 2.302582 : training accuracy 0.109100, and val accuracy 0.109100\n",
      "iteration 1 / 5: loss 2.302507 : training accuracy 0.101100, and val accuracy 0.101100\n",
      "iteration 2 / 5: loss 2.302476 : training accuracy 0.100900, and val accuracy 0.100900\n",
      "iteration 3 / 5: loss 2.302464 : training accuracy 0.100800, and val accuracy 0.100800\n",
      "iteration 4 / 5: loss 2.302460 : training accuracy 0.100800, and val accuracy 0.100800\n",
      "iteration 0 / 5: loss 3.906569 : training accuracy 0.129800, and val accuracy 0.129800\n",
      "iteration 1 / 5: loss 2.302605 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 2 / 5: loss 2.302586 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 3 / 5: loss 2.302585 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 4 / 5: loss 2.302585 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 0 / 5: loss 3.367795 : training accuracy 0.102900, and val accuracy 0.102900\n",
      "iteration 1 / 5: loss 2.325778 : training accuracy 0.133600, and val accuracy 0.133600\n",
      "iteration 2 / 5: loss 2.309267 : training accuracy 0.111200, and val accuracy 0.111200\n",
      "iteration 3 / 5: loss 2.306956 : training accuracy 0.116200, and val accuracy 0.116200\n",
      "iteration 4 / 5: loss 2.306359 : training accuracy 0.120400, and val accuracy 0.120400\n",
      "iteration 0 / 5: loss 2.302590 : training accuracy 0.092800, and val accuracy 0.092800\n",
      "iteration 1 / 5: loss 2.302390 : training accuracy 0.105000, and val accuracy 0.105000\n",
      "iteration 2 / 5: loss 2.302240 : training accuracy 0.102600, and val accuracy 0.102600\n",
      "iteration 3 / 5: loss 2.302165 : training accuracy 0.103700, and val accuracy 0.103700\n",
      "iteration 4 / 5: loss 2.302136 : training accuracy 0.104000, and val accuracy 0.104000\n",
      "iteration 0 / 5: loss 2.302587 : training accuracy 0.092200, and val accuracy 0.092200\n",
      "iteration 1 / 5: loss 2.302539 : training accuracy 0.103800, and val accuracy 0.103800\n",
      "iteration 2 / 5: loss 2.302521 : training accuracy 0.102400, and val accuracy 0.102400\n",
      "iteration 3 / 5: loss 2.302514 : training accuracy 0.102000, and val accuracy 0.102000\n",
      "iteration 4 / 5: loss 2.302512 : training accuracy 0.101700, and val accuracy 0.101700\n",
      "iteration 0 / 5: loss 2.302583 : training accuracy 0.103000, and val accuracy 0.103000\n",
      "iteration 1 / 5: loss 2.302533 : training accuracy 0.104100, and val accuracy 0.104100\n",
      "iteration 2 / 5: loss 2.302516 : training accuracy 0.103200, and val accuracy 0.103200\n",
      "iteration 3 / 5: loss 2.302510 : training accuracy 0.102700, and val accuracy 0.102700\n",
      "iteration 4 / 5: loss 2.302508 : training accuracy 0.102900, and val accuracy 0.102900\n",
      "iteration 0 / 5: loss 2.302594 : training accuracy 0.086200, and val accuracy 0.086200\n",
      "iteration 1 / 5: loss 2.302577 : training accuracy 0.086600, and val accuracy 0.086600\n",
      "iteration 2 / 5: loss 2.302572 : training accuracy 0.093100, and val accuracy 0.093100\n",
      "iteration 3 / 5: loss 2.302570 : training accuracy 0.096900, and val accuracy 0.096900\n",
      "iteration 4 / 5: loss 2.302569 : training accuracy 0.098000, and val accuracy 0.098000\n",
      "iteration 0 / 5: loss 2.302560 : training accuracy 0.105400, and val accuracy 0.105400\n",
      "iteration 1 / 5: loss 2.302530 : training accuracy 0.118200, and val accuracy 0.118200\n",
      "iteration 2 / 5: loss 2.302518 : training accuracy 0.115000, and val accuracy 0.115000\n",
      "iteration 3 / 5: loss 2.302514 : training accuracy 0.113600, and val accuracy 0.113600\n",
      "iteration 4 / 5: loss 2.302513 : training accuracy 0.113400, and val accuracy 0.113400\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "max_count=2\n",
    "for count in xrange(max_count):\n",
    "    reg = 10 ** random.uniform(-5, 5)\n",
    "    lr=10 ** random.uniform(-3, -6)\n",
    "    \n",
    "    net=Two_layer_NN(32*32*3, 50, 10)\n",
    "    net.train(x_train,y_train,x_train,y_train,learning_rate=lr,learning_rate_decay=0.9,num_epochs=5,reg=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
