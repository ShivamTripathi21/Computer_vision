{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some inputs\n",
    "x = -2; y = 5; z = -4\n",
    "\n",
    "# perform the forward pass\n",
    "q = x + y # q becomes 3\n",
    "f = q * z # f becomes -12\n",
    "\n",
    "# perform the backward pass (backpropagation) in reverse order:\n",
    "# first backprop through f = q * z\n",
    "dfdz = q # df/dz = q, so gradient on z becomes 3\n",
    "dfdq = z # df/dq = z, so gradient on q becomes -4\n",
    "# now backprop through q = x + y\n",
    "dfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!\n",
    "dfdy = 1.0 * dfdq # dq/dy = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mul_neural(object):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        \n",
    "    def forward(self):\n",
    "        z=self.x * self.y\n",
    "        return z\n",
    "    \n",
    "    def backword(self,dz):\n",
    "        dx=self.y * dz\n",
    "        dy=self.x * dz\n",
    "        return [dx,dy]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10 \n",
      "[5, -2]\n"
     ]
    }
   ],
   "source": [
    "mul=mul_neural(-2,5)\n",
    "print mul.forward(),'\\n',mul.backword(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for vectorized operation\n",
    "# forward pass\n",
    "W = np.random.randn(5, 10)\n",
    "X = np.random.randn(10, 3)\n",
    "D = W.dot(X)\n",
    "\n",
    "# now suppose we had the gradient on D from above in the circuit\n",
    "dD = np.random.randn(*D.shape) # same shape as D\n",
    "dW = dD.dot(X.T) #.T gives the transpose of the matrix\n",
    "dX = W.T.dot(dD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.61512137 -0.64225289 -0.71275525  0.15587004 -1.2680654   0.16634248\n",
      "  1.55364416  0.17862038  0.34367691  0.24536899] \n",
      "[[ 1.2614398  -0.63082729 -0.53642245]\n",
      " [-0.72267091 -1.28969811 -2.28481648]\n",
      " [-0.36261013  2.5937057  -0.7167907 ]\n",
      " [ 0.24505639  1.24111496 -1.03504687]\n",
      " [ 0.68353875  0.94217214 -1.61585296]\n",
      " [ 1.10166963  1.8382235  -3.01515155]\n",
      " [ 0.64741584 -1.39852177  0.41168859]\n",
      " [-1.367836   -0.06596386  1.01532726]\n",
      " [ 0.58490113  1.13479943 -0.05362989]\n",
      " [-0.93553342  0.81860603  1.30990332]] \n",
      "[[ 1.73922315 -4.71168058 -7.87103709]\n",
      " [ 0.03432731 -2.92156103  4.81837191]\n",
      " [ 0.47872471  0.31887479  2.42742042]\n",
      " [ 0.42753835 -9.56168508  0.43822116]\n",
      " [-1.16785126 -4.21997996  1.77658109]]\n"
     ]
    }
   ],
   "source": [
    "print W[1],'\\n',X,'\\n',D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55444305 -1.30521158  0.56082814]\n",
      " [-0.26263859 -0.6091947   1.11461878]\n",
      " [ 0.46600428 -0.62476782 -0.6826852 ]\n",
      " [-0.84083875  1.2735857  -2.35660913]\n",
      " [-0.8012666   0.80408799 -1.60912995]] \n",
      "[[-0.17687426  0.80261939 -3.58628444 -2.33627085 -2.51493312 -4.70106552\n",
      "   1.69729814  1.41390807 -1.83552488  0.18487658]\n",
      " [-0.54491267 -1.57122084 -2.28378473 -1.9741246  -2.55454998 -4.76992152\n",
      "   1.1408115   1.53113419 -0.90470829  1.20705957]\n",
      " [ 1.34816461  2.02880454 -1.30009932  0.04539983  0.83301205  1.42331923\n",
      "   0.89439624 -1.28935423 -0.3998074  -1.8416529 ]\n",
      " [-0.59994203  4.34952801  5.29739863  3.81381426  4.43312493  8.52034232\n",
      "  -3.29569873 -1.3266106   1.07984147 -1.25773243]\n",
      " [-0.65481679  3.21858794  3.52952436  2.46713505  2.81000993  5.44713303\n",
      "  -2.30574768 -0.59083295  0.53011431 -0.69996171]] \n",
      "[[ 1.95284299 -4.22893776  2.28818541]\n",
      " [-2.92285099  0.54937627 -2.37428382]\n",
      " [ 3.83479554 -1.8716641   2.49345124]\n",
      " [ 1.04583229  1.9651884   2.9744204 ]\n",
      " [-1.58569548  5.7611002  -4.78509359]\n",
      " [ 0.26092002 -5.37207209  3.42820313]\n",
      " [-2.84105654  2.90552455 -5.43582332]\n",
      " [ 0.34533035 -1.77049627 -0.37454346]\n",
      " [-1.34370851  0.22209073  1.02499869]\n",
      " [-0.29791495  0.92537896  1.03363566]]\n"
     ]
    }
   ],
   "source": [
    "print dD,'\\n',dW,'\\n',dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inti_two_layer_model(input_size,hidden_layer_size,output_size):\n",
    "    #initialize model\n",
    "    model={}\n",
    "    model['w1']=0.0001 * np.random.randn(input_size,hidden_layer_size)\n",
    "    model['w2']=0.0001 * np.random.randn(hidden_layer_size,output_size)\n",
    "    model['b1']=np.zeros(hidden_layer_size)\n",
    "    model['b2']=np.zeros(output_size)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 \n",
      "\n",
      "(3072, 20) \n",
      "(1, 20)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.transpose(unpickle('cifar-10/data_batch_1')['data'])\n",
    "x_train = x_train[:,0:20]\n",
    "y_train = [unpickle('cifar-10/data_batch_1')['labels']]\n",
    "y_train = np.transpose(y_train)[0:20,:].T\n",
    "print y_train[0][2],'\\n'\n",
    "print np.shape(x_train),'\\n',np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model= inti_two_layer_model(32*32*3, 50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_net(x_train,model,y_train,reg):\n",
    "    l1=np.dot(x_train,model['w1'])\n",
    "    l2=np.dot(l1,model['w2'])\n",
    "    # l2 is 20 x 10 matrix here other wise it is N x 10 matrix\n",
    "    l2=np.exp(l2)\n",
    "    su=l2.sum(axis=0)\n",
    "    loss= l2/su\n",
    "    t=0\n",
    "    for i in range(0,20):\n",
    "        val= -(np.log(loss[i][y_train[i]]))\n",
    "        t= t + val\n",
    "    \n",
    "    return (t/20)+reg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 3072) \n",
      "(20, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train=np.transpose(x_train)\n",
    "y_train=np.transpose(y_train)\n",
    "print np.shape(x_train),'\\n',np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss=two_layer_net(x_train,model,y_train,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9957191]\n"
     ]
    }
   ],
   "source": [
    "print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TwoLayerNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c7b1e83e4223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TwoLayerNet' is not defined"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(32*32*3, 50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6291e84ede56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m stats = net.train(x_train, y_train, x_train, y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             reg=0.5, verbose=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "stats = net.train(x_train, y_train, x_train, y_train,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16666667 0.33333333 0.5       ]\n",
      " [0.26666667 0.33333333 0.4       ]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "a=np.array([[1,2,3],[4,5,6]])\n",
    "b=a/np.sum(a, axis=1, keepdims=True)\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, std=0.0001):\n",
    "        self.params = {}\n",
    "        self.params['w1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['w2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        # unpack the variable\n",
    "        w1, b1 = self.params['w1'], self.params['b1']\n",
    "        w2, b2 = self.params['w2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "        \n",
    "        #compute the forward pass\n",
    "        scores=None\n",
    "        \n",
    "        z1=np.dot(X,w1) + b1\n",
    "        a1 = np.maximum(0, z1) # pass through ReLU activation function\n",
    "        # we can also pass through sigmoid function\n",
    "        scores=np.dot(a1,w2) + b2\n",
    "        \n",
    "        if y is None:\n",
    "            return scores\n",
    "        \n",
    "        # other wise compute the loss\n",
    "        \n",
    "        loss=None\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "        \n",
    "        #t=0\n",
    "        \n",
    "        corect_logprobs = -np.log(probs[range(N), y])\n",
    "        data_loss = np.sum(corect_logprobs) / N\n",
    "        reg_loss = 0.5 * reg * np.sum(w1 * w1) + 0.5 * reg * np.sum(w2 * w2)\n",
    "        loss = data_loss + reg_loss\n",
    "        \n",
    "        #for i in range(0,N):\n",
    "        #    val= -(np.log(probs[i][y[i][0]]))\n",
    "        #   t= t + val \n",
    "        #data_loss = t/N\n",
    "        #corect_logprobs = -np.log(probs[range(N), y])\n",
    "        #data_loss = np.sum(corect_logprobs) / N\n",
    "    \n",
    "        \n",
    "        grads={}\n",
    "        \n",
    "        dscores=probs\n",
    "        dscores[range(N),y] -= 1\n",
    "        dscores /= N\n",
    "        \n",
    "        grads['w2'] = np.dot(np.transpose(a1), dscores)\n",
    "        grads['b2'] = np.sum(dscores, axis=0)\n",
    "        \n",
    "        dhidden = np.dot(dscores, w2.T)\n",
    "        # backprop the ReLU non-linearity\n",
    "        dhidden[a1 <= 0] = 0\n",
    "        # finally into W,b\n",
    "        grads['w1'] = np.dot(X.T, dhidden)\n",
    "        grads['b1'] = dhidden#np.sum(dhidden, axis=0)\n",
    "        \n",
    "        # add regularization gradient contribution\n",
    "        grads['w2'] += reg * w2\n",
    "        grads['w1'] += reg * w1\n",
    "        \n",
    "        return loss, grads\n",
    "    \n",
    "    def train(self, X, y, X_val, y_val, learning_rate=1e-3, learning_rate_decay=1, reg=1e-5, num_epochs=200, verbose=True):\n",
    "        \n",
    "        num_train = X.shape[0]\n",
    "        #iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "        \n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "        for it in xrange(num_epochs):\n",
    "            \n",
    "            #\n",
    "            #X_batch=None\n",
    "            #y_batch=None\n",
    "            \n",
    "            #sample_indices = np.random.choice(np.arange(num_train), batch_size)\n",
    "            #X_batch = X[sample_indices]\n",
    "            #y_batch = y[sample_indices]\n",
    "    \n",
    "            \n",
    "            \n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            #loss, grads = self.loss(X_batch, y_batch, 0.0)\n",
    "            loss, grads = self.loss(X, y=y, reg=reg)\n",
    "            #loss_history.append(loss)\n",
    "            \n",
    "            self.params['w1'] =self.params['w1'] - (learning_rate * grads['w1'])\n",
    "            self.params['b1'] =self.params['b1'] - (learning_rate * grads['b1'])\n",
    "            self.params['w2'] =self.params['w2'] - (learning_rate * grads['w2'])\n",
    "            self.params['b2'] =self.params['b2'] - (learning_rate * grads['b2'])\n",
    "            \n",
    "            #if verbose and it % 10 == 0:\n",
    "            #print 'iteration %d / %d: loss %f' % (it, num_epochs, loss)\n",
    "                \n",
    "            # Every epoch, check train and val accuracy and decay learning rate.\n",
    "            #\n",
    "           \n",
    "            train_acc = (self.predict(X) == y).mean()\n",
    "            val_acc = (self.predict(X_val) == y_val).mean()\n",
    "            \n",
    "            print 'iteration %d / %d: loss %f : training accuracy %f, and val accuracy %f' % (it, num_epochs, loss,train_acc, val_acc)\n",
    "            \n",
    "            #train_acc_history.append(train_acc)\n",
    "            #val_acc_history.append(val_acc)\n",
    "            \n",
    "            \n",
    "            learning_rate *= learning_rate_decay\n",
    "             \n",
    "                \n",
    "        #return {\n",
    "           #'loss_history': loss_history,\n",
    "          # 'train_acc_history': train_acc_history,\n",
    "         #  'val_acc_history': val_acc_history,\n",
    "        #}\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        y_pred = None\n",
    "        \n",
    "        z1 = np.dot(X,self.params['w1']) + self.params['b1']\n",
    "        a1 = np.maximum(0, z1) # pass through ReLU activation function\n",
    "        scores = np.dot(a1,self.params['w2']) + self.params['b2']\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "    \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) \n",
      "(10000, 1) \n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x_train = np.transpose(unpickle('cifar-10/data_batch_1')['data'])\n",
    "x_train=np.transpose(x_train)\n",
    "#x_train=np.transpose(x_train)[0:20,:]\n",
    "y_train = [unpickle('cifar-10/data_batch_1')['labels']]\n",
    "y_train = np.transpose(y_train)\n",
    "#y_train = np.transpose(y_train)[0:20,:]\n",
    "print np.shape(x_train),'\\n',np.shape(y_train),'\\n',y_train[6][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net=TwoLayerNet(32*32*3, 50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23025.819902107327\n"
     ]
    }
   ],
   "source": [
    "loss,g=net.loss(x_train,y_train)\n",
    "print loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-72895a50f6f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "net.train(x_train,y_train,x_train,y_train,learning_rate=1e-1,learning_rate_decay=1,num_epochs=10,reg=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
